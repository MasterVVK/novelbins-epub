#!/usr/bin/env python3
"""
–°–∫—Ä–∏–ø—Ç –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ EPUB —Ñ–∞–π–ª–∞ –∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –ø–µ—Ä–≤—ã—Ö 10 –≥–ª–∞–≤
"""

import sys
import os
import re
import json
import zipfile
import xml.etree.ElementTree as ET
from collections import Counter
from pathlib import Path
from typing import Dict, List, Optional
import logging

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(level=logging.WARNING)
logger = logging.getLogger(__name__)

# –ü—Ä–æ—Å—Ç–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è EPUBParser –±–µ–∑ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π –æ—Ç –¥—Ä—É–≥–∏—Ö –º–æ–¥—É–ª–µ–π
class SimplifiedEPUBParser:
    """–£–ø—Ä–æ—â–µ–Ω–Ω—ã–π –ø–∞—Ä—Å–µ—Ä EPUB –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞"""
    
    def __init__(self, epub_path: str, max_chapters: Optional[int] = None):
        self.epub_path = epub_path
        self.max_chapters = max_chapters
        self.epub_data = {}
        self.chapters = []
        
        if epub_path:
            self.load_epub(epub_path)
    
    def load_epub(self, epub_path: str) -> bool:
        """–ó–∞–≥—Ä—É–∑–∏—Ç—å –∏ —Ä–∞–∑–æ–±—Ä–∞—Ç—å EPUB —Ñ–∞–π–ª"""
        try:
            if not os.path.exists(epub_path):
                logger.error(f"EPUB —Ñ–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {epub_path}")
                return False
            
            with zipfile.ZipFile(epub_path, 'r') as epub_zip:
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —ç—Ç–æ –¥–µ–π—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ EPUB
                if 'META-INF/container.xml' not in epub_zip.namelist():
                    logger.error("–§–∞–π–ª –Ω–µ —Å–æ–¥–µ—Ä–∂–∏—Ç META-INF/container.xml")
                    return False
                
                # –ß–∏—Ç–∞–µ–º container.xml –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –ø—É—Ç–∏ –∫ OPF —Ñ–∞–π–ª—É
                container_xml = epub_zip.read('META-INF/container.xml')
                container_tree = ET.fromstring(container_xml)
                
                # –ù–∞—Ö–æ–¥–∏–º OPF —Ñ–∞–π–ª
                rootfile = container_tree.find('.//{*}rootfile')
                if rootfile is None:
                    logger.error("–ù–µ –Ω–∞–π–¥–µ–Ω rootfile –≤ container.xml")
                    return False
                
                opf_path = rootfile.get('full-path')
                if not opf_path:
                    logger.error("–ü—É—Ç—å –∫ OPF —Ñ–∞–π–ª—É –Ω–µ –Ω–∞–π–¥–µ–Ω")
                    return False
                
                # –ß–∏—Ç–∞–µ–º OPF —Ñ–∞–π–ª
                opf_content = epub_zip.read(opf_path)
                opf_tree = ET.fromstring(opf_content)
                
                # –ò–∑–≤–ª–µ–∫–∞–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
                self._extract_metadata(opf_tree)
                
                # –ò–∑–≤–ª–µ–∫–∞–µ–º –º–∞–Ω–∏—Ñ–µ—Å—Ç (—Å–ø–∏—Å–æ–∫ —Ñ–∞–π–ª–æ–≤)
                manifest = self._extract_manifest(opf_tree, epub_zip, opf_path)
                
                # –ò–∑–≤–ª–µ–∫–∞–µ–º spine (–ø–æ—Ä—è–¥–æ–∫ —á—Ç–µ–Ω–∏—è)
                spine = self._extract_spine(opf_tree)
                
                if not spine:
                    # –ï—Å–ª–∏ spine –ø—É—Å—Ç, –∏—â–µ–º HTML —Ñ–∞–π–ª—ã –≤ –º–∞–Ω–∏—Ñ–µ—Å—Ç–µ
                    html_items = [item_id for item_id, item in manifest.items() 
                                if item.get('media_type') in ['application/xhtml+xml', 'text/html']]
                    spine = html_items
                
                # –ò–∑–≤–ª–µ–∫–∞–µ–º –≥–ª–∞–≤—ã
                self._extract_chapters(manifest, spine, epub_zip)
                
                return len(self.chapters) > 0
                
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ EPUB: {e}")
            return False
    
    def _extract_metadata(self, opf_tree: ET.Element):
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö –∏–∑ OPF"""
        metadata = opf_tree.find('.//{*}metadata')
        if metadata is None:
            return
        
        self.epub_data['metadata'] = {}
        
        # –ù–∞–∑–≤–∞–Ω–∏–µ
        title_elem = metadata.find('.//{*}title')
        if title_elem is not None:
            self.epub_data['metadata']['title'] = title_elem.text or ''
        
        # –ê–≤—Ç–æ—Ä
        creator_elem = metadata.find('.//{*}creator')
        if creator_elem is not None:
            self.epub_data['metadata']['author'] = creator_elem.text or ''
        
        # –û–ø–∏—Å–∞–Ω–∏–µ
        description_elem = metadata.find('.//{*}description')
        if description_elem is not None:
            self.epub_data['metadata']['description'] = description_elem.text or ''
        
        # –Ø–∑—ã–∫
        language_elem = metadata.find('.//{*}language')
        if language_elem is not None:
            self.epub_data['metadata']['language'] = language_elem.text or 'en'
    
    def _extract_manifest(self, opf_tree: ET.Element, epub_zip: zipfile.ZipFile, opf_path: str) -> Dict:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –º–∞–Ω–∏—Ñ–µ—Å—Ç–∞ (—Å–ø–∏—Å–∫–∞ —Ñ–∞–π–ª–æ–≤)"""
        manifest = {}
        manifest_elem = opf_tree.find('.//{*}manifest')
        
        if manifest_elem is None:
            return manifest
        
        opf_dir = os.path.dirname(opf_path)
        
        for item in manifest_elem.findall('.//{*}item'):
            item_id = item.get('id')
            href = item.get('href')
            media_type = item.get('media-type')
            
            if item_id and href:
                # –ü–æ–ª–Ω—ã–π –ø—É—Ç—å –∫ —Ñ–∞–π–ª—É
                full_path = os.path.join(opf_dir, href) if opf_dir else href
                manifest[item_id] = {
                    'href': href,
                    'full_path': full_path,
                    'media_type': media_type
                }
        
        return manifest
    
    def _extract_spine(self, opf_tree: ET.Element) -> List[str]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ spine (–ø–æ—Ä—è–¥–∫–∞ —á—Ç–µ–Ω–∏—è)"""
        spine = []
        spine_elem = opf_tree.find('.//{*}spine')
        
        if spine_elem is None:
            return spine
        
        for itemref in spine_elem.findall('.//{*}itemref'):
            idref = itemref.get('idref')
            if idref:
                spine.append(idref)
        
        return spine
    
    def _extract_chapters(self, manifest: Dict, spine: List[str], epub_zip: zipfile.ZipFile):
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –≥–ª–∞–≤ –∏–∑ EPUB"""
        self.chapters = []
        chapter_number = 1
        extracted_count = 0
        
        for item_id in spine:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –Ω–∞ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –≥–ª–∞–≤
            if self.max_chapters and extracted_count >= self.max_chapters:
                break
            
            if item_id not in manifest:
                continue
            
            item = manifest[item_id]
            media_type = item.get('media_type', '')
            
            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ç–æ–ª—å–∫–æ HTML/XHTML —Ñ–∞–π–ª—ã
            if media_type not in ['application/xhtml+xml', 'text/html']:
                continue
            
            try:
                full_path = item['full_path']
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —Ñ–∞–π–ª —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –≤ –∞—Ä—Ö–∏–≤–µ
                if full_path not in epub_zip.namelist():
                    continue
                
                # –ß–∏—Ç–∞–µ–º —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ —Ñ–∞–π–ª–∞
                try:
                    content = epub_zip.read(full_path).decode('utf-8')
                except UnicodeDecodeError:
                    try:
                        content = epub_zip.read(full_path).decode('latin-1')
                    except:
                        continue
                
                # –ü–∞—Ä—Å–∏–º HTML –∫–æ–Ω—Ç–µ–Ω—Ç
                chapter_info = self._parse_html_content(content, chapter_number)
                if chapter_info:
                    self.chapters.append(chapter_info)
                    chapter_number += 1
                    extracted_count += 1
                    
            except Exception as e:
                continue
    
    def _parse_html_content(self, html_content: str, chapter_number: int) -> Optional[Dict]:
        """–ü–∞—Ä—Å–∏–Ω–≥ HTML –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –≥–ª–∞–≤—ã"""
        try:
            # –ü—Ä–æ—Å—Ç–æ–π –ø–∞—Ä—Å–∏–Ω–≥ HTML —Å –ø–æ–º–æ—â—å—é —Ä–µ–≥—É–ª—è—Ä–Ω—ã—Ö –≤—ã—Ä–∞–∂–µ–Ω–∏–π
            html_content = re.sub(r'<\?xml[^>]*\?>', '', html_content)
            html_content = re.sub(r'<!DOCTYPE[^>]*>', '', html_content)
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫
            title_match = re.search(r'<title[^>]*>(.*?)</title>', html_content, re.IGNORECASE | re.DOTALL)
            title = title_match.group(1).strip() if title_match else f"–ì–ª–∞–≤–∞ {chapter_number}"
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ body
            body_match = re.search(r'<body[^>]*>(.*?)</body>', html_content, re.IGNORECASE | re.DOTALL)
            if not body_match:
                return None
            
            body_content = body_match.group(1)
            
            # –û—á–∏—â–∞–µ–º HTML —Ç–µ–≥–∏, –æ—Å—Ç–∞–≤–ª—è—è —Ç–æ–ª—å–∫–æ —Ç–µ–∫—Å—Ç
            content = self._clean_html_content(body_content)
            
            if not content.strip():
                return None
            
            return {
                'number': chapter_number,
                'title': title,
                'content': content,
                'chapter_id': f"chapter_{chapter_number}",
                'word_count': len(content.split())
            }
            
        except Exception as e:
            return None
    
    def _clean_html_content(self, html_content: str) -> str:
        """–û—á–∏—Å—Ç–∫–∞ HTML –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –æ—Ç —Ç–µ–≥–æ–≤"""
        # –£–¥–∞–ª—è–µ–º —Å–∫—Ä–∏–ø—Ç—ã –∏ —Å—Ç–∏–ª–∏
        html_content = re.sub(r'<script[^>]*>.*?</script>', '', html_content, flags=re.IGNORECASE | re.DOTALL)
        html_content = re.sub(r'<style[^>]*>.*?</style>', '', html_content, flags=re.IGNORECASE | re.DOTALL)
        
        # –ó–∞–º–µ–Ω—è–µ–º –Ω–µ–∫–æ—Ç–æ—Ä—ã–µ —Ç–µ–≥–∏ –Ω–∞ –ø–µ—Ä–µ–Ω–æ—Å—ã —Å—Ç—Ä–æ–∫
        html_content = re.sub(r'</?(?:p|div|br|h[1-6])[^>]*>', '\n', html_content, flags=re.IGNORECASE)
        
        # –£–¥–∞–ª—è–µ–º –≤—Å–µ –æ—Å—Ç–∞–ª—å–Ω—ã–µ HTML —Ç–µ–≥–∏
        html_content = re.sub(r'<[^>]+>', '', html_content)
        
        # –î–µ–∫–æ–¥–∏—Ä—É–µ–º HTML —Å—É—â–Ω–æ—Å—Ç–∏
        html_content = html_content.replace('&nbsp;', ' ')
        html_content = html_content.replace('&amp;', '&')
        html_content = html_content.replace('&lt;', '<')
        html_content = html_content.replace('&gt;', '>')
        html_content = html_content.replace('&quot;', '"')
        html_content = html_content.replace('&#39;', "'")
        
        # –û—á–∏—â–∞–µ–º –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã –∏ –ø–µ—Ä–µ–Ω–æ—Å—ã —Å—Ç—Ä–æ–∫
        lines = [line.strip() for line in html_content.split('\n')]
        lines = [line for line in lines if line]
        
        return '\n\n'.join(lines)
    
    def get_book_info(self) -> Dict:
        """–ü–æ–ª—É—á–∏—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –∫–Ω–∏–≥–µ"""
        metadata = self.epub_data.get('metadata', {})
        
        return {
            'title': metadata.get('title', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è –∫–Ω–∏–≥–∞'),
            'author': metadata.get('author', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π –∞–≤—Ç–æ—Ä'),
            'description': metadata.get('description', ''),
            'language': metadata.get('language', 'en'),
            'total_chapters': len(self.chapters)
        }


def analyze_content_style(content: str) -> dict:
    """–ê–Ω–∞–ª–∏–∑ —Å—Ç–∏–ª—è –∏ –æ—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–µ–π —Ç–µ–∫—Å—Ç–∞"""
    
    # –î–ª–∏–Ω–∞ —Ç–µ–∫—Å—Ç–∞
    word_count = len(content.split())
    char_count = len(content)
    
    # –ê–Ω–∞–ª–∏–∑ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π
    sentences = re.split(r'[.!?]+', content)
    sentences = [s.strip() for s in sentences if s.strip()]
    avg_sentence_length = sum(len(s.split()) for s in sentences) / len(sentences) if sentences else 0
    
    # –¢–∏–ø—ã –∑–Ω–∞–∫–æ–≤ –ø—Ä–µ–ø–∏–Ω–∞–Ω–∏—è
    punctuation_analysis = {
        'exclamation_marks': content.count('!'),
        'question_marks': content.count('?'),
        'ellipsis': len(re.findall(r'\.{3,}', content)),
        'dashes': content.count('‚Äî') + content.count('--'),
        'quotation_marks': content.count('"') + content.count('"') + content.count('"')
    }
    
    # –î–∏–∞–ª–æ–≥–∏ (–ø—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ–º, —á—Ç–æ —Å—Ç—Ä–æ–∫–∏ –≤ –∫–∞–≤—ã—á–∫–∞—Ö - –¥–∏–∞–ª–æ–≥–∏)
    dialogue_lines = len(re.findall(r'[""¬´¬ª"\'`].+?[""¬´¬ª"\']', content))
    
    # –ü–æ–≤—Ç–æ—Ä—è—é—â–∏–µ—Å—è —Å–ª–æ–≤–∞/—Ñ—Ä–∞–∑—ã (–∏—Å–∫–ª—é—á–∞—è —Å–ª—É–∂–µ–±–Ω—ã–µ)
    words = re.findall(r'\b\w+\b', content.lower())
    stop_words = {'–∏', '–≤', '–Ω–∞', '—Å', '–ø–æ', '–¥–ª—è', '–æ—Ç', '–¥–æ', '–∏–∑', '–∫', '—É', '–æ', '–æ–±', '–∑–∞', '–ø–æ–¥', '–Ω–∞–¥', '–ø—Ä–∏', '—á–µ—Ä–µ–∑', '–º–µ–∂–¥—É', '–±–µ–∑', '—è', '—Ç—ã', '–æ–Ω', '–æ–Ω–∞', '–æ–Ω–æ', '–º—ã', '–≤—ã', '–æ–Ω–∏', '—ç—Ç–æ—Ç', '—ç—Ç–∞', '—ç—Ç–æ', '—ç—Ç–∏', '—Ç–æ—Ç', '—Ç–∞', '—Ç–æ', '—Ç–µ', '–∫–æ—Ç–æ—Ä—ã–π', '–∫–æ—Ç–æ—Ä–∞—è', '–∫–æ—Ç–æ—Ä–æ–µ', '–∫–æ—Ç–æ—Ä—ã–µ', '—á—Ç–æ', '–∫–∞–∫', '–≥–¥–µ', '–∫–æ–≥–¥–∞', '–ø–æ—á–µ–º—É', '–∑–∞—á–µ–º', '–∫—É–¥–∞', '–æ—Ç–∫—É–¥–∞', '—Å–∫–æ–ª—å–∫–æ', '—á–µ–π', '—á—å—è', '—á—å—ë', '—á—å–∏', '–Ω–µ', '–Ω–∏', '–¥–∞', '–Ω–µ—Ç', '—É–∂–µ', '–µ—â–µ', '–µ—â—ë', '—Ç–æ–ª—å–∫–æ', '–ª–∏—à—å', '–¥–∞–∂–µ', '–≤–æ—Ç', '–≤–æ–Ω', '–∑–¥–µ—Å—å', '—Ç–∞–º', '—Ç—É—Ç', '—Å—é–¥–∞', '—Ç—É–¥–∞', '—Å–µ–π—á–∞—Å', '—Ç–µ–ø–µ—Ä—å', '—Ç–æ–≥–¥–∞', '–ø–æ—Ç–æ–º', '–∑–∞—Ç–µ–º', '—Å–Ω–∞—á–∞–ª–∞', '—Å–ø–µ—Ä–≤–∞', '—Å–Ω–æ–≤–∞', '–æ–ø—è—Ç—å', 'again', 'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by', 'from', 'up', 'about', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'between', 'among', 'throughout', 'despite', 'towards', 'upon', 'within', 'without', 'against', 'his', 'her', 'its', 'their', 'our', 'your', 'my', 'this', 'that', 'these', 'those', 'who', 'whom', 'whose', 'which', 'what', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 'can', 'will', 'just', 'should', 'now', 'than', 'also', 'back', 'other', 'many', 'much', 'well', 'may'}
    
    content_words = [w for w in words if len(w) > 2 and w not in stop_words]
    word_freq = Counter(content_words)
    most_common_words = word_freq.most_common(10)
    
    return {
        'word_count': word_count,
        'character_count': char_count,
        'sentence_count': len(sentences),
        'avg_sentence_length': round(avg_sentence_length, 1),
        'punctuation_analysis': punctuation_analysis,
        'dialogue_lines': dialogue_lines,
        'most_common_words': most_common_words,
        'writing_style': {
            'descriptive_ratio': (punctuation_analysis['exclamation_marks'] + punctuation_analysis['ellipsis']) / max(word_count / 100, 1),
            'dialogue_ratio': dialogue_lines / max(len(sentences), 1) * 100,
            'complexity_score': avg_sentence_length / 10  # –ø—Ä–æ—Å—Ç–∞—è –º–µ—Ç—Ä–∏–∫–∞ —Å–ª–æ–∂–Ω–æ—Å—Ç–∏
        }
    }


def extract_entities(text: str) -> dict:
    """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø–µ—Ä—Å–æ–Ω–∞–∂–µ–π, –ª–æ–∫–∞—Ü–∏–π –∏ —Ç–µ—Ä–º–∏–Ω–æ–≤"""
    
    # –ò–º–µ–Ω–∞ –ø–µ—Ä—Å–æ–Ω–∞–∂–µ–π (—Å–ª–æ–≤–∞ —Å –∑–∞–≥–ª–∞–≤–Ω–æ–π –±—É–∫–≤—ã, –≤—Å—Ç—Ä–µ—á–∞—é—â–∏–µ—Å—è —á–∞—Å—Ç–æ)
    capitalized_words = re.findall(r'\b[A-Z–ê-–Ø][a-z–∞-—è]+\b', text)
    name_counter = Counter(capitalized_words)
    
    # –ò—Å–∫–ª—é—á–∞–µ–º —Å–ª–æ–≤–∞, –∫–æ—Ç–æ—Ä—ã–µ —á–∞—Å—Ç–æ –≤—Å—Ç—Ä–µ—á–∞—é—Ç—Å—è –≤ –Ω–∞—á–∞–ª–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π
    common_sentence_starters = {'–ò', '–í', '–ù–∞', '–°', '–ü–æ', '–î–ª—è', '–û—Ç', '–î–æ', '–ò–∑', '–ö', '–£', '–û', '–û–±', '–ó–∞', '–ü–æ–¥', '–ù–∞–¥', '–ü—Ä–∏', '–ß–µ—Ä–µ–∑', '–ú–µ–∂–¥—É', '–ë–µ–∑', '–û–Ω', '–û–Ω–∞', '–û–Ω–æ', '–ú—ã', '–í—ã', '–û–Ω–∏', '–≠—Ç–æ—Ç', '–≠—Ç–∞', '–≠—Ç–æ', '–≠—Ç–∏', '–¢–æ—Ç', '–¢–∞', '–¢–æ', '–¢–µ', '–ö–æ—Ç–æ—Ä—ã–π', '–ö–æ—Ç–æ—Ä–∞—è', '–ö–æ—Ç–æ—Ä–æ–µ', '–ö–æ—Ç–æ—Ä—ã–µ', '–ß—Ç–æ', '–ö–∞–∫', '–ì–¥–µ', '–ö–æ–≥–¥–∞', '–ù–æ', '–ê', '–î–∞', '–ù–µ—Ç', '–£–∂–µ', '–ï—â–µ', '–¢–æ–ª—å–∫–æ', '–õ–∏—à—å', '–î–∞–∂–µ', '–í–æ—Ç', '–í–æ–Ω', '–ó–¥–µ—Å—å', '–¢–∞–º', '–¢—É—Ç', 'The', 'And', 'But', 'Or', 'So', 'Then', 'Now', 'Here', 'There', 'This', 'That', 'These', 'Those', 'When', 'Where', 'Why', 'How', 'What', 'Who', 'Which'}
    
    potential_names = {name: count for name, count in name_counter.most_common(20) 
                      if count > 2 and name not in common_sentence_starters and len(name) > 2}
    
    # –õ–æ–∫–∞—Ü–∏–∏ (—á–∞—Å—Ç–æ –∏–¥—É—Ç –ø–æ—Å–ª–µ –ø—Ä–µ–¥–ª–æ–≥–æ–≤)
    location_patterns = [
        r'(?:–≤|–Ω–∞|–∏–∑|–∫|–æ—Ç|–¥–æ|–æ–∫–æ–ª–æ|—Ä—è–¥–æ–º —Å|–≤–æ–∑–ª–µ)\s+([–ê-–ØA-Z][–∞-—èa-z]+(?:\s+[–ê-–ØA-Z][–∞-—èa-z]+)*)',
        r'(?:in|at|to|from|near|around)\s+([A-Z][a-z]+(?:\s+[A-Z][a-z]+)*)'
    ]
    
    locations = []
    for pattern in location_patterns:
        matches = re.findall(pattern, text, re.IGNORECASE)
        locations.extend(matches)
    
    location_counter = Counter(locations)
    
    # –¢–µ—Ä–º–∏–Ω—ã - —Å–ª–æ–≤–∞, –∫–æ—Ç–æ—Ä—ã–µ –ø–∏—à—É—Ç—Å—è —Å –∑–∞–≥–ª–∞–≤–Ω–æ–π –±—É–∫–≤—ã –≤ —Å–µ—Ä–µ–¥–∏–Ω–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
    sentences = re.split(r'[.!?]+', text)
    terms = []
    
    for sentence in sentences:
        words = sentence.strip().split()
        if len(words) > 1:  # –ø—Ä–æ–ø—É—Å–∫–∞–µ–º –ø–µ—Ä–≤–æ–µ —Å–ª–æ–≤–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
            for word in words[1:]:
                # –ò—â–µ–º —Å–ª–æ–≤–∞ —Å –∑–∞–≥–ª–∞–≤–Ω–æ–π –±—É–∫–≤—ã –≤ —Å–µ—Ä–µ–¥–∏–Ω–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
                if re.match(r'^[A-Z–ê-–Ø][a-z–∞-—è]+$', word) and len(word) > 3:
                    terms.append(word)
    
    term_counter = Counter(terms)
    
    return {
        'characters': dict(potential_names),
        'locations': dict(location_counter.most_common(10)),
        'terms': dict(term_counter.most_common(15))
    }


def analyze_epub_file(epub_path: str, max_chapters: int = 10):
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –∞–Ω–∞–ª–∏–∑–∞ EPUB —Ñ–∞–π–ª–∞"""
    
    print(f"–ê–Ω–∞–ª–∏–∑ EPUB —Ñ–∞–π–ª–∞: {epub_path}")
    print("=" * 60)
    
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø–∞—Ä—Å–µ—Ä–∞ —Å –ª–∏–º–∏—Ç–æ–º –≥–ª–∞–≤
    parser = SimplifiedEPUBParser(epub_path=epub_path, max_chapters=max_chapters)
    
    if not parser.chapters:
        print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å EPUB —Ñ–∞–π–ª –∏–ª–∏ –∏–∑–≤–ª–µ—á—å –≥–ª–∞–≤—ã")
        return
    
    # –ü–æ–ª—É—á–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –∫–Ω–∏–≥–µ
    book_info = parser.get_book_info()
    
    print("üìö –ò–ù–§–û–†–ú–ê–¶–ò–Ø –û –ü–†–û–ò–ó–í–ï–î–ï–ù–ò–ò")
    print("-" * 40)
    print(f"–ù–∞–∑–≤–∞–Ω–∏–µ: {book_info['title']}")
    print(f"–ê–≤—Ç–æ—Ä: {book_info['author']}")
    print(f"–û–ø–∏—Å–∞–Ω–∏–µ: {book_info['description'][:200]}{'...' if len(book_info['description']) > 200 else ''}")
    print(f"–Ø–∑—ã–∫: {book_info['language']}")
    print(f"–í—Å–µ–≥–æ –≥–ª–∞–≤ –≤ —Ñ–∞–π–ª–µ: {book_info['total_chapters']}")
    print(f"–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º—ã—Ö –≥–ª–∞–≤: {len(parser.chapters)}")
    print()
    
    # –ê–Ω–∞–ª–∏–∑ –≥–ª–∞–≤
    print("üìñ –ü–ï–†–í–´–ï 10 –ì–õ–ê–í")
    print("-" * 40)
    
    all_content = ""
    chapter_analyses = []
    
    for i, chapter in enumerate(parser.chapters, 1):
        print(f"\n{i}. {chapter['title']}")
        print(f"   –°–ª–æ–≤: {chapter['word_count']}")
        
        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –Ω–∞—á–∞–ª–æ –≥–ª–∞–≤—ã (–ø–µ—Ä–≤—ã–µ 200 —Å–∏–º–≤–æ–ª–æ–≤)
        content_preview = chapter['content'][:300].replace('\n', ' ')
        print(f"   –ù–∞—á–∞–ª–æ: {content_preview}{'...' if len(chapter['content']) > 300 else ''}")
        
        # –î–æ–±–∞–≤–ª—è–µ–º –∫ –æ–±—â–µ–º—É –∫–æ–Ω—Ç–µ–Ω—Ç—É –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
        all_content += "\n\n" + chapter['content']
        
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Å—Ç–∏–ª—å –∫–∞–∂–¥–æ–π –≥–ª–∞–≤—ã
        chapter_analysis = analyze_content_style(chapter['content'])
        chapter_analyses.append(chapter_analysis)
    
    print("\n")
    
    # –û–±—â–∏–π –∞–Ω–∞–ª–∏–∑ —Å—Ç–∏–ª—è
    print("üé® –ê–ù–ê–õ–ò–ó –°–¢–ò–õ–Ø –ò –û–°–û–ë–ï–ù–ù–û–°–¢–ï–ô")
    print("-" * 40)
    
    overall_analysis = analyze_content_style(all_content)
    
    print(f"–û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ–≤: {overall_analysis['word_count']:,}")
    print(f"–û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–∏–º–≤–æ–ª–æ–≤: {overall_analysis['character_count']:,}")
    print(f"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π: {overall_analysis['sentence_count']:,}")
    print(f"–°—Ä–µ–¥–Ω—è—è –¥–ª–∏–Ω–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è: {overall_analysis['avg_sentence_length']} —Å–ª–æ–≤")
    
    print(f"\n–ü—É–Ω–∫—Ç—É–∞—Ü–∏—è:")
    punct = overall_analysis['punctuation_analysis']
    print(f"  - –í–æ—Å–∫–ª–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–µ –∑–Ω–∞–∫–∏: {punct['exclamation_marks']}")
    print(f"  - –í–æ–ø—Ä–æ—Å–∏—Ç–µ–ª—å–Ω—ã–µ –∑–Ω–∞–∫–∏: {punct['question_marks']}")
    print(f"  - –ú–Ω–æ–≥–æ—Ç–æ—á–∏—è: {punct['ellipsis']}")
    print(f"  - –¢–∏—Ä–µ: {punct['dashes']}")
    print(f"  - –ö–∞–≤—ã—á–∫–∏: {punct['quotation_marks']}")
    
    print(f"\n–°—Ç–∏–ª—å –ø–∏—Å—å–º–∞:")
    style = overall_analysis['writing_style']
    print(f"  - –≠–∫—Å–ø—Ä–µ—Å—Å–∏–≤–Ω–æ—Å—Ç—å: {style['descriptive_ratio']:.1f} (–≤–æ—Å–∫–ª–∏—Ü–∞–Ω–∏—è/–º–Ω–æ–≥–æ—Ç–æ—á–∏—è –Ω–∞ 100 —Å–ª–æ–≤)")
    print(f"  - –î–æ–ª—è –¥–∏–∞–ª–æ–≥–æ–≤: {style['dialogue_ratio']:.1f}%")
    print(f"  - –°–ª–æ–∂–Ω–æ—Å—Ç—å —Ç–µ–∫—Å—Ç–∞: {style['complexity_score']:.1f}/10")
    
    # –ê–Ω–∞–ª–∏–∑ —Å—É—â–Ω–æ—Å—Ç–µ–π
    print("\nüîç –ö–õ–Æ–ß–ï–í–´–ï –ü–ï–†–°–û–ù–ê–ñ–ò, –õ–û–ö–ê–¶–ò–ò –ò –¢–ï–†–ú–ò–ù–´")
    print("-" * 40)
    
    entities = extract_entities(all_content)
    
    print("–ü–µ—Ä—Å–æ–Ω–∞–∂–∏ (–ø–æ —á–∞—Å—Ç–æ—Ç–µ —É–ø–æ–º–∏–Ω–∞–Ω–∏—è):")
    for name, count in sorted(entities['characters'].items(), key=lambda x: x[1], reverse=True)[:10]:
        print(f"  - {name}: {count} —É–ø–æ–º–∏–Ω–∞–Ω–∏–π")
    
    if entities['locations']:
        print(f"\n–õ–æ–∫–∞—Ü–∏–∏:")
        for location, count in sorted(entities['locations'].items(), key=lambda x: x[1], reverse=True)[:8]:
            print(f"  - {location}: {count} —É–ø–æ–º–∏–Ω–∞–Ω–∏–π")
    
    if entities['terms']:
        print(f"\n–í–∞–∂–Ω—ã–µ —Ç–µ—Ä–º–∏–Ω—ã:")
        for term, count in sorted(entities['terms'].items(), key=lambda x: x[1], reverse=True)[:10]:
            print(f"  - {term}: {count} —É–ø–æ–º–∏–Ω–∞–Ω–∏–π")
    
    # –ù–∞–∏–±–æ–ª–µ–µ —á–∞—Å—Ç—ã–µ —Å–ª–æ–≤–∞
    print(f"\n–ù–∞–∏–±–æ–ª–µ–µ —á–∞—Å—Ç—ã–µ –∑–Ω–∞—á–∏–º—ã–µ —Å–ª–æ–≤–∞:")
    for word, count in overall_analysis['most_common_words'][:15]:
        print(f"  - {word}: {count}")
    
    # –ê–Ω–∞–ª–∏–∑ –ø—Ä–æ–≥—Ä–µ—Å—Å–∏–∏ –ø–æ –≥–ª–∞–≤–∞–º
    print("\nüìä –°–¢–ê–¢–ò–°–¢–ò–ö–ê –ü–û –ì–õ–ê–í–ê–ú")
    print("-" * 40)
    
    total_words = sum(analysis['word_count'] for analysis in chapter_analyses)
    avg_words_per_chapter = total_words / len(chapter_analyses)
    
    print(f"–°—Ä–µ–¥–Ω—è—è –¥–ª–∏–Ω–∞ –≥–ª–∞–≤—ã: {avg_words_per_chapter:.0f} —Å–ª–æ–≤")
    
    shortest_chapter = min(chapter_analyses, key=lambda x: x['word_count'])
    longest_chapter = max(chapter_analyses, key=lambda x: x['word_count'])
    
    shortest_idx = chapter_analyses.index(shortest_chapter) + 1
    longest_idx = chapter_analyses.index(longest_chapter) + 1
    
    print(f"–°–∞–º–∞—è –∫–æ—Ä–æ—Ç–∫–∞—è –≥–ª–∞–≤–∞: #{shortest_idx} ({shortest_chapter['word_count']} —Å–ª–æ–≤)")
    print(f"–°–∞–º–∞—è –¥–ª–∏–Ω–Ω–∞—è –≥–ª–∞–≤–∞: #{longest_idx} ({longest_chapter['word_count']} —Å–ª–æ–≤)")
    
    # –ü—Ä–æ–≥—Ä–µ—Å—Å–∏—è —Å–ª–æ–∂–Ω–æ—Å—Ç–∏
    complexity_scores = [analysis['writing_style']['complexity_score'] for analysis in chapter_analyses]
    avg_complexity = sum(complexity_scores) / len(complexity_scores)
    print(f"–°—Ä–µ–¥–Ω—è—è —Å–ª–æ–∂–Ω–æ—Å—Ç—å —Ç–µ–∫—Å—Ç–∞: {avg_complexity:.1f}/10")
    
    print("\n‚úÖ –ê–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à–µ–Ω!")


if __name__ == "__main__":
    epub_file_path = "/home/user/novelbins-epub/web_app/instance/epub_files/qinkan.net.epub"
    
    if not os.path.exists(epub_file_path):
        print(f"‚ùå EPUB —Ñ–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {epub_file_path}")
        sys.exit(1)
    
    analyze_epub_file(epub_file_path, max_chapters=10)