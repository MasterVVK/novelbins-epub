"""
–°–µ—Ä–≤–∏—Å –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –≥–ª–æ—Å—Å–∞—Ä–∏—è –∏–∑ –ø–µ—Ä–µ–≤–µ–¥–µ–Ω–Ω—ã—Ö –≥–ª–∞–≤
"""
import re
from typing import Dict, List, Tuple, Optional
from collections import Counter
from difflib import SequenceMatcher

from app import db
from app.models import Chapter, Translation, GlossaryItem, Novel
from app.services.glossary_service import GlossaryService
from app.services.translator_service import TranslatorService


class GlossaryExtractor:
    """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –≥–ª–æ—Å—Å–∞—Ä–∏—è –∏–∑ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –ø–µ—Ä–µ–≤–æ–¥–æ–≤"""
    
    def __init__(self, novel_id: int):
        self.novel_id = novel_id
        self.novel = Novel.query.get(novel_id)
        self.existing_glossary = GlossaryItem.get_glossary_dict(novel_id)
        
    def extract_from_all_chapters(self, min_frequency: int = 2) -> Dict:
        """
        –ò–∑–≤–ª–µ—á—å —Ç–µ—Ä–º–∏–Ω—ã –∏–∑ –≤—Å–µ—Ö –ø–µ—Ä–µ–≤–µ–¥–µ–Ω–Ω—ã—Ö –≥–ª–∞–≤
        
        Args:
            min_frequency: –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è —á–∞—Å—Ç–æ—Ç–∞ –ø–æ—è–≤–ª–µ–Ω–∏—è —Ç–µ—Ä–º–∏–Ω–∞
        """
        print(f"üìö –ù–∞—á–∏–Ω–∞–µ–º –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –≥–ª–æ—Å—Å–∞—Ä–∏—è –¥–ª—è —Ä–æ–º–∞–Ω–∞ {self.novel.title}")
        
        # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ –ø–µ—Ä–µ–≤–æ–¥—ã
        chapters = Chapter.query.filter_by(novel_id=self.novel_id).all()
        all_terms = Counter()
        chapter_terms = {}
        
        for chapter in chapters:
            # –ü–æ–ª—É—á–∞–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª –∏ –ø–µ—Ä–µ–≤–æ–¥
            translation = Translation.query.filter_by(
                chapter_id=chapter.id,
                translation_type='initial'
            ).first()
            
            if not translation:
                continue
                
            # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ—Ä–º–∏–Ω—ã –∏–∑ –≥–ª–∞–≤—ã
            terms = self.extract_from_chapter(
                chapter.original_text,
                translation.translated_text,
                chapter.chapter_number
            )
            
            chapter_terms[chapter.chapter_number] = terms
            
            # –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º —á–∞—Å—Ç–æ—Ç—É
            for category in terms:
                for term in terms[category]:
                    all_terms[term] += 1
        
        # –§–∏–ª—å—Ç—Ä—É–µ–º –ø–æ —á–∞—Å—Ç–æ—Ç–µ –∏ —Å–æ–∑–¥–∞–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—ã–π –≥–ª–æ—Å—Å–∞—Ä–∏–π
        filtered_glossary = self._filter_by_frequency(chapter_terms, all_terms, min_frequency)
        
        print(f"‚úÖ –ò–∑–≤–ª–µ—á–µ–Ω–æ —Ç–µ—Ä–º–∏–Ω–æ–≤: {sum(len(terms) for terms in filtered_glossary.values())}")
        return filtered_glossary
    
    def extract_from_chapter(self, original: str, translated: str, chapter_num: int) -> Dict:
        """
        –ò–∑–≤–ª–µ—á—å —Ç–µ—Ä–º–∏–Ω—ã –∏–∑ –æ–¥–Ω–æ–π –≥–ª–∞–≤—ã
        """
        terms = {
            'characters': {},
            'locations': {},
            'terms': {},
            'techniques': {},
            'artifacts': {}
        }
        
        # 1. –ò–∑–≤–ª–µ–∫–∞–µ–º –∏–º–µ–Ω–∞ –ø–µ—Ä—Å–æ–Ω–∞–∂–µ–π (–∑–∞–≥–ª–∞–≤–Ω—ã–µ –±—É–∫–≤—ã –≤ —Ä—É—Å—Å–∫–æ–º —Ç–µ–∫—Å—Ç–µ)
        characters = self._extract_proper_names(translated)
        for char in characters:
            # –ü—ã—Ç–∞–µ–º—Å—è –Ω–∞–π—Ç–∏ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ –≤ –æ—Ä–∏–≥–∏–Ω–∞–ª–µ
            orig_char = self._find_original_match(char, original, translated)
            if orig_char:
                terms['characters'][orig_char] = char
        
        # 2. –ò–∑–≤–ª–µ–∫–∞–µ–º —Å–ø–µ—Ü–∏—Ñ–∏—á–µ—Å–∫–∏–µ —Ç–µ—Ä–º–∏–Ω—ã (–ø–æ–≤—Ç–æ—Ä—è—é—â–∏–µ—Å—è –Ω–µ–æ–±—ã—á–Ω—ã–µ —Å–ª–æ–≤–∞)
        specific_terms = self._extract_specific_terms(translated)
        for term in specific_terms:
            orig_term = self._find_original_match(term, original, translated)
            if orig_term:
                category = self._categorize_term(term)
                terms[category][orig_term] = term
        
        return terms
    
    def _extract_proper_names(self, text: str) -> List[str]:
        """–ò–∑–≤–ª–µ—á—å –∏–º–µ–Ω–∞ —Å–æ–±—Å—Ç–≤–µ–Ω–Ω—ã–µ –∏–∑ —Ç–µ–∫—Å—Ç–∞"""
        names = []
        
        # –ü–∞—Ç—Ç–µ—Ä–Ω –¥–ª—è –∏–º–µ–Ω (—Å–ª–æ–≤–∞ —Å –∑–∞–≥–ª–∞–≤–Ω–æ–π –±—É–∫–≤—ã, –∏—Å–∫–ª—é—á–∞—è –Ω–∞—á–∞–ª–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π)
        words = text.split()
        for i, word in enumerate(words):
            if len(word) > 1 and word[0].isupper():
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —ç—Ç–æ –Ω–µ –Ω–∞—á–∞–ª–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
                if i > 0 and not words[i-1].endswith(('.', '!', '?')):
                    # –§–∏–ª—å—Ç—Ä—É–µ–º –æ–±—â–∏–µ —Å–ª–æ–≤–∞
                    if word not in ['–û–Ω', '–û–Ω–∞', '–≠—Ç–æ', '–ï–≥–æ', '–ï—ë', '–ò–º', '–ò—Ö']:
                        names.append(word)
        
        # –£–Ω–∏–∫–∞–ª—å–Ω—ã–µ –∏–º–µ–Ω–∞ —Å —á–∞—Å—Ç–æ—Ç–æ–π > 1
        name_counts = Counter(names)
        return [name for name, count in name_counts.items() if count > 1]
    
    def _extract_specific_terms(self, text: str) -> List[str]:
        """–ò–∑–≤–ª–µ—á—å —Å–ø–µ—Ü–∏—Ñ–∏—á–µ—Å–∫–∏–µ —Ç–µ—Ä–º–∏–Ω—ã"""
        terms = []
        
        # –ò—â–µ–º —Å–ª–æ–≤–∞, —Ö–∞—Ä–∞–∫—Ç–µ—Ä–Ω—ã–µ –¥–ª—è –∂–∞–Ω—Ä–∞
        cultivation_patterns = [
            r'–∫—É–ª—å—Ç–∏–≤–∞—Ü\w*', r'—Ü–∏\b', r'–¥–∞–æ\b', r'–º–µ—Ä–∏–¥–∏–∞–Ω\w*',
            r'–ø–∏–ª—é–ª\w*', r'–∞—Ä—Ç–µ—Ñ–∞–∫—Ç\w*', r'—Ç–µ—Ö–Ω–∏–∫\w*', r'–ø–µ—á–∞—Ç\w*',
            r'—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω\w*', r'–ø—Ä–æ—Ä—ã–≤\w*', r'—Ç—Ä–∏–±—É–ª–∞—Ü\w*'
        ]
        
        for pattern in cultivation_patterns:
            matches = re.findall(pattern, text.lower())
            terms.extend(matches)
        
        # –ò—â–µ–º —Å–æ—Å—Ç–∞–≤–Ω—ã–µ —Ç–µ—Ä–º–∏–Ω—ã
        compound_patterns = [
            r'[–ê-–Ø][–∞-—è]+\s+[–ê-–Ø][–∞-—è]+',  # –î–≤–∞ —Å–ª–æ–≤–∞ —Å –∑–∞–≥–ª–∞–≤–Ω—ã—Ö
            r'[–∞-—è]+\s+[–∞-—è]+—Ü–∏\w*',  # —á—Ç–æ-—Ç–æ —Å "—Ü–∏"
            r'[–∞-—è]+\s+–¥–∞–æ',  # —á—Ç–æ-—Ç–æ —Å "–¥–∞–æ"
        ]
        
        for pattern in compound_patterns:
            matches = re.findall(pattern, text)
            terms.extend(matches)
        
        return list(set(terms))
    
    def _find_original_match(self, russian_term: str, original: str, translated: str) -> Optional[str]:
        """
        –ù–∞–π—Ç–∏ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ –≤ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–º —Ç–µ–∫—Å—Ç–µ
        –£–ø—Ä–æ—â–µ–Ω–Ω—ã–π –º–µ—Ç–æ–¥ - –º–æ–∂–Ω–æ —É–ª—É—á—à–∏—Ç—å —Å –ø–æ–º–æ—â—å—é ML
        """
        # –ù–∞—Ö–æ–¥–∏–º –ø–æ–∑–∏—Ü–∏—é —Ç–µ—Ä–º–∏–Ω–∞ –≤ –ø–µ—Ä–µ–≤–æ–¥–µ
        pos = translated.find(russian_term)
        if pos == -1:
            return None
        
        # –û—Ü–µ–Ω–∏–≤–∞–µ–º –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—É—é –ø–æ–∑–∏—Ü–∏—é (0-1)
        relative_pos = pos / len(translated)
        
        # –ò—â–µ–º –≤ –æ—Ä–∏–≥–∏–Ω–∞–ª–µ –≤ –ø–æ—Ö–æ–∂–µ–π –ø–æ–∑–∏—Ü–∏–∏
        search_start = int(len(original) * max(0, relative_pos - 0.1))
        search_end = int(len(original) * min(1, relative_pos + 0.1))
        search_window = original[search_start:search_end]
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –∫–∏—Ç–∞–π—Å–∫–∏–µ —Ç–µ—Ä–º–∏–Ω—ã –∏–∑ –æ–∫–Ω–∞
        chinese_terms = re.findall(r'[\u4e00-\u9fff]+', search_window)
        
        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –Ω–∞–∏–±–æ–ª–µ–µ –≤–µ—Ä–æ—è—Ç–Ω—ã–π (—Å–∞–º—ã–π –¥–ª–∏–Ω–Ω—ã–π —É–Ω–∏–∫–∞–ª—å–Ω—ã–π)
        if chinese_terms:
            term_counts = Counter(chinese_terms)
            # –ü—Ä–µ–¥–ø–æ—á–∏—Ç–∞–µ–º —Ç–µ—Ä–º–∏–Ω—ã –¥–ª–∏–Ω–æ–π 2-4 –∏–µ—Ä–æ–≥–ª–∏—Ñ–∞
            good_terms = [t for t in term_counts if 2 <= len(t) <= 4]
            if good_terms:
                return max(good_terms, key=lambda t: term_counts[t])
        
        return None
    
    def _categorize_term(self, term: str) -> str:
        """–û–ø—Ä–µ–¥–µ–ª–∏—Ç—å –∫–∞—Ç–µ–≥–æ—Ä–∏—é —Ç–µ—Ä–º–∏–Ω–∞"""
        term_lower = term.lower()
        
        if any(word in term_lower for word in ['–≥–æ—Ä–∞', '–¥–æ–ª–∏–Ω–∞', '–≥–æ—Ä–æ–¥', '–¥–µ—Ä–µ–≤–Ω—è', '–ø–µ—â–µ—Ä–∞', '–¥–≤–æ—Ä–µ—Ü']):
            return 'locations'
        elif any(word in term_lower for word in ['—Ç–µ—Ö–Ω–∏–∫–∞', '–∏—Å–∫—É—Å—Å—Ç–≤–æ', '–º–µ—Ç–æ–¥', '—Å—Ç–∏–ª—å']):
            return 'techniques'
        elif any(word in term_lower for word in ['–º–µ—á', '–ø–µ—á–∞—Ç—å', '–ø–∏–ª—é–ª—è', '—Å–æ–∫—Ä–æ–≤–∏—â–µ']):
            return 'artifacts'
        else:
            return 'terms'
    
    def _filter_by_frequency(self, chapter_terms: Dict, all_terms: Counter, min_freq: int) -> Dict:
        """–§–∏–ª—å—Ç—Ä–æ–≤–∞—Ç—å —Ç–µ—Ä–º–∏–Ω—ã –ø–æ —á–∞—Å—Ç–æ—Ç–µ –ø–æ—è–≤–ª–µ–Ω–∏—è"""
        filtered = {'characters': {}, 'locations': {}, 'terms': {}, 'techniques': {}, 'artifacts': {}}
        
        for chapter_num, terms in chapter_terms.items():
            for category in terms:
                for orig, trans in terms[category].items():
                    key = (orig, trans)
                    if all_terms[key] >= min_freq:
                        filtered[category][orig] = trans
        
        return filtered
    
    def save_to_database(self, extracted_glossary: Dict) -> int:
        """–°–æ—Ö—Ä–∞–Ω–∏—Ç—å –∏–∑–≤–ª–µ—á–µ–Ω–Ω—ã–π –≥–ª–æ—Å—Å–∞—Ä–∏–π –≤ –ë–î"""
        saved_count = 0
        
        for category, terms in extracted_glossary.items():
            for original, translation in terms.items():
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–µ—Ç –ª–∏ —É–∂–µ —Ç–∞–∫–æ–≥–æ —Ç–µ—Ä–º–∏–Ω–∞
                existing = GlossaryItem.query.filter_by(
                    novel_id=self.novel_id,
                    english_term=original,
                    is_active=True
                ).first()
                
                if not existing:
                    item = GlossaryItem(
                        novel_id=self.novel_id,
                        english_term=original,
                        russian_term=translation,
                        category=category,
                        is_auto_generated=True,
                        is_active=True
                    )
                    db.session.add(item)
                    saved_count += 1
        
        db.session.commit()
        print(f"üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ {saved_count} –Ω–æ–≤—ã—Ö —Ç–µ—Ä–º–∏–Ω–æ–≤ –≤ –≥–ª–æ—Å—Å–∞—Ä–∏–π")
        return saved_count


class GlossaryCopier:
    """–ö–æ–ø–∏—Ä–æ–≤–∞–Ω–∏–µ –≥–ª–æ—Å—Å–∞—Ä–∏—è –º–µ–∂–¥—É –∫–Ω–∏–≥–∞–º–∏"""
    
    @staticmethod
    def copy_glossary(source_novel_id: int, target_novel_id: int, 
                     categories: List[str] = None, theme_filter: str = None) -> int:
        """
        –ö–æ–ø–∏—Ä–æ–≤–∞—Ç—å –≥–ª–æ—Å—Å–∞—Ä–∏–π –∏–∑ –æ–¥–Ω–æ–π –∫–Ω–∏–≥–∏ –≤ –¥—Ä—É–≥—É—é
        
        Args:
            source_novel_id: ID –∫–Ω–∏–≥–∏-–∏—Å—Ç–æ—á–Ω–∏–∫–∞
            target_novel_id: ID —Ü–µ–ª–µ–≤–æ–π –∫–Ω–∏–≥–∏
            categories: –°–ø–∏—Å–æ–∫ –∫–∞—Ç–µ–≥–æ—Ä–∏–π –¥–ª—è –∫–æ–ø–∏—Ä–æ–≤–∞–Ω–∏—è (–µ—Å–ª–∏ None - –≤—Å–µ)
            theme_filter: –§–∏–ª—å—Ç—Ä –ø–æ —Ç–µ–º–∞—Ç–∏–∫–µ (–Ω–∞–ø—Ä–∏–º–µ—Ä, 'ballistics')
        """
        copied_count = 0
        
        # –ü–æ–ª—É—á–∞–µ–º —Ç–µ—Ä–º–∏–Ω—ã –∏–∑ –∏—Å—Ç–æ—á–Ω–∏–∫–∞
        query = GlossaryItem.query.filter_by(
            novel_id=source_novel_id,
            is_active=True
        )
        
        if categories:
            query = query.filter(GlossaryItem.category.in_(categories))
        
        source_items = query.all()
        
        for item in source_items:
            # –ü—Ä–∏–º–µ–Ω—è–µ–º —Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–π —Ñ–∏–ª—å—Ç—Ä
            if theme_filter and not GlossaryCopier._matches_theme(item, theme_filter):
                continue
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–µ—Ç –ª–∏ —É–∂–µ —Ç–∞–∫–æ–≥–æ —Ç–µ—Ä–º–∏–Ω–∞ –≤ —Ü–µ–ª–µ–≤–æ–π –∫–Ω–∏–≥–µ
            existing = GlossaryItem.query.filter_by(
                novel_id=target_novel_id,
                english_term=item.english_term,
                is_active=True
            ).first()
            
            if not existing:
                # –°–æ–∑–¥–∞–µ–º –∫–æ–ø–∏—é
                new_item = GlossaryItem(
                    novel_id=target_novel_id,
                    english_term=item.english_term,
                    russian_term=item.russian_term,
                    category=item.category,
                    description=item.description,
                    is_auto_generated=True,
                    is_active=True
                )
                db.session.add(new_item)
                copied_count += 1
        
        db.session.commit()
        return copied_count
    
    @staticmethod
    def _matches_theme(item: GlossaryItem, theme: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∏—Ç—å —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ —Ç–µ—Ä–º–∏–Ω–∞ —Ç–µ–º–∞—Ç–∏–∫–µ"""
        theme_keywords = {
            'ballistics': ['–ø—É–ª—è', '–≤—ã—Å—Ç—Ä–µ–ª', '–∫–∞–ª–∏–±—Ä', '—Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏—è', '–≤–∏–Ω—Ç–æ–≤–∫–∞', '–ø–∞—Ç—Ä–æ–Ω'],
            'cultivation': ['–∫—É–ª—å—Ç–∏–≤–∞—Ü–∏—è', '—Ü–∏', '–¥–∞–æ', '–º–µ—Ä–∏–¥–∏–∞–Ω', '–ø—Ä–æ—Ä—ã–≤'],
            'military': ['—Å–æ–ª–¥–∞—Ç', '–∞—Ä–º–∏—è', '–≥–µ–Ω–µ—Ä–∞–ª', '–±–∏—Ç–≤–∞', '–≤–æ–π–Ω–∞'],
            'medicine': ['–ª–µ–∫–∞—Ä—Å—Ç–≤–æ', '–ø–∏–ª—é–ª—è', '–≤—Ä–∞—á', '–ª–µ—á–µ–Ω–∏–µ', '–±–æ–ª–µ–∑–Ω—å']
        }
        
        keywords = theme_keywords.get(theme, [])
        term_text = f"{item.english_term} {item.russian_term} {item.description or ''}".lower()
        
        return any(kw in term_text for kw in keywords)
    
    @staticmethod
    def merge_glossaries(novel_id: int, source_glossaries: List[int]) -> int:
        """
        –û–±—ä–µ–¥–∏–Ω–∏—Ç—å –Ω–µ—Å–∫–æ–ª—å–∫–æ –≥–ª–æ—Å—Å–∞—Ä–∏–µ–≤ –≤ –æ–¥–∏–Ω
        
        Args:
            novel_id: ID —Ü–µ–ª–µ–≤–æ–π –∫–Ω–∏–≥–∏
            source_glossaries: –°–ø–∏—Å–æ–∫ ID –∫–Ω–∏–≥-–∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
        """
        merged_count = 0
        
        for source_id in source_glossaries:
            count = GlossaryCopier.copy_glossary(source_id, novel_id)
            merged_count += count
        
        return merged_count