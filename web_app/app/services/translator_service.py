"""
–£–ª—É—á—à–µ–Ω–Ω—ã–π —Å–µ—Ä–≤–∏—Å –ø–µ—Ä–µ–≤–æ–¥–∞ —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π —à–∞–±–ª–æ–Ω–æ–≤ –ø—Ä–æ–º–ø—Ç–æ–≤ –∏ –≥–ª–æ—Å—Å–∞—Ä–∏—è
"""
import os
import time
import json
import re
from typing import Dict, List, Optional, Tuple
import httpx
from httpx_socks import SyncProxyTransport
from app.models import Chapter, Translation, GlossaryItem, PromptTemplate
from app import db
from app.services.settings_service import SettingsService


class TranslatorConfig:
    """–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –ø–µ—Ä–µ–≤–æ–¥—á–∏–∫–∞"""
    def __init__(self, api_keys: List[str] = None, proxy_url: Optional[str] = None, 
                 model_name: str = None, temperature: float = None, max_output_tokens: int = None):
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –∏–∑ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö, –µ—Å–ª–∏ –Ω–µ –ø–µ—Ä–µ–¥–∞–Ω—ã
        self.api_keys = api_keys or SettingsService.get_gemini_api_keys()
        self.proxy_url = proxy_url or os.getenv('PROXY_URL')
        self.model_name = model_name or SettingsService.get_default_model()
        self.temperature = temperature or SettingsService.get_default_temperature()
        self.max_output_tokens = max_output_tokens or SettingsService.get_max_tokens()


class LLMTranslator:
    """–ü–µ—Ä–µ–≤–æ–¥—á–∏–∫ —á–µ—Ä–µ–∑ LLM API —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π –ø—Ä–æ–∫—Å–∏ –∏ —Ä–æ—Ç–∞—Ü–∏–∏ –∫–ª—é—á–µ–π"""

    def __init__(self, config: TranslatorConfig):
        self.config = config
        self.current_key_index = 0
        self.failed_keys = set()

        # HTTP –∫–ª–∏–µ–Ω—Ç —Å —É–≤–µ–ª–∏—á–µ–Ω–Ω—ã–º —Ç–∞–π–º–∞—É—Ç–æ–º
        timeout_config = httpx.Timeout(
            connect=30.0,      # –í—Ä–µ–º—è –Ω–∞ —É—Å—Ç–∞–Ω–æ–≤–∫—É —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è
            read=300.0,        # –í—Ä–µ–º—è –Ω–∞ —á—Ç–µ–Ω–∏–µ –æ—Ç–≤–µ—Ç–∞ (5 –º–∏–Ω—É—Ç –¥–ª—è –±–æ–ª—å—à–∏—Ö –ø–µ—Ä–µ–≤–æ–¥–æ–≤)
            write=30.0,        # –í—Ä–µ–º—è –Ω–∞ –æ—Ç–ø—Ä–∞–≤–∫—É –∑–∞–ø—Ä–æ—Å–∞
            pool=30.0          # –í—Ä–µ–º—è –æ–∂–∏–¥–∞–Ω–∏—è —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è –∏–∑ –ø—É–ª–∞
        )

        if config.proxy_url:
            self.transport = SyncProxyTransport.from_url(config.proxy_url)
            self.client = httpx.Client(transport=self.transport, timeout=timeout_config)
        else:
            self.client = httpx.Client(timeout=timeout_config)

        self.api_url = f"https://generativelanguage.googleapis.com/v1beta/models/{config.model_name}:generateContent"

    @property
    def current_key(self) -> str:
        return self.config.api_keys[self.current_key_index]

    def switch_to_next_key(self):
        """–ü–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏–µ –Ω–∞ —Å–ª–µ–¥—É—é—â–∏–π –∫–ª—é—á"""
        self.current_key_index = (self.current_key_index + 1) % len(self.config.api_keys)
        print(f"  ‚Üª –ü–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏–µ –Ω–∞ –∫–ª—é—á #{self.current_key_index + 1}")

    def mark_key_as_failed(self):
        """–ü–æ–º–µ—á–∞–µ–º —Ç–µ–∫—É—â–∏–π –∫–ª—é—á –∫–∞–∫ –Ω–µ—Ä–∞–±–æ—Ç–∞—é—â–∏–π"""
        self.failed_keys.add(self.current_key_index)
        print(f"  ‚ùå –ö–ª—é—á #{self.current_key_index + 1} –ø–æ–º–µ—á–µ–Ω –∫–∞–∫ –Ω–µ—Ä–∞–±–æ—Ç–∞—é—â–∏–π")

    def reset_failed_keys(self):
        """–°–±—Ä–æ—Å —Å–ø–∏—Å–∫–∞ –Ω–µ—Ä–∞–±–æ—Ç–∞—é—â–∏—Ö –∫–ª—é—á–µ–π"""
        self.failed_keys.clear()
        print("  üîÑ –°–±—Ä–æ—Å —Å–ø–∏—Å–∫–∞ –Ω–µ—Ä–∞–±–æ—Ç–∞—é—â–∏—Ö –∫–ª—é—á–µ–π")

    def all_keys_failed(self) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ–º, –≤—Å–µ –ª–∏ –∫–ª—é—á–∏ –Ω–µ—Ä–∞–±–æ—Ç–∞—é—â–∏–µ"""
        return len(self.failed_keys) == len(self.config.api_keys)

    def make_request(self, system_prompt: str, user_prompt: str, temperature: float = None) -> Optional[str]:
        """–ë–∞–∑–æ–≤—ã–π –º–µ—Ç–æ–¥ –¥–ª—è –∑–∞–ø—Ä–æ—Å–æ–≤ –∫ API —Å —É–º–Ω–æ–π —Ä–æ—Ç–∞—Ü–∏–µ–π –∫–ª—é—á–µ–π"""
        generation_config = {
            "temperature": temperature or self.config.temperature,
            "topP": 0.95,
            "topK": 40,
            "maxOutputTokens": self.config.max_output_tokens
        }

        max_attempts = len(self.config.api_keys) * 2

        for attempt in range(max_attempts):
            # –ï—Å–ª–∏ —Ç–µ–∫—É—â–∏–π –∫–ª—é—á –≤ —Å–ø–∏—Å–∫–µ –Ω–µ—Ä–∞–±–æ—Ç–∞—é—â–∏—Ö, –ø–µ—Ä–µ–∫–ª—é—á–∞–µ–º—Å—è
            if self.current_key_index in self.failed_keys:
                self.switch_to_next_key()
                continue

            try:
                print(f"   –ò—Å–ø–æ–ª—å–∑—É–µ–º –∫–ª—é—á #{self.current_key_index + 1} –∏–∑ {len(self.config.api_keys)}")

                response = self.client.post(
                    self.api_url,
                    params={"key": self.current_key},
                    headers={"Content-Type": "application/json"},
                    json={
                        "generationConfig": generation_config,
                        "contents": [{
                            "parts": [
                                {"text": system_prompt},
                                {"text": user_prompt}
                            ]
                        }]
                    }
                )

                if response.status_code == 200:
                    data = response.json()
                    if 'candidates' in data and data['candidates']:
                        content = data['candidates'][0]['content']
                        if 'parts' in content and content['parts']:
                            return content['parts'][0]['text']
                    
                    print(f"   ‚ö†Ô∏è –ù–µ–æ–∂–∏–¥–∞–Ω–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç –æ—Ç–≤–µ—Ç–∞: {data}")
                    self.mark_key_as_failed()
                    
                elif response.status_code == 429:  # Rate limit
                    print(f"   ‚è≥ Rate limit –¥–ª—è –∫–ª—é—á–∞ #{self.current_key_index + 1}")
                    self.mark_key_as_failed()
                    
                elif response.status_code == 400:  # Bad request
                    print(f"   ‚ùå –û—à–∏–±–∫–∞ –∑–∞–ø—Ä–æ—Å–∞ –¥–ª—è –∫–ª—é—á–∞ #{self.current_key_index + 1}: {response.text}")
                    self.mark_key_as_failed()
                    
                else:
                    print(f"   ‚ùå HTTP {response.status_code} –¥–ª—è –∫–ª—é—á–∞ #{self.current_key_index + 1}")
                    self.mark_key_as_failed()

            except Exception as e:
                print(f"   ‚ùå –û—à–∏–±–∫–∞ –¥–ª—è –∫–ª—é—á–∞ #{self.current_key_index + 1}: {e}")
                self.mark_key_as_failed()

            # –ü–∞—É–∑–∞ –ø–µ—Ä–µ–¥ —Å–ª–µ–¥—É—é—â–µ–π –ø–æ–ø—ã—Ç–∫–æ–π
            time.sleep(2)

        print("   ‚ùå –í—Å–µ –∫–ª—é—á–∏ –∏—Å—á–µ—Ä–ø–∞–Ω—ã")
        return None

    def translate_text(self, text: str, system_prompt: str, context: str = "") -> Optional[str]:
        """–ü–µ—Ä–µ–≤–æ–¥ —Ç–µ–∫—Å—Ç–∞ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –∫–∞—Å—Ç–æ–º–Ω–æ–≥–æ –ø—Ä–æ–º–ø—Ç–∞"""
        user_prompt = f"{context}\n\n–¢–ï–ö–°–¢ –î–õ–Ø –ü–ï–†–ï–í–û–î–ê:\n{text}"
        return self.make_request(system_prompt, user_prompt)

    def generate_summary(self, text: str, summary_prompt: str) -> Optional[str]:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ä–µ–∑—é–º–µ –≥–ª–∞–≤—ã"""
        user_prompt = f"–¢–ï–ö–°–¢ –ì–õ–ê–í–´:\n{text}"
        return self.make_request(summary_prompt, user_prompt, temperature=0.3)

    def extract_terms(self, text: str, extraction_prompt: str, existing_glossary: Dict) -> Optional[str]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –Ω–æ–≤—ã—Ö —Ç–µ—Ä–º–∏–Ω–æ–≤ –∏–∑ —Ç–µ–∫—Å—Ç–∞"""
        glossary_text = self.format_glossary_for_prompt(existing_glossary)
        user_prompt = f"–°–£–©–ï–°–¢–í–£–Æ–©–ò–ô –ì–õ–û–°–°–ê–†–ò–ô:\n{glossary_text}\n\n–¢–ï–ö–°–¢ –î–õ–Ø –ê–ù–ê–õ–ò–ó–ê:\n{text}"
        return self.make_request(extraction_prompt, user_prompt, temperature=0.2)


class TranslationContext:
    """–ö–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è –ø–µ—Ä–µ–≤–æ–¥–∞ –≥–ª–∞–≤—ã"""
    
    def __init__(self, novel_id: int):
        self.novel_id = novel_id
        self.previous_summaries = []
        self.glossary = {}
        self._load_context()
    
    def _load_context(self):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –∏–∑ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö"""
        # –ó–∞–≥—Ä—É–∂–∞–µ–º —Ä–µ–∑—é–º–µ –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö –≥–ª–∞–≤ (–¥–æ 5)
        from app.models import Chapter
        chapters = Chapter.query.filter_by(
            novel_id=self.novel_id, 
            status='translated'
        ).order_by(Chapter.chapter_number.desc()).limit(5).all()
        
        for chapter in reversed(chapters):
            if chapter.current_translation and chapter.current_translation.summary:
                self.previous_summaries.append({
                    'chapter': chapter.chapter_number,
                    'summary': chapter.current_translation.summary
                })
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –≥–ª–æ—Å—Å–∞—Ä–∏–π
        self.glossary = GlossaryItem.get_glossary_dict(self.novel_id)
    
    def build_context_prompt(self) -> str:
        """–ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –¥–ª—è –ø—Ä–æ–º–ø—Ç–∞"""
        lines = []

        # –î–æ–±–∞–≤–ª—è–µ–º —Ä–µ–∑—é–º–µ –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö –≥–ª–∞–≤
        if self.previous_summaries:
            lines.append("–ö–û–ù–¢–ï–ö–°–¢ –ü–†–ï–î–´–î–£–©–ò–• –ì–õ–ê–í:")
            lines.append("=" * 50)
            for item in self.previous_summaries:
                lines.append(f"\n–ì–ª–∞–≤–∞ {item['chapter']}:")
                lines.append(item['summary'])
            lines.append("\n" + "=" * 50 + "\n")

        # –î–æ–±–∞–≤–ª—è–µ–º –≥–ª–æ—Å—Å–∞—Ä–∏–π
        if self.glossary['characters']:
            lines.append("–£–°–¢–ê–ù–û–í–õ–ï–ù–ù–´–ï –ü–ï–†–ï–í–û–î–´ –ò–ú–Å–ù:")
            for eng, rus in sorted(self.glossary['characters'].items()):
                lines.append(f"- {eng} ‚Üí {rus}")
            lines.append("")

        if self.glossary['locations']:
            lines.append("–£–°–¢–ê–ù–û–í–õ–ï–ù–ù–´–ï –ü–ï–†–ï–í–û–î–´ –õ–û–ö–ê–¶–ò–ô:")
            for eng, rus in sorted(self.glossary['locations'].items()):
                lines.append(f"- {eng} ‚Üí {rus}")
            lines.append("")

        if self.glossary['terms']:
            lines.append("–£–°–¢–ê–ù–û–í–õ–ï–ù–ù–´–ï –ü–ï–†–ï–í–û–î–´ –¢–ï–†–ú–ò–ù–û–í:")
            for eng, rus in sorted(self.glossary['terms'].items()):
                lines.append(f"- {eng} ‚Üí {rus}")
            lines.append("")

        return "\n".join(lines)


class TranslatorService:
    """–û—Å–Ω–æ–≤–Ω–æ–π —Å–µ—Ä–≤–∏—Å –ø–µ—Ä–µ–≤–æ–¥–∞"""

    def __init__(self, config: Dict = None):
        self.config = TranslatorConfig(**config) if config else TranslatorConfig()
        self.translator = LLMTranslator(self.config)

    def translate_chapter(self, chapter: Chapter) -> bool:
        """–ü–µ—Ä–µ–≤–æ–¥ –≥–ª–∞–≤—ã —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º —à–∞–±–ª–æ–Ω–∞ –ø—Ä–æ–º–ø—Ç–∞ –∏ –≥–ª–æ—Å—Å–∞—Ä–∏—è"""
        print(f"üîÑ –ü–µ—Ä–µ–≤–æ–¥ –≥–ª–∞–≤—ã {chapter.chapter_number}: {chapter.original_title}")
        
        try:
            # –ü–æ–ª—É—á–∞–µ–º —à–∞–±–ª–æ–Ω –ø—Ä–æ–º–ø—Ç–∞ –¥–ª—è –Ω–æ–≤–µ–ª–ª—ã
            prompt_template = chapter.novel.get_prompt_template()
            if not prompt_template:
                print("‚ùå –ù–µ –Ω–∞–π–¥–µ–Ω —à–∞–±–ª–æ–Ω –ø—Ä–æ–º–ø—Ç–∞")
                return False
            
            # –°–æ–∑–¥–∞–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç –ø–µ—Ä–µ–≤–æ–¥–∞
            context = TranslationContext(chapter.novel_id)
            context_prompt = context.build_context_prompt()
            
            # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º —Ç–µ–∫—Å—Ç –¥–ª—è –ø–µ—Ä–µ–≤–æ–¥–∞
            text_to_translate = self.preprocess_text(chapter.original_text)
            
            # –†–∞–∑–±–∏–≤–∞–µ–º –¥–ª–∏–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç –Ω–∞ —á–∞—Å—Ç–∏
            text_parts = self.split_long_text(text_to_translate)
            
            translated_parts = []
            
            for i, part in enumerate(text_parts):
                print(f"   üìù –ü–µ—Ä–µ–≤–æ–¥ —á–∞—Å—Ç–∏ {i+1}/{len(text_parts)}")
                
                # –ü–µ—Ä–µ–≤–æ–¥–∏–º —á–∞—Å—Ç—å
                translated_part = self.translator.translate_text(
                    part, 
                    prompt_template.translation_prompt,
                    context_prompt
                )
                
                if not translated_part:
                    print(f"   ‚ùå –û—à–∏–±–∫–∞ –ø–µ—Ä–µ–≤–æ–¥–∞ —á–∞—Å—Ç–∏ {i+1}")
                    return False
                
                translated_parts.append(translated_part)
                time.sleep(1)  # –ü–∞—É–∑–∞ –º–µ–∂–¥—É —á–∞—Å—Ç—è–º–∏
            
            # –û–±—ä–µ–¥–∏–Ω—è–µ–º —á–∞—Å—Ç–∏
            full_translation = "\n\n".join(translated_parts)
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫ –∏ –æ—Å–Ω–æ–≤–Ω–æ–π —Ç–µ–∫—Å—Ç
            title, content = self.extract_title_and_content(full_translation)
            
            # –í–∞–ª–∏–¥–∏—Ä—É–µ–º –ø–µ—Ä–µ–≤–æ–¥
            validation = self.validate_translation(chapter.original_text, content, chapter.chapter_number)
            if validation['critical']:
                print(f"   ‚ö†Ô∏è –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–æ–±–ª–µ–º—ã –≤ –ø–µ—Ä–µ–≤–æ–¥–µ: {validation['critical_issues']}")
                return False
            
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Ä–µ–∑—é–º–µ
            summary = None
            if prompt_template.summary_prompt:
                summary = self.translator.generate_summary(content, prompt_template.summary_prompt)
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –Ω–æ–≤—ã–µ —Ç–µ—Ä–º–∏–Ω—ã
            if prompt_template.terms_extraction_prompt:
                new_terms = self.extract_new_terms(content, prompt_template.terms_extraction_prompt, context.glossary)
                if new_terms:
                    self.save_new_terms(new_terms, chapter.novel_id, chapter.chapter_number)
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø–µ—Ä–µ–≤–æ–¥
            translation = Translation(
                chapter_id=chapter.id,
                translated_title=title,
                translated_text=content,
                summary=summary,
                quality_score=self.calculate_quality_score(validation),
                translation_time=time.time(),
                metadata={
                    'template_used': prompt_template.name,
                    'validation': validation,
                    'parts_count': len(text_parts)
                }
            )
            
            db.session.add(translation)
            chapter.status = 'translated'
            db.session.commit()
            
            print(f"   ‚úÖ –ì–ª–∞–≤–∞ {chapter.chapter_number} –ø–µ—Ä–µ–≤–µ–¥–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ")
            return True
            
        except Exception as e:
            print(f"   ‚ùå –û—à–∏–±–∫–∞ –ø–µ—Ä–µ–≤–æ–¥–∞ –≥–ª–∞–≤—ã {chapter.chapter_number}: {e}")
            db.session.rollback()
            return False

    def preprocess_text(self, text: str) -> str:
        """–ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞ –¥–ª—è –ø–µ—Ä–µ–≤–æ–¥–∞"""
        # –£–¥–∞–ª—è–µ–º –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã
        text = re.sub(r'\s+', ' ', text)
        
        # –£–¥–∞–ª—è–µ–º –ø–æ–≤—Ç–æ—Ä—è—é—â–∏–µ—Å—è —Å–∏–º–≤–æ–ª—ã (–±–æ–ª–µ–µ 3 –ø–æ–¥—Ä—è–¥)
        text = re.sub(r'(.)\1{3,}', r'\1\1\1', text)
        
        return text.strip()

    def split_long_text(self, text: str, max_words: int = 1200) -> List[str]:
        """–†–∞–∑–±–∏–≤–∞–µ—Ç –¥–ª–∏–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç –Ω–∞ —á–∞—Å—Ç–∏ —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º —Ü–µ–ª–æ—Å—Ç–Ω–æ—Å—Ç–∏ –∞–±–∑–∞—Ü–µ–≤"""
        paragraphs = text.split('\n\n')
        parts = []
        current_part = []
        current_words = 0
        
        for paragraph in paragraphs:
            paragraph_words = len(paragraph.split())
            
            if current_words + paragraph_words > max_words and current_part:
                parts.append('\n\n'.join(current_part))
                current_part = [paragraph]
                current_words = paragraph_words
            else:
                current_part.append(paragraph)
                current_words += paragraph_words
        
        if current_part:
            parts.append('\n\n'.join(current_part))
        
        return parts

    def extract_title_and_content(self, translated_text: str) -> Tuple[str, str]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∑–∞–≥–æ–ª–æ–≤–∫–∞ –∏ –æ—Å–Ω–æ–≤–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞ –∏–∑ –ø–µ—Ä–µ–≤–æ–¥–∞"""
        lines = translated_text.strip().split('\n')
        
        if lines:
            title = lines[0].strip()
            content = '\n'.join(lines[1:]).strip()
            return title, content
        
        return "", translated_text

    def validate_translation(self, original: str, translated: str, chapter_num: int) -> Dict:
        """–í–∞–ª–∏–¥–∞—Ü–∏—è –∫–∞—á–µ—Å—Ç–≤–∞ –ø–µ—Ä–µ–≤–æ–¥–∞"""
        issues = []
        warnings = []
        critical_issues = []

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–æ–æ—Ç–Ω–æ—à–µ–Ω–∏—è –¥–ª–∏–Ω—ã
        orig_len = len(original)
        trans_len = len(translated)
        ratio = trans_len / orig_len if orig_len > 0 else 0

        if ratio < 0.6:
            critical_issues.append(f"–ü–µ—Ä–µ–≤–æ–¥ —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–π: {ratio:.2f} –æ—Ç –æ—Ä–∏–≥–∏–Ω–∞–ª–∞")
        elif ratio < 0.9:
            issues.append(f"–ü–µ—Ä–µ–≤–æ–¥ –∫–æ—Ä–æ—Ç–∫–∏–π: {ratio:.2f} –æ—Ç –æ—Ä–∏–≥–∏–Ω–∞–ª–∞")
        elif ratio > 1.6:
            warnings.append(f"–ü–µ—Ä–µ–≤–æ–¥ —Å–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω—ã–π: {ratio:.2f} –æ—Ç –æ—Ä–∏–≥–∏–Ω–∞–ª–∞")

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –∞–±–∑–∞—Ü–µ–≤
        orig_paragraphs = len([p for p in original.split('\n\n') if p.strip()])
        trans_paragraphs = len([p for p in translated.split('\n\n') if p.strip()])

        para_diff = abs(orig_paragraphs - trans_paragraphs)
        para_ratio = trans_paragraphs / orig_paragraphs if orig_paragraphs > 0 else 0

        if para_ratio < 0.6:
            critical_issues.append(f"–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è —Ä–∞–∑–Ω–∏—Ü–∞ –≤ –∞–±–∑–∞—Ü–∞—Ö: {orig_paragraphs} ‚Üí {trans_paragraphs}")
        elif para_diff > 2:
            issues.append(f"–†–∞–∑–Ω–∏—Ü–∞ –≤ –∫–æ–ª–∏—á–µ—Å—Ç–≤–µ –∞–±–∑–∞—Ü–µ–≤: {orig_paragraphs} ‚Üí {trans_paragraphs}")

        return {
            'valid': len(issues) == 0 and len(critical_issues) == 0,
            'critical': len(critical_issues) > 0,
            'issues': issues,
            'warnings': warnings,
            'critical_issues': critical_issues,
            'stats': {
                'length_ratio': ratio,
                'paragraph_diff': para_diff,
                'paragraph_ratio': para_ratio
            }
        }

    def calculate_quality_score(self, validation: Dict) -> int:
        """–†–∞—Å—á–µ—Ç –æ—Ü–µ–Ω–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –ø–µ—Ä–µ–≤–æ–¥–∞"""
        score = 10
        
        # –®—Ç—Ä–∞—Ñ—ã –∑–∞ –ø—Ä–æ–±–ª–µ–º—ã
        score -= len(validation['critical_issues']) * 3
        score -= len(validation['issues']) * 1
        score -= len(validation['warnings']) * 0.5
        
        # –ë–æ–Ω—É—Å—ã –∑–∞ —Ö–æ—Ä–æ—à–∏–µ –ø–æ–∫–∞–∑–∞—Ç–µ–ª–∏
        if validation['stats']['length_ratio'] >= 0.9 and validation['stats']['length_ratio'] <= 1.3:
            score += 1
        
        return max(1, min(10, int(score)))

    def extract_new_terms(self, text: str, extraction_prompt: str, existing_glossary: Dict) -> Optional[Dict]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –Ω–æ–≤—ã—Ö —Ç–µ—Ä–º–∏–Ω–æ–≤ –∏–∑ –ø–µ—Ä–µ–≤–µ–¥–µ–Ω–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞"""
        result = self.translator.extract_terms(text, extraction_prompt, existing_glossary)
        if not result:
            return None
        
        return self.parse_extraction_result(result)

    def parse_extraction_result(self, text: str) -> Dict:
        """–ü–∞—Ä—Å–∏–Ω–≥ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Ç–µ—Ä–º–∏–Ω–æ–≤"""
        result = {'characters': {}, 'locations': {}, 'terms': {}, 'techniques': {}, 'artifacts': {}}
        
        current_section = None
        for line in text.split('\n'):
            line = line.strip()
            
            if '–ü–ï–†–°–û–ù–ê–ñ–ò:' in line:
                current_section = 'characters'
            elif '–õ–û–ö–ê–¶–ò–ò:' in line:
                current_section = 'locations'
            elif '–¢–ï–†–ú–ò–ù–´:' in line:
                current_section = 'terms'
            elif '–¢–ï–•–ù–ò–ö–ò:' in line:
                current_section = 'techniques'
            elif '–ê–†–¢–ï–§–ê–ö–¢–´:' in line:
                current_section = 'artifacts'
            elif line.startswith('- ') and current_section:
                if '–Ω–µ—Ç –Ω–æ–≤—ã—Ö' in line.lower():
                    continue
                
                parts = line[2:].split(' = ')
                if len(parts) == 2:
                    eng, rus = parts[0].strip(), parts[1].strip()
                    if eng and rus and eng != rus:
                        result[current_section][eng] = rus
        
        return result

    def save_new_terms(self, new_terms: Dict, novel_id: int, chapter_number: int):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –Ω–æ–≤—ã—Ö —Ç–µ—Ä–º–∏–Ω–æ–≤ –≤ –≥–ª–æ—Å—Å–∞—Ä–∏–π"""
        for category, terms in new_terms.items():
            for eng, rus in terms.items():
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–µ—Ç –ª–∏ —É–∂–µ —Ç–∞–∫–æ–≥–æ —Ç–µ—Ä–º–∏–Ω–∞
                existing = GlossaryItem.query.filter_by(
                    novel_id=novel_id,
                    english_term=eng,
                    is_active=True
                ).first()
                
                if not existing:
                    glossary_item = GlossaryItem(
                        novel_id=novel_id,
                        english_term=eng,
                        russian_term=rus,
                        category=category,
                        first_appearance_chapter=chapter_number,
                        is_auto_generated=True
                    )
                    db.session.add(glossary_item)
        
        db.session.commit()
        print(f"   üìö –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ {sum(len(terms) for terms in new_terms.values())} –Ω–æ–≤—ã—Ö —Ç–µ—Ä–º–∏–Ω–æ–≤") 