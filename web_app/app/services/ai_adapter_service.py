"""
–ê–¥–∞–ø—Ç–µ—Ä –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å —Ä–∞–∑–Ω—ã–º–∏ AI –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞–º–∏ —á–µ—Ä–µ–∑ —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å
"""
import httpx
import json
import logging
from typing import Dict, List, Optional
from app.models.ai_model import AIModel
from app.services.ai_model_service import AIModelService
from app.services.log_service import LogService

logger = logging.getLogger(__name__)


class AIAdapterService:
    """–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –∞–¥–∞–ø—Ç–µ—Ä –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å —Ä–∞–∑–Ω—ã–º–∏ AI –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞–º–∏"""

    def __init__(self, model_id: int = None, model_name: str = None):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∞–¥–∞–ø—Ç–µ—Ä–∞
        Args:
            model_id: ID –º–æ–¥–µ–ª–∏ –∏–∑ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö
            model_name: –ò–º—è –º–æ–¥–µ–ª–∏ (–∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–∞ ID)
        """
        if model_id:
            self.model = AIModelService.get_model_by_id(model_id)
        elif model_name:
            self.model = AIModel.query.filter_by(name=model_name).first()
        else:
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –º–æ–¥–µ–ª—å –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
            self.model = AIModel.query.filter_by(is_default=True).first()

        if not self.model:
            raise ValueError("AI –º–æ–¥–µ–ª—å –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")

        logger.info(f"–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω –∞–¥–∞–ø—Ç–µ—Ä –¥–ª—è –º–æ–¥–µ–ª–∏: {self.model.name} ({self.model.provider})")

    async def generate_content(self, system_prompt: str, user_prompt: str,
                              temperature: float = None, max_tokens: int = None) -> Dict:
        """
        –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∫–æ–Ω—Ç–µ–Ω—Ç–∞ —á–µ—Ä–µ–∑ –≤—ã–±—Ä–∞–Ω–Ω—É—é –º–æ–¥–µ–ª—å
        Returns:
            Dict —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–º: {'success': bool, 'content': str, 'error': str}
        """
        temperature = temperature or self.model.default_temperature
        max_tokens = max_tokens or self.model.max_output_tokens

        try:
            if self.model.provider == 'gemini':
                return await self._call_gemini(system_prompt, user_prompt, temperature, max_tokens)
            elif self.model.provider == 'openai':
                return await self._call_openai(system_prompt, user_prompt, temperature, max_tokens)
            elif self.model.provider == 'anthropic':
                return await self._call_anthropic(system_prompt, user_prompt, temperature, max_tokens)
            elif self.model.provider == 'ollama':
                return await self._call_ollama(system_prompt, user_prompt, temperature, max_tokens)
            else:
                return {'success': False, 'error': f'–ù–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–π –ø—Ä–æ–≤–∞–π–¥–µ—Ä: {self.model.provider}'}

        except Exception as e:
            import traceback
            logger.error(f"–û—à–∏–±–∫–∞ –≤—ã–∑–æ–≤–∞ –º–æ–¥–µ–ª–∏ {self.model.name}: {e}")
            logger.error(f"Traceback: {traceback.format_exc()}")
            error_msg = str(e) if str(e) else f"{type(e).__name__}: {repr(e)}"
            return {'success': False, 'error': error_msg}

    async def _call_gemini(self, system_prompt: str, user_prompt: str,
                          temperature: float, max_tokens: int) -> Dict:
        """–í—ã–∑–æ–≤ Gemini API"""
        if not self.model.api_key:
            return {'success': False, 'error': 'API –∫–ª—é—á –Ω–µ —É–∫–∞–∑–∞–Ω'}

        # –£–≤–µ–ª–∏—á–µ–Ω–Ω—ã–π —Ç–∞–π–º–∞—É—Ç –¥–ª—è Gemini (–±–æ–ª—å—à–∏–µ —Ç–µ–∫—Å—Ç—ã —Ç—Ä–µ–±—É—é—Ç –≤—Ä–µ–º–µ–Ω–∏)
        async with httpx.AsyncClient(timeout=300.0) as client:
            url = f"{self.model.api_endpoint}/models/{self.model.model_id}:generateContent"
            
            # –õ–æ–≥–∏—Ä—É–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∑–∞–ø—Ä–æ—Å–∞
            actual_max_tokens = min(max_tokens, self.model.max_output_tokens)
            LogService.log_info(f"Gemini –∑–∞–ø—Ä–æ—Å: {self.model.model_id} | Temperature: {temperature} | Max tokens: {actual_max_tokens:,} / {self.model.max_output_tokens:,}")

            response = await client.post(
                url,
                params={'key': self.model.api_key},
                json={
                    'contents': [{
                        'parts': [
                            {'text': system_prompt},
                            {'text': user_prompt}
                        ]
                    }],
                    'generationConfig': {
                        'temperature': temperature,
                        'maxOutputTokens': actual_max_tokens,
                        'topP': 0.95,
                        'topK': 40
                    },
                    'safetySettings': [
                        {'category': 'HARM_CATEGORY_HATE_SPEECH', 'threshold': 'BLOCK_NONE'},
                        {'category': 'HARM_CATEGORY_SEXUALLY_EXPLICIT', 'threshold': 'BLOCK_NONE'},
                        {'category': 'HARM_CATEGORY_HARASSMENT', 'threshold': 'BLOCK_NONE'},
                        {'category': 'HARM_CATEGORY_DANGEROUS_CONTENT', 'threshold': 'BLOCK_NONE'}
                    ]
                }
            )

            if response.status_code == 200:
                data = response.json()

                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –±–ª–æ–∫–∏—Ä–æ–≤–∫—É –ø—Ä–æ–º–ø—Ç–∞
                if 'promptFeedback' in data and data['promptFeedback'].get('blockReason'):
                    return {
                        'success': False,
                        'error': f"–ü—Ä–æ–º–ø—Ç –∑–∞–±–ª–æ–∫–∏—Ä–æ–≤–∞–Ω: {data['promptFeedback']['blockReason']}"
                    }

                # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–∫—Å—Ç –∏–∑ –æ—Ç–≤–µ—Ç–∞
                candidates = data.get('candidates', [])
                if candidates:
                    content = candidates[0].get('content', {}).get('parts', [{}])[0].get('text', '')
                    return {
                        'success': True,
                        'content': content,
                        'usage': data.get('usageMetadata', {}),
                        'finish_reason': candidates[0].get('finishReason', 'UNKNOWN')
                    }
                else:
                    return {'success': False, 'error': '–ù–µ—Ç –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ –≤ –æ—Ç–≤–µ—Ç–µ'}

            elif response.status_code == 429:
                return {'success': False, 'error': 'Rate limit –ø—Ä–µ–≤—ã—à–µ–Ω', 'retry_after': 60}
            else:
                error_data = response.json()
                return {
                    'success': False,
                    'error': error_data.get('error', {}).get('message', f'HTTP {response.status_code}')
                }

    async def _call_openai(self, system_prompt: str, user_prompt: str,
                           temperature: float, max_tokens: int) -> Dict:
        """–í—ã–∑–æ–≤ OpenAI API"""
        if not self.model.api_key:
            return {'success': False, 'error': 'API –∫–ª—é—á –Ω–µ —É–∫–∞–∑–∞–Ω'}

        # –õ–æ–≥–∏—Ä—É–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∑–∞–ø—Ä–æ—Å–∞
        actual_max_tokens = min(max_tokens, self.model.max_output_tokens)
        LogService.log_info(f"OpenAI –∑–∞–ø—Ä–æ—Å: {self.model.model_id} | Temperature: {temperature} | Max tokens: {actual_max_tokens:,} / {self.model.max_output_tokens:,}")

        async with httpx.AsyncClient(timeout=60.0) as client:
            response = await client.post(
                f"{self.model.api_endpoint}/chat/completions",
                headers={
                    'Authorization': f'Bearer {self.model.api_key}',
                    'Content-Type': 'application/json'
                },
                json={
                    'model': self.model.model_id,
                    'messages': [
                        {'role': 'system', 'content': system_prompt},
                        {'role': 'user', 'content': user_prompt}
                    ],
                    'temperature': temperature,
                    'max_tokens': actual_max_tokens
                }
            )

            if response.status_code == 200:
                data = response.json()
                choices = data.get('choices', [])
                if choices:
                    return {
                        'success': True,
                        'content': choices[0].get('message', {}).get('content', ''),
                        'usage': data.get('usage', {}),
                        'finish_reason': choices[0].get('finish_reason', 'unknown')
                    }
                else:
                    return {'success': False, 'error': '–ù–µ—Ç –≤–∞—Ä–∏–∞–Ω—Ç–æ–≤ –≤ –æ—Ç–≤–µ—Ç–µ'}
            else:
                error_data = response.json()
                return {
                    'success': False,
                    'error': error_data.get('error', {}).get('message', f'HTTP {response.status_code}')
                }

    async def _call_anthropic(self, system_prompt: str, user_prompt: str,
                              temperature: float, max_tokens: int) -> Dict:
        """–í—ã–∑–æ–≤ Anthropic API"""
        if not self.model.api_key:
            return {'success': False, 'error': 'API –∫–ª—é—á –Ω–µ —É–∫–∞–∑–∞–Ω'}

        # –õ–æ–≥–∏—Ä—É–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∑–∞–ø—Ä–æ—Å–∞
        actual_max_tokens = min(max_tokens, self.model.max_output_tokens)
        LogService.log_info(f"Anthropic –∑–∞–ø—Ä–æ—Å: {self.model.model_id} | Temperature: {temperature} | Max tokens: {actual_max_tokens:,} / {self.model.max_output_tokens:,}")

        async with httpx.AsyncClient(timeout=60.0) as client:
            response = await client.post(
                f"{self.model.api_endpoint}/messages",
                headers={
                    'x-api-key': self.model.api_key,
                    'anthropic-version': '2023-06-01',
                    'Content-Type': 'application/json'
                },
                json={
                    'model': self.model.model_id,
                    'system': system_prompt,
                    'messages': [
                        {'role': 'user', 'content': user_prompt}
                    ],
                    'temperature': temperature,
                    'max_tokens': actual_max_tokens
                }
            )

            if response.status_code == 200:
                data = response.json()
                content_blocks = data.get('content', [])
                if content_blocks:
                    return {
                        'success': True,
                        'content': content_blocks[0].get('text', ''),
                        'usage': data.get('usage', {}),
                        'finish_reason': data.get('stop_reason', 'unknown')
                    }
                else:
                    return {'success': False, 'error': '–ù–µ—Ç –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –≤ –æ—Ç–≤–µ—Ç–µ'}
            else:
                error_data = response.json()
                return {
                    'success': False,
                    'error': error_data.get('error', {}).get('message', f'HTTP {response.status_code}')
                }

    async def _call_ollama(self, system_prompt: str, user_prompt: str,
                           temperature: float, max_tokens: int) -> Dict:
        """–í—ã–∑–æ–≤ Ollama API —Å –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–º —Ä–∞—Å—á–µ—Ç–æ–º —Ä–∞–∑–º–µ—Ä–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –º–æ–¥–µ–ª–∏"""
        # –£–≤–µ–ª–∏—á–µ–Ω–Ω—ã–π —Ç–∞–π–º–∞—É—Ç –¥–ª—è Ollama (–±–æ–ª—å—à–∏–µ –º–æ–¥–µ–ª–∏ —Ç—Ä–µ–±—É—é—Ç –≤—Ä–µ–º–µ–Ω–∏ –Ω–∞ –∑–∞–≥—Ä—É–∑–∫—É)
        try:
            async with httpx.AsyncClient(timeout=600.0) as client:
                # –°–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏
                try:
                    models_response = await client.get(f"{self.model.api_endpoint.rstrip('/api')}/api/tags")
                    if models_response.status_code == 200:
                        models_data = models_response.json()
                        available_models = [m['name'] for m in models_data.get('models', [])]

                        if self.model.model_id not in available_models:
                            return {
                                'success': False,
                                'error': f'–ú–æ–¥–µ–ª—å {self.model.model_id} –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –≤ Ollama',
                                'available_models': available_models
                            }
                except httpx.ConnectError:
                    return {'success': False, 'error': '–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–¥–∫–ª—é—á–∏—Ç—å—Å—è –∫ Ollama —Å–µ—Ä–≤–µ—Ä—É'}

                # –û–±—ä–µ–¥–∏–Ω—è–µ–º –ø—Ä–æ–º–ø—Ç—ã –¥–ª—è —Ä–∞—Å—á–µ—Ç–∞ —Ä–∞–∑–º–µ—Ä–∞
                full_prompt = f"{system_prompt}\n{user_prompt}"

                # –û—Ü–µ–Ω–∫–∞ –¥–ª–∏–Ω—ã –ø—Ä–æ–º–ø—Ç–∞ (–ø—Ä–∏–º–µ—Ä–Ω–æ 1 —Ç–æ–∫–µ–Ω = 4 —Å–∏–º–≤–æ–ª–∞)
                prompt_length = len(full_prompt) // 4

                # üîß –î–ò–ù–ê–ú–ò–ß–ï–°–ö–ò–ô –†–ê–°–ß–ï–¢ –ü–ê–†–ê–ú–ï–¢–†–û–í –ù–ê –û–°–ù–û–í–ï –ú–û–î–ï–õ–ò
                # –ü–æ–ª—É—á–∞–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∏–∑ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –º–æ–¥–µ–ª–∏ –∏–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–µ–º —É–º–Ω—ã–µ –¥–µ—Ñ–æ–ª—Ç—ã
                
                # 1. –ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç –º–æ–¥–µ–ª–∏
                model_max_context = self.model.max_input_tokens
                
                # 2. –ü–æ–ª—É—á–∞–µ–º –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –∏–∑ provider_config (–µ—Å–ª–∏ –µ—Å—Ç—å)
                provider_config = self.model.provider_config or {}
                
                # –î–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã —Å —É–º–Ω—ã–º–∏ –¥–µ—Ñ–æ–ª—Ç–∞–º–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫ –º–æ–¥–µ–ª–∏
                safety_buffer = provider_config.get('safety_buffer', 0.2)  # 20% –±—É—Ñ–µ—Ä –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
                min_generation_ratio = provider_config.get('min_generation_ratio', 0.1)  # –ú–∏–Ω 10% –æ—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
                max_generation_ratio = provider_config.get('max_generation_ratio', 0.5)  # –ú–∞–∫—Å 50% –æ—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
                min_context_size = provider_config.get('min_context_size', 2048)  # –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
                
                # –ö–æ—Ä—Ä–µ–∫—Ç–∏—Ä–æ–≤–∫–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ä–µ–π—Ç–∏–Ω–≥–æ–≤ –º–æ–¥–µ–ª–∏
                if self.model.speed_rating >= 4:  # –ë—ã—Å—Ç—Ä—ã–µ –º–æ–¥–µ–ª–∏
                    max_generation_ratio = min(max_generation_ratio, 0.6)  # –ú–æ–∂–µ–º –ø–æ–∑–≤–æ–ª–∏—Ç—å –±–æ–ª—å—à–µ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
                elif self.model.speed_rating <= 2:  # –ú–µ–¥–ª–µ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏
                    max_generation_ratio = min(max_generation_ratio, 0.3)  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏
                
                # 3. –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º –±–µ–∑–æ–ø–∞—Å–Ω—ã–π —Ä–∞–∑–º–µ—Ä –ø—Ä–æ–º–ø—Ç–∞ —Å –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–º –±—É—Ñ–µ—Ä–æ–º
                safe_prompt_size = int(prompt_length * (1 + safety_buffer))
                
                # 4. –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º —Ä–∞–∑—É–º–Ω—ã–π —Ä–∞–∑–º–µ—Ä –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
                available_space = model_max_context - safe_prompt_size
                
                # 5. –î–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–µ –º–∏–Ω/–º–∞–∫—Å –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–æ—Ü–µ–Ω—Ç–∞ –æ—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
                min_generation = int(model_max_context * min_generation_ratio)
                max_generation = int(model_max_context * max_generation_ratio)
                
                # 6. –û–ø—Ç–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Å —É—á–µ—Ç–æ–º –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–π
                optimal_generation = max(min_generation, min(available_space, max_generation))
                
                # 7. –£—á–∏—Ç—ã–≤–∞–µ–º –º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π –≤—ã—Ö–æ–¥ –º–æ–¥–µ–ª–∏ –∏–∑ –Ω–∞—Å—Ç—Ä–æ–µ–∫
                final_generation_size = min(optimal_generation, self.model.max_output_tokens)
                
                # 8. –§–∏–Ω–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
                final_context_size = safe_prompt_size + final_generation_size
                actual_context_size = min(final_context_size, model_max_context)
                actual_context_size = max(actual_context_size, min_context_size)

                # –õ–æ–≥–∏—Ä—É–µ–º –Ω–æ–≤—É—é –¥–∏–Ω–∞–º–∏—á–µ—Å–∫—É—é –ª–æ–≥–∏–∫—É —Ä–∞—Å—á–µ—Ç–∞
                logger.info(f"Ollama: –î–ò–ù–ê–ú–ò–ß–ï–°–ö–ò–ô —Ä–∞—Å—á–µ—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –¥–ª—è {self.model.name}:")
                logger.info(f"  üìä –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–¥–µ–ª–∏: speed={self.model.speed_rating}/5, max_context={model_max_context:,}")
                logger.info(f"  üìù –ü—Ä–æ–º–ø—Ç: ~{prompt_length} —Ç–æ–∫–µ–Ω–æ–≤ (+{int(safety_buffer*100)}% –±—É—Ñ–µ—Ä = {safe_prompt_size})")
                logger.info(f"  üéØ –î–∏–∞–ø–∞–∑–æ–Ω –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: {min_generation:,}-{max_generation:,} —Ç–æ–∫–µ–Ω–æ–≤ ({int(min_generation_ratio*100)}%-{int(max_generation_ratio*100)}% –æ—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞)")
                logger.info(f"  üí° –û–ø—Ç–∏–º–∞–ª—å–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è: {optimal_generation:,} —Ç–æ–∫–µ–Ω–æ–≤")
                logger.info(f"  üîß –§–∏–Ω–∞–ª—å–Ω—ã–π num_predict: {final_generation_size:,} / {self.model.max_output_tokens:,} (–º–∞–∫—Å. –º–æ–¥–µ–ª–∏)")
                logger.info(f"  üìè –ö–æ–Ω—Ç–µ–∫—Å—Ç: {actual_context_size:,} —Ç–æ–∫–µ–Ω–æ–≤")
                logger.info(f"  ‚öôÔ∏è  –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –∏–∑ provider_config: {provider_config}")

                # –õ–æ–≥–∏—Ä—É–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∑–∞–ø—Ä–æ—Å–∞
                LogService.log_info(f"Ollama –∑–∞–ø—Ä–æ—Å: {self.model.model_id} | Temperature: {temperature} | Num predict: {final_generation_size:,} / {self.model.max_output_tokens:,}")
                logger.debug(f"Ollama endpoint: {self.model.api_endpoint}")
                logger.debug(f"Context size: {actual_context_size}")

                # –û–±—ä–µ–¥–∏–Ω—è–µ–º system –∏ user –ø—Ä–æ–º–ø—Ç—ã –≤ –æ–¥–∏–Ω
                # Ollama –ª—É—á—à–µ —Ä–∞–±–æ—Ç–∞–µ—Ç —Å –µ–¥–∏–Ω—ã–º –ø—Ä–æ–º–ø—Ç–æ–º
                full_prompt = f"{system_prompt}\n\n{user_prompt}"

                # –î–µ–ª–∞–µ–º –∑–∞–ø—Ä–æ—Å –∫ –º–æ–¥–µ–ª–∏ —Å –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º
                response = await client.post(
                    f"{self.model.api_endpoint}/generate",
                    json={
                        'model': self.model.model_id,
                        'prompt': full_prompt,  # –ï–¥–∏–Ω—ã–π –ø—Ä–æ–º–ø—Ç –≤–º–µ—Å—Ç–æ system + prompt
                        'stream': False,
                        'options': {
                            'temperature': temperature,
                            'num_predict': final_generation_size,
                            'num_ctx': actual_context_size,  # –î–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–π —Ä–∞–∑–º–µ—Ä –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
                            'num_keep': actual_context_size  # –°–∫–æ–ª—å–∫–æ —Ç–æ–∫–µ–Ω–æ–≤ —Å–æ—Ö—Ä–∞–Ω—è—Ç—å
                        }
                    }
                )

                if response.status_code == 200:
                    try:
                        data = response.json()
                        content = data.get('response', '')
                        finish_reason = 'stop' if data.get('done') else 'length'

                        # –õ–æ–≥–∏—Ä—É–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –æ—Ç–≤–µ—Ç–µ
                        logger.info(f"Ollama response received: {len(content)} chars, {data.get('eval_count', 0)} tokens")
                        logger.info(f"Finish reason: {finish_reason}, Done: {data.get('done')}")

                        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ –æ–±—Ä–µ–∑–∫—É
                        if not data.get('done'):
                            logger.warning(f"‚ö†Ô∏è Ollama response was truncated! Done=False")
                            logger.warning(f"Requested num_predict: {min(max_tokens, self.model.max_output_tokens)}")
                            logger.warning(f"Actual tokens generated: {data.get('eval_count', 0)}")

                        # –û—á–∏—Å—Ç–∫–∞ –æ—Ç–≤–µ—Ç–∞ –æ—Ç —Å–ª—É–∂–µ–±–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤
                        import re

                        # –£–¥–∞–ª—è–µ–º —Å–ª—É–∂–µ–±–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã –º–æ–¥–µ–ª–∏
                        content = content.replace("</start_of_turn>", "")
                        content = content.replace("</end_of_turn>", "")
                        content = content.replace("<start_of_turn>", "")
                        content = content.replace("<end_of_turn>", "")

                        # –£–¥–∞–ª—è–µ–º –±–ª–æ–∫–∏ <think>...</think> –µ—Å–ª–∏ –æ–Ω–∏ –µ—Å—Ç—å
                        think_blocks = re.findall(r'<think>.*?</think>', content, flags=re.DOTALL)
                        if think_blocks:
                            logger.debug(f"Found {len(think_blocks)} <think> blocks, removing them")
                            content = re.sub(r'<think>.*?</think>', '', content, flags=re.DOTALL)
                            content = content.strip()

                        return {
                            'success': True,
                            'content': content,
                            'usage': {
                                'prompt_tokens': data.get('prompt_eval_count', 0),
                                'completion_tokens': data.get('eval_count', 0),
                                'total_tokens': data.get('prompt_eval_count', 0) + data.get('eval_count', 0)
                            },
                            'finish_reason': finish_reason
                        }
                    except json.JSONDecodeError as je:
                        error_text = response.text
                        logger.error(f"Ollama returned HTTP 200 but invalid JSON!")
                        logger.error(f"JSON decode error: {je}")
                        logger.error(f"Response text (first 1000 chars): {error_text[:1000]}")
                        logger.error(f"Response headers: {dict(response.headers)}")
                        return {
                            'success': False,
                            'error': f'Ollama –≤–µ—Ä–Ω—É–ª HTTP 200, –Ω–æ –Ω–µ–≤–∞–ª–∏–¥–Ω—ã–π JSON: {error_text[:200]}',
                            'error_type': 'invalid_json'
                        }
                else:
                    # –ü—ã—Ç–∞–µ–º—Å—è –ø–æ–ª—É—á–∏—Ç—å –¥–µ—Ç–∞–ª—å–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ–± –æ—à–∏–±–∫–µ
                    error_detail = f'HTTP {response.status_code}'
                    error_text = None

                    try:
                        # –°–Ω–∞—á–∞–ª–∞ –ø–æ–ª—É—á–∞–µ–º —Å—ã—Ä–æ–π —Ç–µ–∫—Å—Ç –æ—Ç–≤–µ—Ç–∞
                        error_text = response.text
                        logger.error(f"Ollama raw response (first 1000 chars): {error_text[:1000]}")

                        # –ü—ã—Ç–∞–µ–º—Å—è —Ä–∞—Å–ø–∞—Ä—Å–∏—Ç—å –∫–∞–∫ JSON
                        error_data = response.json()
                        if 'error' in error_data:
                            error_detail = error_data['error']
                        logger.error(f"Ollama error response JSON: {error_data}")
                    except json.JSONDecodeError as je:
                        # –ù–µ JSON - –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ç–µ–∫—Å—Ç
                        if error_text:
                            error_detail = f'HTTP {response.status_code}: {error_text[:500]}'
                            logger.error(f"Ollama returned non-JSON response. JSON decode error: {je}")
                        else:
                            logger.error(f"Failed to decode JSON and no text available")
                    except Exception as e:
                        logger.error(f"Unexpected error parsing response: {e}")
                        if error_text:
                            error_detail = f'HTTP {response.status_code}: {error_text[:500]}'

                    logger.error(f"Ollama request failed: {error_detail}")
                    logger.error(f"Model: {self.model.model_id}, Endpoint: {self.model.api_endpoint}")
                    logger.error(f"Context size: {actual_context_size}, Max tokens: {max_tokens}")
                    logger.error(f"Response headers: {dict(response.headers)}")

                    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø –æ—à–∏–±–∫–∏ –¥–ª—è —Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏
                    error_type = 'general'
                    error_detail_lower = error_detail.lower()

                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ —Ä–∞–∑–Ω—ã–µ —Ç–∏–ø—ã –ª–∏–º–∏—Ç–æ–≤
                    if 'weekly usage limit' in error_detail_lower:
                        error_type = 'weekly_limit'
                        logger.error(f"üö´ –û–±–Ω–∞—Ä—É–∂–µ–Ω –ù–ï–î–ï–õ–¨–ù–´–ô –ª–∏–º–∏—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è Ollama –º–æ–¥–µ–ª–∏")
                    elif 'daily usage limit' in error_detail_lower:
                        error_type = 'daily_limit'
                        logger.error(f"üö´ –û–±–Ω–∞—Ä—É–∂–µ–Ω –î–ù–ï–í–ù–û–ô –ª–∏–º–∏—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è Ollama –º–æ–¥–µ–ª–∏")
                    elif 'hourly usage limit' in error_detail_lower or 'usage limit' in error_detail_lower:
                        error_type = 'rate_limit'
                        logger.error(f"‚ö†Ô∏è –û–±–Ω–∞—Ä—É–∂–µ–Ω–∞ –æ—à–∏–±–∫–∞ –ª–∏–º–∏—Ç–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è Ollama –º–æ–¥–µ–ª–∏")
                    elif 'upstream timeout' in error_detail_lower or response.status_code == 504:
                        error_type = 'upstream_timeout'
                        logger.error(f"‚ö†Ô∏è –û–±–Ω–∞—Ä—É–∂–µ–Ω–∞ –æ—à–∏–±–∫–∞ upstream timeout (504) - —Å–µ—Ä–≤–µ—Ä –Ω–µ –æ—Ç–≤–µ—Ç–∏–ª –≤–æ–≤—Ä–µ–º—è")
                    elif 'upstream error' in error_detail_lower or response.status_code == 502:
                        error_type = 'upstream_error'
                        logger.error(f"‚ö†Ô∏è –û–±–Ω–∞—Ä—É–∂–µ–Ω–∞ –æ—à–∏–±–∫–∞ upstream (502) - –≤—Ä–µ–º–µ–Ω–Ω–∞—è –ø—Ä–æ–±–ª–µ–º–∞ —Å–µ—Ä–≤–µ—Ä–∞")
                    elif 'unmarshal' in error_detail_lower or 'unexpected end of json' in error_detail_lower or response.status_code == 500:
                        error_type = 'server_error'
                        logger.error(f"‚ö†Ô∏è –û–±–Ω–∞—Ä—É–∂–µ–Ω–∞ –≤–Ω—É—Ç—Ä–µ–Ω–Ω—è—è –æ—à–∏–±–∫–∞ —Å–µ—Ä–≤–µ—Ä–∞ (–Ω–µ–≤–∞–ª–∏–¥–Ω—ã–π JSON –∏–ª–∏ –ø—Ä–µ—Ä–≤–∞–Ω–Ω—ã–π –æ—Ç–≤–µ—Ç)")
                    elif 'not found' in error_detail_lower:
                        error_type = 'model_not_found'

                    return {
                        'success': False,
                        'error': f'–û—à–∏–±–∫–∞ Ollama: {error_detail}',
                        'error_type': error_type,
                        'status_code': response.status_code
                    }

        except httpx.TimeoutException as e:
            error_msg = f'–¢–∞–π–º–∞—É—Ç –ø—Ä–∏ –æ–±—Ä–∞—â–µ–Ω–∏–∏ –∫ Ollama (>600s). –ú–æ–¥–µ–ª—å: {self.model.model_id}'
            logger.error(error_msg)
            logger.error(f"–†–∞–∑–º–µ—Ä –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –±—ã–ª: {actual_context_size if 'actual_context_size' in locals() else 'unknown'}")
            return {
                'success': False,
                'error': error_msg,
                'error_type': 'timeout'
            }
        except httpx.ConnectError as e:
            error_msg = f'–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–¥–∫–ª—é—á–∏—Ç—å—Å—è –∫ Ollama —Å–µ—Ä–≤–µ—Ä—É: {self.model.api_endpoint}'
            logger.error(error_msg)
            logger.error(f"Connection error: {str(e)}")
            return {
                'success': False,
                'error': error_msg,
                'error_type': 'connection'
            }
        except Exception as e:
            error_msg = f'–ù–µ–æ–∂–∏–¥–∞–Ω–Ω–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–ø—Ä–æ—Å–µ –∫ Ollama: {type(e).__name__}: {str(e)}'
            logger.error(error_msg)
            import traceback
            logger.error(f"Traceback: {traceback.format_exc()}")
            return {
                'success': False,
                'error': error_msg,
                'error_type': 'unexpected'
            }

    @staticmethod
    def get_available_models(provider: str = None, active_only: bool = True) -> List[AIModel]:
        """–ü–æ–ª—É—á–∏—Ç—å —Å–ø–∏—Å–æ–∫ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π"""
        query = AIModel.query

        if active_only:
            query = query.filter_by(is_active=True)

        if provider:
            query = query.filter_by(provider=provider)

        return query.all()

    @staticmethod
    def get_default_model() -> Optional[AIModel]:
        """–ü–æ–ª—É—á–∏—Ç—å –º–æ–¥–µ–ª—å –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é"""
        return AIModel.query.filter_by(is_default=True).first()