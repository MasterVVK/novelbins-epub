"""
–ê–¥–∞–ø—Ç–µ—Ä –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å —Ä–∞–∑–Ω—ã–º–∏ AI –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞–º–∏ —á–µ—Ä–µ–∑ —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å
"""
import httpx
import json
import logging
from typing import Dict, List, Optional
from app.models.ai_model import AIModel
from app.services.ai_model_service import AIModelService
from app.services.log_service import LogService

logger = logging.getLogger(__name__)


class AIAdapterService:
    """–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –∞–¥–∞–ø—Ç–µ—Ä –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å —Ä–∞–∑–Ω—ã–º–∏ AI –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞–º–∏"""

    def __init__(self, model_id: int = None, model_name: str = None, chapter_id: int = None):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∞–¥–∞–ø—Ç–µ—Ä–∞
        Args:
            model_id: ID –º–æ–¥–µ–ª–∏ –∏–∑ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö
            model_name: –ò–º—è –º–æ–¥–µ–ª–∏ (–∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–∞ ID)
            chapter_id: ID –≥–ª–∞–≤—ã –¥–ª—è –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
        """
        if model_id:
            self.model = AIModelService.get_model_by_id(model_id)
        elif model_name:
            self.model = AIModel.query.filter_by(name=model_name).first()
        else:
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –º–æ–¥–µ–ª—å –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
            self.model = AIModel.query.filter_by(is_default=True).first()

        if not self.model:
            raise ValueError("AI –º–æ–¥–µ–ª—å –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")

        self.chapter_id = chapter_id

        logger.info(f"–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω –∞–¥–∞–ø—Ç–µ—Ä –¥–ª—è –º–æ–¥–µ–ª–∏: {self.model.name} ({self.model.provider})")

    def _estimate_tokens(self, text: str) -> int:
        """
        –£–ª—É—á—à–µ–Ω–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Ç–æ–∫–µ–Ω–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∞–Ω–∞–ª–∏–∑–∞ —è–∑—ã–∫–∞ —Ç–µ–∫—Å—Ç–∞

        Args:
            text: –¢–µ–∫—Å—Ç –¥–ª—è –æ—Ü–µ–Ω–∫–∏

        Returns:
            –ü—Ä–∏–º–µ—Ä–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤
        """
        if not text:
            return 0

        total_chars = len(text)

        # –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º —Å–∏–º–≤–æ–ª—ã —Ä–∞–∑–Ω—ã—Ö —Ç–∏–ø–æ–≤
        cyrillic_count = sum(1 for c in text if '\u0400' <= c <= '\u04FF')  # –ö–∏—Ä–∏–ª–ª–∏—Ü–∞
        cjk_count = sum(1 for c in text if '\u4E00' <= c <= '\u9FFF')       # –ö–∏—Ç–∞–π—Å–∫–∏–µ –∏–µ—Ä–æ–≥–ª–∏—Ñ—ã

        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –¥–æ–ª–∏ —Ä–∞–∑–Ω—ã—Ö —Ç–∏–ø–æ–≤ —Å–∏–º–≤–æ–ª–æ–≤
        cyrillic_ratio = cyrillic_count / total_chars if total_chars > 0 else 0
        cjk_ratio = cjk_count / total_chars if total_chars > 0 else 0

        # –í—ã–±–∏—Ä–∞–µ–º –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–µ–æ–±–ª–∞–¥–∞—é—â–µ–≥–æ —è–∑—ã–∫–∞
        if cjk_ratio > 0.3:  # –ú–Ω–æ–≥–æ –∫–∏—Ç–∞–π—Å–∫–∏—Ö —Å–∏–º–≤–æ–ª–æ–≤
            # –ö–∏—Ç–∞–π—Å–∫–∏–µ –∏–µ—Ä–æ–≥–ª–∏—Ñ—ã: ~1.5-2 —Å–∏–º–≤–æ–ª–∞ –Ω–∞ —Ç–æ–∫–µ–Ω
            chars_per_token = 1.5
            language = "–∫–∏—Ç–∞–π—Å–∫–∏–π"
        elif cyrillic_ratio > 0.3:  # –ú–Ω–æ–≥–æ –∫–∏—Ä–∏–ª–ª–∏—Ü—ã
            # –ö–∏—Ä–∏–ª–ª–∏—Ü–∞: ~2.5-3 —Å–∏–º–≤–æ–ª–∞ –Ω–∞ —Ç–æ–∫–µ–Ω
            chars_per_token = 2.5
            language = "—Ä—É—Å—Å–∫–∏–π"
        else:  # –õ–∞—Ç–∏–Ω–∏—Ü–∞ –∏–ª–∏ —Å–º–µ—à–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç
            # –õ–∞—Ç–∏–Ω–∏—Ü–∞: ~4 —Å–∏–º–≤–æ–ª–∞ –Ω–∞ —Ç–æ–∫–µ–Ω
            chars_per_token = 4.0
            language = "–∞–Ω–≥–ª–∏–π—Å–∫–∏–π/–ª–∞—Ç–∏–Ω–∏—Ü–∞"

        estimated_tokens = int(total_chars / chars_per_token)

        # –õ–æ–≥–∏—Ä—É–µ–º –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏ (—Ç–æ–ª—å–∫–æ –≤ debug —Ä–µ–∂–∏–º–µ)
        logger.debug(f"–û—Ü–µ–Ω–∫–∞ —Ç–æ–∫–µ–Ω–æ–≤: {total_chars:,} —Å–∏–º–≤–æ–ª–æ–≤, —è–∑—ã–∫: {language}, "
                    f"~{estimated_tokens:,} —Ç–æ–∫–µ–Ω–æ–≤ ({chars_per_token} —Å–∏–º–≤/—Ç–æ–∫–µ–Ω)")

        return estimated_tokens

    async def generate_content(self, system_prompt: str, user_prompt: str,
                              temperature: float = None, max_tokens: int = None) -> Dict:
        """
        –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∫–æ–Ω—Ç–µ–Ω—Ç–∞ —á–µ—Ä–µ–∑ –≤—ã–±—Ä–∞–Ω–Ω—É—é –º–æ–¥–µ–ª—å
        Returns:
            Dict —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–º: {'success': bool, 'content': str, 'error': str}
        """
        temperature = temperature or self.model.default_temperature
        max_tokens = max_tokens or self.model.max_output_tokens

        try:
            if self.model.provider == 'gemini':
                return await self._call_gemini(system_prompt, user_prompt, temperature, max_tokens)
            elif self.model.provider == 'openai':
                return await self._call_openai(system_prompt, user_prompt, temperature, max_tokens)
            elif self.model.provider == 'anthropic':
                return await self._call_anthropic(system_prompt, user_prompt, temperature, max_tokens)
            elif self.model.provider == 'ollama':
                return await self._call_ollama(system_prompt, user_prompt, temperature, max_tokens)
            elif self.model.provider == 'openrouter':
                return await self._call_openrouter(system_prompt, user_prompt, temperature, max_tokens)
            else:
                return {'success': False, 'error': f'–ù–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–π –ø—Ä–æ–≤–∞–π–¥–µ—Ä: {self.model.provider}'}

        except Exception as e:
            import traceback
            logger.error(f"–û—à–∏–±–∫–∞ –≤—ã–∑–æ–≤–∞ –º–æ–¥–µ–ª–∏ {self.model.name}: {e}")
            logger.error(f"Traceback: {traceback.format_exc()}")
            error_msg = str(e) if str(e) else f"{type(e).__name__}: {repr(e)}"
            return {'success': False, 'error': error_msg}

    async def _call_gemini(self, system_prompt: str, user_prompt: str,
                          temperature: float, max_tokens: int) -> Dict:
        """–í—ã–∑–æ–≤ Gemini API"""
        if not self.model.api_key:
            return {'success': False, 'error': 'API –∫–ª—é—á –Ω–µ —É–∫–∞–∑–∞–Ω'}

        # –£–≤–µ–ª–∏—á–µ–Ω–Ω—ã–π —Ç–∞–π–º–∞—É—Ç –¥–ª—è Gemini (–±–æ–ª—å—à–∏–µ —Ç–µ–∫—Å—Ç—ã —Ç—Ä–µ–±—É—é—Ç –≤—Ä–µ–º–µ–Ω–∏)
        async with httpx.AsyncClient(timeout=300.0) as client:
            url = f"{self.model.api_endpoint}/models/{self.model.model_id}:generateContent"
            
            # –õ–æ–≥–∏—Ä—É–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∑–∞–ø—Ä–æ—Å–∞
            actual_max_tokens = min(max_tokens, self.model.max_output_tokens)
            LogService.log_info(f"Gemini –∑–∞–ø—Ä–æ—Å: {self.model.model_id} | Temperature: {temperature} | Max tokens: {actual_max_tokens:,} / {self.model.max_output_tokens:,}")

            response = await client.post(
                url,
                params={'key': self.model.api_key},
                json={
                    'contents': [{
                        'parts': [
                            {'text': system_prompt},
                            {'text': user_prompt}
                        ]
                    }],
                    'generationConfig': {
                        'temperature': temperature,
                        'maxOutputTokens': actual_max_tokens,
                        'topP': 0.95,
                        'topK': 40
                    },
                    'safetySettings': [
                        {'category': 'HARM_CATEGORY_HATE_SPEECH', 'threshold': 'BLOCK_NONE'},
                        {'category': 'HARM_CATEGORY_SEXUALLY_EXPLICIT', 'threshold': 'BLOCK_NONE'},
                        {'category': 'HARM_CATEGORY_HARASSMENT', 'threshold': 'BLOCK_NONE'},
                        {'category': 'HARM_CATEGORY_DANGEROUS_CONTENT', 'threshold': 'BLOCK_NONE'}
                    ]
                }
            )

            if response.status_code == 200:
                data = response.json()

                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –±–ª–æ–∫–∏—Ä–æ–≤–∫—É –ø—Ä–æ–º–ø—Ç–∞
                if 'promptFeedback' in data and data['promptFeedback'].get('blockReason'):
                    return {
                        'success': False,
                        'error': f"–ü—Ä–æ–º–ø—Ç –∑–∞–±–ª–æ–∫–∏—Ä–æ–≤–∞–Ω: {data['promptFeedback']['blockReason']}"
                    }

                # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–∫—Å—Ç –∏–∑ –æ—Ç–≤–µ—Ç–∞
                candidates = data.get('candidates', [])
                if candidates:
                    content = candidates[0].get('content', {}).get('parts', [{}])[0].get('text', '')
                    return {
                        'success': True,
                        'content': content,
                        'usage': data.get('usageMetadata', {}),
                        'finish_reason': candidates[0].get('finishReason', 'UNKNOWN')
                    }
                else:
                    return {'success': False, 'error': '–ù–µ—Ç –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ –≤ –æ—Ç–≤–µ—Ç–µ'}

            elif response.status_code == 429:
                return {'success': False, 'error': 'Rate limit –ø—Ä–µ–≤—ã—à–µ–Ω', 'retry_after': 60}
            else:
                error_data = response.json()
                return {
                    'success': False,
                    'error': error_data.get('error', {}).get('message', f'HTTP {response.status_code}')
                }

    async def _call_openai(self, system_prompt: str, user_prompt: str,
                           temperature: float, max_tokens: int) -> Dict:
        """–í—ã–∑–æ–≤ OpenAI API"""
        if not self.model.api_key:
            return {'success': False, 'error': 'API –∫–ª—é—á –Ω–µ —É–∫–∞–∑–∞–Ω'}

        # –õ–æ–≥–∏—Ä—É–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∑–∞–ø—Ä–æ—Å–∞
        actual_max_tokens = min(max_tokens, self.model.max_output_tokens)
        LogService.log_info(f"OpenAI –∑–∞–ø—Ä–æ—Å: {self.model.model_id} | Temperature: {temperature} | Max tokens: {actual_max_tokens:,} / {self.model.max_output_tokens:,}")

        async with httpx.AsyncClient(timeout=60.0) as client:
            response = await client.post(
                f"{self.model.api_endpoint}/chat/completions",
                headers={
                    'Authorization': f'Bearer {self.model.api_key}',
                    'Content-Type': 'application/json'
                },
                json={
                    'model': self.model.model_id,
                    'messages': [
                        {'role': 'system', 'content': system_prompt},
                        {'role': 'user', 'content': user_prompt}
                    ],
                    'temperature': temperature,
                    'max_tokens': actual_max_tokens
                }
            )

            if response.status_code == 200:
                data = response.json()
                choices = data.get('choices', [])
                if choices:
                    return {
                        'success': True,
                        'content': choices[0].get('message', {}).get('content', ''),
                        'usage': data.get('usage', {}),
                        'finish_reason': choices[0].get('finish_reason', 'unknown')
                    }
                else:
                    return {'success': False, 'error': '–ù–µ—Ç –≤–∞—Ä–∏–∞–Ω—Ç–æ–≤ –≤ –æ—Ç–≤–µ—Ç–µ'}
            else:
                error_data = response.json()
                return {
                    'success': False,
                    'error': error_data.get('error', {}).get('message', f'HTTP {response.status_code}')
                }

    async def _call_anthropic(self, system_prompt: str, user_prompt: str,
                              temperature: float, max_tokens: int) -> Dict:
        """–í—ã–∑–æ–≤ Anthropic API"""
        if not self.model.api_key:
            return {'success': False, 'error': 'API –∫–ª—é—á –Ω–µ —É–∫–∞–∑–∞–Ω'}

        # –õ–æ–≥–∏—Ä—É–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∑–∞–ø—Ä–æ—Å–∞
        actual_max_tokens = min(max_tokens, self.model.max_output_tokens)
        LogService.log_info(f"Anthropic –∑–∞–ø—Ä–æ—Å: {self.model.model_id} | Temperature: {temperature} | Max tokens: {actual_max_tokens:,} / {self.model.max_output_tokens:,}")

        async with httpx.AsyncClient(timeout=60.0) as client:
            response = await client.post(
                f"{self.model.api_endpoint}/messages",
                headers={
                    'x-api-key': self.model.api_key,
                    'anthropic-version': '2023-06-01',
                    'Content-Type': 'application/json'
                },
                json={
                    'model': self.model.model_id,
                    'system': system_prompt,
                    'messages': [
                        {'role': 'user', 'content': user_prompt}
                    ],
                    'temperature': temperature,
                    'max_tokens': actual_max_tokens
                }
            )

            if response.status_code == 200:
                data = response.json()
                content_blocks = data.get('content', [])
                if content_blocks:
                    return {
                        'success': True,
                        'content': content_blocks[0].get('text', ''),
                        'usage': data.get('usage', {}),
                        'finish_reason': data.get('stop_reason', 'unknown')
                    }
                else:
                    return {'success': False, 'error': '–ù–µ—Ç –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –≤ –æ—Ç–≤–µ—Ç–µ'}
            else:
                error_data = response.json()
                return {
                    'success': False,
                    'error': error_data.get('error', {}).get('message', f'HTTP {response.status_code}')
                }

    async def _call_ollama(self, system_prompt: str, user_prompt: str,
                           temperature: float, max_tokens: int) -> Dict:
        """–í—ã–∑–æ–≤ Ollama API —Å –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–º —Ä–∞—Å—á–µ—Ç–æ–º —Ä–∞–∑–º–µ—Ä–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –º–æ–¥–µ–ª–∏"""
        # –£–≤–µ–ª–∏—á–µ–Ω–Ω—ã–π —Ç–∞–π–º–∞—É—Ç –¥–ª—è Ollama (–±–æ–ª—å—à–∏–µ –º–æ–¥–µ–ª–∏ —Ç—Ä–µ–±—É—é—Ç –≤—Ä–µ–º–µ–Ω–∏ –Ω–∞ –∑–∞–≥—Ä—É–∑–∫—É –∏ –æ–±—Ä–∞–±–æ—Ç–∫—É)
        try:
            async with httpx.AsyncClient(timeout=1200.0) as client:  # 20 –º–∏–Ω—É—Ç
                # –°–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏
                try:
                    models_response = await client.get(f"{self.model.api_endpoint.rstrip('/api')}/api/tags")
                    if models_response.status_code == 200:
                        models_data = models_response.json()
                        available_models = [m['name'] for m in models_data.get('models', [])]

                        if self.model.model_id not in available_models:
                            return {
                                'success': False,
                                'error': f'–ú–æ–¥–µ–ª—å {self.model.model_id} –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –≤ Ollama',
                                'available_models': available_models
                            }
                except httpx.ConnectError:
                    return {'success': False, 'error': '–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–¥–∫–ª—é—á–∏—Ç—å—Å—è –∫ Ollama —Å–µ—Ä–≤–µ—Ä—É'}

                # –û–±—ä–µ–¥–∏–Ω—è–µ–º –ø—Ä–æ–º–ø—Ç—ã –¥–ª—è —Ä–∞—Å—á–µ—Ç–∞ —Ä–∞–∑–º–µ—Ä–∞
                full_prompt = f"{system_prompt}\n{user_prompt}"

                # –£–ª—É—á—à–µ–Ω–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –¥–ª–∏–Ω—ã –ø—Ä–æ–º–ø—Ç–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ —è–∑—ã–∫–∞
                prompt_length = self._estimate_tokens(full_prompt)

                # üîß –£–ü–†–û–©–ï–ù–ù–´–ô –†–ê–°–ß–ï–¢: num_ctx = —Ä–∞–∑–º–µ—Ä –ø—Ä–æ–º–ø—Ç–∞ + 20%
                # num_ctx –∑–∞–¥–∞–µ—Ç —Ä–∞–∑–º–µ—Ä –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ–≥–æ –æ–∫–Ω–∞ –¥–ª—è –ø—Ä–æ–º–ø—Ç–∞
                num_ctx = int(prompt_length * 1.2)  # –ü—Ä–æ–º–ø—Ç + 20% –±—É—Ñ–µ—Ä

                # –ü–æ–ª—É—á–∞–µ–º –º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç –º–æ–¥–µ–ª–∏ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏
                model_max_context = self.model.max_input_tokens

                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –Ω–µ –ø—Ä–µ–≤—ã—à–∞–µ–º –ª–∏–º–∏—Ç—ã –º–æ–¥–µ–ª–∏
                if num_ctx > model_max_context:
                    logger.warning(f"‚ö†Ô∏è num_ctx ({num_ctx:,}) –ø—Ä–µ–≤—ã—à–∞–µ—Ç max_input_tokens ({model_max_context:,}), –æ–±—Ä–µ–∑–∞–µ–º")
                    num_ctx = model_max_context

                # –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
                min_context_size = 2048
                if num_ctx < min_context_size:
                    logger.info(f"num_ctx ({num_ctx:,}) –º–µ–Ω—å—à–µ –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–≥–æ ({min_context_size:,}), —É—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –º–∏–Ω–∏–º—É–º")
                    num_ctx = min_context_size

                # num_predict = num_ctx √ó 2 (–æ–±—ã—á–Ω—ã–µ –º–æ–¥–µ–ª–∏)
                # –î–ª—è reasoning –º–æ–¥–µ–ª–µ–π: num_ctx √ó 4 (—Ç—Ä–µ–±—É—é—Ç –±–æ–ª—å—à–µ —Ç–æ–∫–µ–Ω–æ–≤ –¥–ª—è –≤–Ω—É—Ç—Ä–µ–Ω–Ω–µ–≥–æ –º—ã—à–ª–µ–Ω–∏—è)
                if hasattr(self.model, 'enable_thinking') and self.model.enable_thinking:
                    predict_multiplier = 4  # Reasoning –º–æ–¥–µ–ª–∏
                    logger.info(f"  üß† Reasoning –º–æ–¥–µ–ª—å: –∏—Å–ø–æ–ª—å–∑—É–µ–º multiplier √ó {predict_multiplier} –¥–ª—è num_predict")
                else:
                    predict_multiplier = 2  # –û–±—ã—á–Ω—ã–µ –º–æ–¥–µ–ª–∏

                num_predict = min(num_ctx * predict_multiplier, self.model.max_output_tokens)

                # –õ–æ–≥–∏—Ä—É–µ–º —É–ø—Ä–æ—â–µ–Ω–Ω—É—é –ª–æ–≥–∏–∫—É —Ä–∞—Å—á–µ—Ç–∞
                logger.info(f"Ollama: –†–∞—Å—á–µ—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –¥–ª—è {self.model.name}:")
                logger.info(f"  üìù –†–∞–∑–º–µ—Ä –ø—Ä–æ–º–ø—Ç–∞: ~{prompt_length:,} —Ç–æ–∫–µ–Ω–æ–≤")
                logger.info(f"  üìè num_ctx (–ø—Ä–æ–º–ø—Ç + 20%): {num_ctx:,} —Ç–æ–∫–µ–Ω–æ–≤")
                logger.info(f"  üîß num_predict: {num_predict:,} —Ç–æ–∫–µ–Ω–æ–≤ (num_ctx √ó {predict_multiplier})")
                logger.info(f"  üìä –õ–∏–º–∏—Ç—ã –º–æ–¥–µ–ª–∏: max_input={model_max_context:,}, max_output={self.model.max_output_tokens:,}")

                # –õ–æ–≥–∏—Ä—É–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∑–∞–ø—Ä–æ—Å–∞
                log_prefix = ""
                if self.chapter_id:
                    from app.models import Chapter
                    chapter = Chapter.query.get(self.chapter_id)
                    if chapter:
                        log_prefix = f"[Novel:{chapter.novel_id}, Ch:{chapter.chapter_number}] "

                LogService.log_info(f"{log_prefix}Ollama –∑–∞–ø—Ä–æ—Å: {self.model.model_id} | Temperature: {temperature} | Num ctx: {num_ctx:,} | Num predict: {num_predict:,} / {self.model.max_output_tokens:,}")
                logger.debug(f"Ollama endpoint: {self.model.api_endpoint}")
                logger.debug(f"Context size: {num_ctx}")

                # –û–±—ä–µ–¥–∏–Ω—è–µ–º system –∏ user –ø—Ä–æ–º–ø—Ç—ã –≤ –æ–¥–∏–Ω
                # Ollama –ª—É—á—à–µ —Ä–∞–±–æ—Ç–∞–µ—Ç —Å –µ–¥–∏–Ω—ã–º –ø—Ä–æ–º–ø—Ç–æ–º
                full_prompt = f"{system_prompt}\n\n{user_prompt}"

                # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º JSON –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞
                request_json = {
                    'model': self.model.model_id,
                    'prompt': full_prompt,  # –ï–¥–∏–Ω—ã–π –ø—Ä–æ–º–ø—Ç –≤–º–µ—Å—Ç–æ system + prompt
                    'stream': False,
                    'options': {
                        'temperature': temperature,
                        'num_predict': num_predict,      # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
                        'num_ctx': num_ctx,              # –†–∞–∑–º–µ—Ä –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ = –ø—Ä–æ–º–ø—Ç + 20%
                        'num_keep': num_ctx              # –°–∫–æ–ª—å–∫–æ —Ç–æ–∫–µ–Ω–æ–≤ —Å–æ—Ö—Ä–∞–Ω—è—Ç—å
                    }
                }

                # –í–∫–ª—é—á–∞–µ–º thinking mode –µ—Å–ª–∏ –æ–Ω –∞–∫—Ç–∏–≤–∏—Ä–æ–≤–∞–Ω –¥–ª—è –º–æ–¥–µ–ª–∏
                if hasattr(self.model, 'enable_thinking') and self.model.enable_thinking:
                    request_json['think'] = True
                    logger.info(f"üß† Thinking mode –∞–∫—Ç–∏–≤–∏—Ä–æ–≤–∞–Ω –¥–ª—è {self.model.model_id}")

                # –î–µ–ª–∞–µ–º –∑–∞–ø—Ä–æ—Å –∫ –º–æ–¥–µ–ª–∏ —Å —É–ø—Ä–æ—â–µ–Ω–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
                response = await client.post(
                    f"{self.model.api_endpoint}/generate",
                    json=request_json
                )

                if response.status_code == 200:
                    try:
                        data = response.json()
                        content = data.get('response', '')
                        finish_reason = 'stop' if data.get('done') else 'length'

                        # –û–±—Ä–∞–±–æ—Ç–∫–∞ thinking mode
                        if 'thinking' in data and data['thinking']:
                            thinking_text = data['thinking']
                            logger.info(f"üß† Thinking –ø—Ä–æ—Ü–µ—Å—Å: {len(thinking_text)} —Å–∏–º–≤–æ–ª–æ–≤")
                            logger.debug(f"Thinking —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ: {thinking_text[:500]}...")
                            # –í thinking mode –æ—Å–Ω–æ–≤–Ω–æ–π –æ—Ç–≤–µ—Ç –º–æ–∂–µ—Ç –±—ã—Ç—å –≤ –¥—Ä—É–≥–æ–º –ø–æ–ª–µ
                            # –Ω–æ –æ–±—ã—á–Ω–æ –æ–Ω –≤—Å–µ —Ä–∞–≤–Ω–æ –≤ 'response'

                        # –õ–æ–≥–∏—Ä—É–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –æ—Ç–≤–µ—Ç–µ
                        logger.info(f"Ollama response received: {len(content)} chars, {data.get('eval_count', 0)} tokens")
                        logger.info(f"Finish reason: {finish_reason}, Done: {data.get('done')}")

                        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ –æ–±—Ä–µ–∑–∫—É
                        if not data.get('done'):
                            logger.warning(f"‚ö†Ô∏è Ollama response was truncated! Done=False")
                            logger.warning(f"Requested num_predict: {min(max_tokens, self.model.max_output_tokens)}")
                            logger.warning(f"Actual tokens generated: {data.get('eval_count', 0)}")

                        # –û—á–∏—Å—Ç–∫–∞ –æ—Ç–≤–µ—Ç–∞ –æ—Ç —Å–ª—É–∂–µ–±–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤
                        import re

                        # –£–¥–∞–ª—è–µ–º —Å–ª—É–∂–µ–±–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã –º–æ–¥–µ–ª–∏
                        content = content.replace("</start_of_turn>", "")
                        content = content.replace("</end_of_turn>", "")
                        content = content.replace("<start_of_turn>", "")
                        content = content.replace("<end_of_turn>", "")

                        # –£–¥–∞–ª—è–µ–º –±–ª–æ–∫–∏ <think>...</think> –µ—Å–ª–∏ –æ–Ω–∏ –µ—Å—Ç—å
                        think_blocks = re.findall(r'<think>.*?</think>', content, flags=re.DOTALL)
                        if think_blocks:
                            logger.debug(f"Found {len(think_blocks)} <think> blocks, removing them")
                            content = re.sub(r'<think>.*?</think>', '', content, flags=re.DOTALL)
                            content = content.strip()

                        return {
                            'success': True,
                            'content': content,
                            'usage': {
                                'prompt_tokens': data.get('prompt_eval_count', 0),
                                'completion_tokens': data.get('eval_count', 0),
                                'total_tokens': data.get('prompt_eval_count', 0) + data.get('eval_count', 0)
                            },
                            'finish_reason': finish_reason
                        }
                    except json.JSONDecodeError as je:
                        error_text = response.text
                        logger.error(f"Ollama returned HTTP 200 but invalid JSON!")
                        logger.error(f"JSON decode error: {je}")
                        logger.error(f"Response text (first 1000 chars): {error_text[:1000]}")
                        logger.error(f"Response headers: {dict(response.headers)}")
                        return {
                            'success': False,
                            'error': f'Ollama –≤–µ—Ä–Ω—É–ª HTTP 200, –Ω–æ –Ω–µ–≤–∞–ª–∏–¥–Ω—ã–π JSON: {error_text[:200]}',
                            'error_type': 'invalid_json'
                        }
                else:
                    # –ü—ã—Ç–∞–µ–º—Å—è –ø–æ–ª—É—á–∏—Ç—å –¥–µ—Ç–∞–ª—å–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ–± –æ—à–∏–±–∫–µ
                    error_detail = f'HTTP {response.status_code}'
                    error_text = None

                    try:
                        # –°–Ω–∞—á–∞–ª–∞ –ø–æ–ª—É—á–∞–µ–º —Å—ã—Ä–æ–π —Ç–µ–∫—Å—Ç –æ—Ç–≤–µ—Ç–∞
                        error_text = response.text
                        logger.error(f"Ollama raw response (first 1000 chars): {error_text[:1000]}")

                        # –ü—ã—Ç–∞–µ–º—Å—è —Ä–∞—Å–ø–∞—Ä—Å–∏—Ç—å –∫–∞–∫ JSON
                        error_data = response.json()
                        if 'error' in error_data:
                            error_detail = error_data['error']
                        logger.error(f"Ollama error response JSON: {error_data}")
                    except json.JSONDecodeError as je:
                        # –ù–µ JSON - –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ç–µ–∫—Å—Ç
                        if error_text:
                            error_detail = f'HTTP {response.status_code}: {error_text[:500]}'
                            logger.error(f"Ollama returned non-JSON response. JSON decode error: {je}")
                        else:
                            logger.error(f"Failed to decode JSON and no text available")
                    except Exception as e:
                        logger.error(f"Unexpected error parsing response: {e}")
                        if error_text:
                            error_detail = f'HTTP {response.status_code}: {error_text[:500]}'

                    logger.error(f"Ollama request failed: {error_detail}")
                    logger.error(f"Model: {self.model.model_id}, Endpoint: {self.model.api_endpoint}")
                    logger.error(f"Context size: {num_ctx}, Max tokens: {max_tokens}")
                    logger.error(f"Response headers: {dict(response.headers)}")

                    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø –æ—à–∏–±–∫–∏ –¥–ª—è —Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏
                    error_type = 'general'
                    error_detail_lower = error_detail.lower()

                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ —Ä–∞–∑–Ω—ã–µ —Ç–∏–ø—ã –ª–∏–º–∏—Ç–æ–≤
                    if 'weekly usage limit' in error_detail_lower:
                        error_type = 'weekly_limit'
                        logger.error(f"üö´ –û–±–Ω–∞—Ä—É–∂–µ–Ω –ù–ï–î–ï–õ–¨–ù–´–ô –ª–∏–º–∏—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è Ollama –º–æ–¥–µ–ª–∏")
                    elif 'daily usage limit' in error_detail_lower:
                        error_type = 'daily_limit'
                        logger.error(f"üö´ –û–±–Ω–∞—Ä—É–∂–µ–Ω –î–ù–ï–í–ù–û–ô –ª–∏–º–∏—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è Ollama –º–æ–¥–µ–ª–∏")
                    elif 'hourly usage limit' in error_detail_lower or 'usage limit' in error_detail_lower:
                        error_type = 'rate_limit'
                        logger.error(f"‚ö†Ô∏è –û–±–Ω–∞—Ä—É–∂–µ–Ω–∞ –æ—à–∏–±–∫–∞ –ª–∏–º–∏—Ç–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è Ollama –º–æ–¥–µ–ª–∏")
                    elif 'upstream timeout' in error_detail_lower or response.status_code == 504:
                        error_type = 'upstream_timeout'
                        logger.error(f"‚ö†Ô∏è –û–±–Ω–∞—Ä—É–∂–µ–Ω–∞ –æ—à–∏–±–∫–∞ upstream timeout (504) - —Å–µ—Ä–≤–µ—Ä –Ω–µ –æ—Ç–≤–µ—Ç–∏–ª –≤–æ–≤—Ä–µ–º—è")
                    elif 'upstream error' in error_detail_lower or response.status_code == 502:
                        error_type = 'upstream_error'
                        logger.error(f"‚ö†Ô∏è –û–±–Ω–∞—Ä—É–∂–µ–Ω–∞ –æ—à–∏–±–∫–∞ upstream (502) - –≤—Ä–µ–º–µ–Ω–Ω–∞—è –ø—Ä–æ–±–ª–µ–º–∞ —Å–µ—Ä–≤–µ—Ä–∞")
                    elif 'unmarshal' in error_detail_lower or 'unexpected end of json' in error_detail_lower or response.status_code == 500:
                        error_type = 'server_error'
                        logger.error(f"‚ö†Ô∏è –û–±–Ω–∞—Ä—É–∂–µ–Ω–∞ –≤–Ω—É—Ç—Ä–µ–Ω–Ω—è—è –æ—à–∏–±–∫–∞ —Å–µ—Ä–≤–µ—Ä–∞ (–Ω–µ–≤–∞–ª–∏–¥–Ω—ã–π JSON –∏–ª–∏ –ø—Ä–µ—Ä–≤–∞–Ω–Ω—ã–π –æ—Ç–≤–µ—Ç)")
                    elif 'not found' in error_detail_lower:
                        error_type = 'model_not_found'

                    return {
                        'success': False,
                        'error': f'–û—à–∏–±–∫–∞ Ollama: {error_detail}',
                        'error_type': error_type,
                        'status_code': response.status_code
                    }

        except httpx.TimeoutException as e:
            error_msg = f'–¢–∞–π–º–∞—É—Ç –ø—Ä–∏ –æ–±—Ä–∞—â–µ–Ω–∏–∏ –∫ Ollama (>1200s / 20 –º–∏–Ω—É—Ç). –ú–æ–¥–µ–ª—å: {self.model.model_id}'
            logger.error(error_msg)
            logger.error(f"–†–∞–∑–º–µ—Ä –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –±—ã–ª: {num_ctx if 'num_ctx' in locals() else 'unknown'}")
            return {
                'success': False,
                'error': error_msg,
                'error_type': 'timeout'
            }
        except httpx.ConnectError as e:
            error_msg = f'–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–¥–∫–ª—é—á–∏—Ç—å—Å—è –∫ Ollama —Å–µ—Ä–≤–µ—Ä—É: {self.model.api_endpoint}'
            logger.error(error_msg)
            logger.error(f"Connection error: {str(e)}")
            return {
                'success': False,
                'error': error_msg,
                'error_type': 'connection'
            }
        except Exception as e:
            error_msg = f'–ù–µ–æ–∂–∏–¥–∞–Ω–Ω–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–ø—Ä–æ—Å–µ –∫ Ollama: {type(e).__name__}: {str(e)}'
            logger.error(error_msg)
            import traceback
            logger.error(f"Traceback: {traceback.format_exc()}")
            return {
                'success': False,
                'error': error_msg,
                'error_type': 'unexpected'
            }

    async def _call_openrouter(self, system_prompt: str, user_prompt: str,
                               temperature: float, max_tokens: int) -> Dict:
        """–í—ã–∑–æ–≤ OpenRouter API (OpenAI-—Å–æ–≤–º–µ—Å—Ç–∏–º—ã–π —Ñ–æ—Ä–º–∞—Ç) —Å –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–º —Ä–∞—Å—á–µ—Ç–æ–º max_tokens"""
        if not self.model.api_key:
            return {'success': False, 'error': 'API –∫–ª—é—á –Ω–µ —É–∫–∞–∑–∞–Ω'}

        # –õ–æ–≥–∏—Ä—É–µ–º —Å—Ç–∞—Ç—É—Å thinking –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏
        if hasattr(self.model, 'enable_thinking'):
            logger.info(f"üîç enable_thinking = {self.model.enable_thinking} –¥–ª—è –º–æ–¥–µ–ª–∏ {self.model.model_id}")

        # üîß –î–ò–ù–ê–ú–ò–ß–ï–°–ö–ò–ô –†–ê–°–ß–ï–¢ max_tokens (–∫–∞–∫ –¥–ª—è Ollama)
        # –û–±—ä–µ–¥–∏–Ω—è–µ–º –ø—Ä–æ–º–ø—Ç—ã –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Ä–∞–∑–º–µ—Ä–∞
        full_prompt = f"{system_prompt}\n\n{user_prompt}"

        # –û—Ü–µ–Ω–∫–∞ –¥–ª–∏–Ω—ã –ø—Ä–æ–º–ø—Ç–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ —è–∑—ã–∫–∞
        prompt_length = self._estimate_tokens(full_prompt)

        # –î–ª—è –ø–µ—Ä–µ–≤–æ–¥–∞: –≤—ã—Ö–æ–¥ –æ–±—ã—á–Ω–æ ‚âà –≤—Ö–æ–¥ √ó 1.5 (–∫–∏—Ç–∞–π—Å–∫–∏–π ‚Üí —Ä—É—Å—Å–∫–∏–π)
        # –î–ª—è reasoning –º–æ–¥–µ–ª–µ–π: –Ω—É–∂–Ω–æ √ó 4.0 –∏–∑-–∑–∞ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–µ–≥–æ –ø—Ä–æ—Ü–µ—Å—Å–∞ –º—ã—à–ª–µ–Ω–∏—è
        if hasattr(self.model, 'enable_thinking') and self.model.enable_thinking:
            multiplier = 4.0  # Reasoning –º–æ–¥–µ–ª–∏ —Ç—Ä–µ–±—É—é—Ç –±–æ–ª—å—à–µ —Ç–æ–∫–µ–Ω–æ–≤
            logger.info(f"  üß† Reasoning –º–æ–¥–µ–ª—å: –∏—Å–ø–æ–ª—å–∑—É–µ–º multiplier √ó {multiplier}")
        else:
            multiplier = 1.5  # –û–±—ã—á–Ω—ã–µ –º–æ–¥–µ–ª–∏

        estimated_output = int(prompt_length * multiplier)

        # –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è:
        # 1. –ù–µ –±–æ–ª—å—à–µ max_output_tokens –º–æ–¥–µ–ª–∏
        # 2. –ù–µ –±–æ–ª—å—à–µ 16,384 –¥–ª—è –±–µ—Å–ø–ª–∞—Ç–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π (—á—Ç–æ–±—ã –∏–∑–±–µ–∂–∞—Ç—å rate limit)
        # 3. –ú–∏–Ω–∏–º—É–º 2,048 —Ç–æ–∫–µ–Ω–æ–≤ –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
        actual_max_tokens = min(estimated_output, self.model.max_output_tokens, 16384)

        if actual_max_tokens < 2048:
            actual_max_tokens = 2048

        # –õ–æ–≥–∏—Ä—É–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∑–∞–ø—Ä–æ—Å–∞ —Å –ø—Ä–µ—Ñ–∏–∫—Å–æ–º –≥–ª–∞–≤—ã
        log_prefix = ""
        if self.chapter_id:
            from app.models import Chapter
            chapter = Chapter.query.get(self.chapter_id)
            if chapter:
                log_prefix = f"[Novel:{chapter.novel_id}, Ch:{chapter.chapter_number}] "

        logger.info(f"OpenRouter –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–π —Ä–∞—Å—á–µ—Ç –¥–ª—è {self.model.name}:")
        logger.info(f"  üìù –†–∞–∑–º–µ—Ä –ø—Ä–æ–º–ø—Ç–∞: ~{prompt_length:,} —Ç–æ–∫–µ–Ω–æ–≤")
        logger.info(f"  üìè –†–∞—Å—á–µ—Ç–Ω—ã–π –≤—ã—Ö–æ–¥ (–ø—Ä–æ–º–ø—Ç √ó {multiplier}): {estimated_output:,} —Ç–æ–∫–µ–Ω–æ–≤")
        logger.info(f"  üîß –ó–∞–ø—Ä–æ—Å max_tokens: {actual_max_tokens:,} —Ç–æ–∫–µ–Ω–æ–≤")
        logger.info(f"  üìä –õ–∏–º–∏—Ç –º–æ–¥–µ–ª–∏: {self.model.max_output_tokens:,} —Ç–æ–∫–µ–Ω–æ–≤")

        LogService.log_info(f"{log_prefix}OpenRouter –∑–∞–ø—Ä–æ—Å: {self.model.model_id} | Temperature: {temperature} | Max tokens: {actual_max_tokens:,} (–¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–π) / {self.model.max_output_tokens:,}")

        try:
            async with httpx.AsyncClient(timeout=120.0) as client:
                response = await client.post(
                    'https://openrouter.ai/api/v1/chat/completions',
                    headers={
                        'Authorization': f'Bearer {self.model.api_key}',
                        'Content-Type': 'application/json',
                        'HTTP-Referer': 'https://github.com/novelbins/novelbins-epub',  # –û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ
                        'X-Title': 'NovelBins EPUB Translator'  # –û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ
                    },
                    json={
                        'model': self.model.model_id,
                        'messages': [
                            {'role': 'system', 'content': system_prompt},
                            {'role': 'user', 'content': user_prompt}
                        ],
                        'temperature': temperature,
                        'max_tokens': actual_max_tokens,
                        # –î–ª—è reasoning –º–æ–¥–µ–ª–µ–π: –∏—Å–∫–ª—é—á–∞–µ–º reasoning –∏–∑ –æ—Ç–≤–µ—Ç–∞, –æ—Å—Ç–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ —Ñ–∏–Ω–∞–ª—å–Ω—ã–π content
                        'reasoning': {
                            'effort': 'high',
                            'exclude': True  # –ú–æ–¥–µ–ª—å –¥—É–º–∞–µ—Ç –≤–Ω—É—Ç—Ä–µ–Ω–Ω–µ, –Ω–æ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ç–æ–ª—å–∫–æ content
                        }
                    }
                )

                if response.status_code == 200:
                    data = response.json()

                    # üîç –û–¢–õ–ê–î–û–ß–ù–û–ï –õ–û–ì–ò–†–û–í–ê–ù–ò–ï: –ü–æ–ª–Ω–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ –æ—Ç–≤–µ—Ç–∞
                    logger.info(f"üîç DEBUG: Response keys: {list(data.keys())}")
                    if 'choices' in data:
                        logger.info(f"üîç DEBUG: Choices count: {len(data['choices'])}")
                        if data['choices']:
                            first_choice = data['choices'][0]
                            logger.info(f"üîç DEBUG: First choice keys: {list(first_choice.keys())}")
                            if 'message' in first_choice:
                                msg = first_choice['message']
                                logger.info(f"üîç DEBUG: Message keys: {list(msg.keys())}")
                                logger.info(f"üîç DEBUG: Content length: {len(msg.get('content', ''))}")
                                logger.info(f"üîç DEBUG: Content preview: {msg.get('content', '')[:200]}")
                                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–æ–ª–µ reasoning (–Ω–µ reasoning_details!)
                                if 'reasoning' in msg:
                                    logger.info(f"üîç DEBUG: Reasoning field exists!")
                                    logger.info(f"üîç DEBUG: Reasoning length: {len(msg.get('reasoning', ''))}")
                                    logger.info(f"üîç DEBUG: Reasoning preview: {msg.get('reasoning', '')[:200]}")

                    choices = data.get('choices', [])
                    if choices:
                        message = choices[0].get('message', {})
                        content = message.get('content', '')

                        # üîß –õ–û–ì–ò–†–û–í–ê–ù–ò–ï REASONING –ú–û–î–ï–õ–ï–ô (—Ç–æ–ª—å–∫–æ –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏)
                        # –î–ª—è reasoning –º–æ–¥–µ–ª–µ–π (kimi-k2-thinking, deepseek-reasoner, o1, o3, Claude 3.7+)
                        # —Ñ–∏–Ω–∞–ª—å–Ω—ã–π –æ—Ç–≤–µ—Ç –í–°–ï–ì–î–ê –≤ content, reasoning_details —Å–æ–¥–µ—Ä–∂–∏—Ç –ø—Ä–æ—Ü–µ—Å—Å –º—ã—à–ª–µ–Ω–∏—è
                        if hasattr(self.model, 'enable_thinking') and self.model.enable_thinking:
                            reasoning_details = message.get('reasoning_details', [])

                            # –õ–æ–≥–∏—Ä—É–µ–º –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏ (–Ω–æ –ù–ï –∏—Å–ø–æ–ª—å–∑—É–µ–º reasoning_details –∫–∞–∫ –∫–æ–Ω—Ç–µ–Ω—Ç!)
                            if reasoning_details:
                                reasoning_length = sum(len(d.get('text', '')) for d in reasoning_details if d.get('type') == 'reasoning.text')
                                logger.info(f"üß† Reasoning –º–æ–¥–µ–ª—å: –ø–æ–ª—É—á–µ–Ω–æ {len(reasoning_details)} reasoning –±–ª–æ–∫–æ–≤ (–ø—Ä–æ—Ü–µ—Å—Å –º—ã—à–ª–µ–Ω–∏—è)")
                                logger.info(f"   Reasoning –¥–ª–∏–Ω–∞: {reasoning_length} —Å–∏–º–≤–æ–ª–æ–≤ (–ù–ï –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –≤ –ø–µ—Ä–µ–≤–æ–¥–µ)")
                                logger.info(f"   Content –¥–ª–∏–Ω–∞: {len(content)} —Å–∏–º–≤–æ–ª–æ–≤ (—Ñ–∏–Ω–∞–ª—å–Ω—ã–π –æ—Ç–≤–µ—Ç)")
                                logger.info(f"   Reasoning types: {[d.get('type') for d in reasoning_details]}")

                                # üîç DEBUG: –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ reasoning_details
                                for idx, detail in enumerate(reasoning_details):
                                    detail_text = detail.get('text', '')
                                    logger.info(f"   üîç Reasoning_detail[{idx}] length: {len(detail_text)}")
                                    logger.info(f"   üîç Reasoning_detail[{idx}] first 300 chars: {detail_text[:300]}")
                                    logger.info(f"   üîç Reasoning_detail[{idx}] last 500 chars: {detail_text[-500:]}")

                        # üîç –§–ò–ù–ê–õ–¨–ù–û–ï –õ–û–ì–ò–†–û–í–ê–ù–ò–ï –ø–µ—Ä–µ–¥ –≤–æ–∑–≤—Ä–∞—Ç–æ–º
                        logger.info(f"üîç DEBUG: Returning content length: {len(content)}")
                        logger.info(f"üîç DEBUG: Content is empty: {not content}")
                        logger.info(f"üîç DEBUG: Content is None: {content is None}")

                        return {
                            'success': True,
                            'content': content,
                            'usage': data.get('usage', {}),
                            'finish_reason': choices[0].get('finish_reason', 'unknown')
                        }
                    else:
                        return {'success': False, 'error': '–ù–µ—Ç –≤–∞—Ä–∏–∞–Ω—Ç–æ–≤ –≤ –æ—Ç–≤–µ—Ç–µ'}
                else:
                    # –û–±—Ä–∞–±–æ—Ç–∫–∞ –æ—à–∏–±–æ–∫
                    error_detail = f'HTTP {response.status_code}'
                    try:
                        error_data = response.json()
                        if 'error' in error_data:
                            error_message = error_data['error']
                            if isinstance(error_message, dict):
                                error_detail = error_message.get('message', str(error_message))
                            else:
                                error_detail = str(error_message)
                    except:
                        error_detail = response.text[:500]

                    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø –æ—à–∏–±–∫–∏
                    error_type = 'general'
                    error_lower = error_detail.lower()

                    if response.status_code == 429:
                        error_type = 'rate_limit'
                    elif response.status_code == 401:
                        error_type = 'auth_error'
                    elif response.status_code == 402:
                        error_type = 'insufficient_credits'
                    elif 'weekly usage limit' in error_lower or 'weekly limit' in error_lower:
                        error_type = 'weekly_limit'
                    elif 'daily usage limit' in error_lower or 'daily limit' in error_lower:
                        error_type = 'daily_limit'

                    logger.error(f"OpenRouter error: {error_detail} (type: {error_type})")

                    return {
                        'success': False,
                        'error': f'–û—à–∏–±–∫–∞ OpenRouter: {error_detail}',
                        'error_type': error_type,
                        'status_code': response.status_code
                    }

        except httpx.TimeoutException:
            error_msg = f'–¢–∞–π–º–∞—É—Ç –ø—Ä–∏ –æ–±—Ä–∞—â–µ–Ω–∏–∏ –∫ OpenRouter (>120s). –ú–æ–¥–µ–ª—å: {self.model.model_id}'
            logger.error(error_msg)
            return {
                'success': False,
                'error': error_msg,
                'error_type': 'timeout'
            }
        except httpx.ConnectError as e:
            error_msg = f'–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–¥–∫–ª—é—á–∏—Ç—å—Å—è –∫ OpenRouter API'
            logger.error(f"{error_msg}: {str(e)}")
            return {
                'success': False,
                'error': error_msg,
                'error_type': 'connection'
            }
        except Exception as e:
            error_msg = f'–ù–µ–æ–∂–∏–¥–∞–Ω–Ω–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–ø—Ä–æ—Å–µ –∫ OpenRouter: {type(e).__name__}: {str(e)}'
            logger.error(error_msg)
            import traceback
            logger.error(f"Traceback: {traceback.format_exc()}")
            return {
                'success': False,
                'error': error_msg,
                'error_type': 'unexpected'
            }

    @staticmethod
    def get_available_models(provider: str = None, active_only: bool = True) -> List[AIModel]:
        """–ü–æ–ª—É—á–∏—Ç—å —Å–ø–∏—Å–æ–∫ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π"""
        query = AIModel.query

        if active_only:
            query = query.filter_by(is_active=True)

        if provider:
            query = query.filter_by(provider=provider)

        return query.all()

    @staticmethod
    def get_default_model() -> Optional[AIModel]:
        """–ü–æ–ª—É—á–∏—Ç—å –º–æ–¥–µ–ª—å –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é"""
        return AIModel.query.filter_by(is_default=True).first()