"""
2_translator_improved_filtered.py - –ü–µ—Ä–µ–≤–æ–¥—á–∏–∫ —Å –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω–æ–π —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–µ–π –≥–ª–æ—Å—Å–∞—Ä–∏—è
–ü—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–∞–µ—Ç —Ä–æ—Å—Ç –≥–ª–æ—Å—Å–∞—Ä–∏—è –¥–ª—è –Ω–æ–≤—ã—Ö –Ω–æ–≤–µ–ª–ª
"""

import os
import time
import re
import json
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Optional, Tuple

# –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º —Ñ–∏–ª—å—Ç—Ä –≥–ª–æ—Å—Å–∞—Ä–∏—è
from glossary_filter import GlossaryFilter

# –û—Å—Ç–∞–ª—å–Ω—ã–µ –∏–º–ø–æ—Ä—Ç—ã –∏–∑ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–≥–æ —Ñ–∞–π–ª–∞
import httpx
from httpx_socks import SyncProxyTransport
from dotenv import load_dotenv
from models import Chapter, TranslatorConfig, GlossaryItem
from database import DatabaseManager

load_dotenv()

# ===================== –£–õ–£–ß–®–ï–ù–ù–´–ô –ü–†–û–ú–ü–¢ –î–õ–Ø –ò–ó–í–õ–ï–ß–ï–ù–ò–Ø =====================

EXTRACT_TERMS_PROMPT_STRICT = """–¢—ã —Ä–∞–±–æ—Ç–∞–µ—à—å —Å –∫–∏—Ç–∞–π—Å–∫–æ–π –≤–µ–±-–Ω–æ–≤–µ–ª–ª–æ–π –∂–∞–Ω—Ä–∞ —Å—è–Ω—å—Å—è/—Å—é–∞–Ω—å—Ö—É–∞–Ω—å.

–ö–†–ò–¢–ò–ß–ï–°–ö–ò –í–ê–ñ–ù–û: –ò–∑–≤–ª–µ–∫–∞–π –¢–û–õ–¨–ö–û —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ –∏ —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ –¥–ª—è —ç—Ç–æ–π –Ω–æ–≤–µ–ª–ª—ã —ç–ª–µ–º–µ–Ω—Ç—ã!

–ù–ï –î–û–ë–ê–í–õ–Ø–ô:
‚ùå –û–±—â–∏–µ —Å–ª–æ–≤–∞ (—á–µ–ª–æ–≤–µ–∫, –≤—Ä–µ–º—è, –º–µ—Å—Ç–æ, –≤–µ—â—å)
‚ùå –≠–º–æ—Ü–∏–∏ (—É–ª—ã–±–∫–∞, –≥—Ä—É—Å—Ç—å, —Ä–∞–¥–æ—Å—Ç—å, —Å—Ç—Ä–∞—Ö)
‚ùå –ß–∞—Å—Ç–∏ —Ç–µ–ª–∞ (—Ä—É–∫–∞, –Ω–æ–≥–∞, –≥–æ–ª–æ–≤–∞, –≥–ª–∞–∑)
‚ùå –ü—Ä–æ—Å—Ç—ã–µ –¥–µ–π—Å—Ç–≤–∏—è (–∏–¥—Ç–∏, —Å–º–æ—Ç—Ä–µ—Ç—å, –≥–æ–≤–æ—Ä–∏—Ç—å, –¥—É–º–∞—Ç—å)
‚ùå –ë–∞–∑–æ–≤—ã–µ –∫–∞—á–µ—Å—Ç–≤–∞ (–±–æ–ª—å—à–æ–π, –º–∞–ª–µ–Ω—å–∫–∏–π, —Å–∏–ª—å–Ω—ã–π, —Å–ª–∞–±—ã–π)
‚ùå –í—Ä–µ–º—è (–¥–µ–Ω—å, –Ω–æ—á—å, —É—Ç—Ä–æ, –≤–µ—á–µ—Ä, –º–æ–º–µ–Ω—Ç)
‚ùå –ù–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è (–≤–ø–µ—Ä–µ–¥, –Ω–∞–∑–∞–¥, –≤–≤–µ—Ä—Ö, –≤–Ω–∏–∑)

–î–û–ë–ê–í–õ–Ø–ô –¢–û–õ–¨–ö–û:
‚úÖ –ò–º–µ–Ω–∞ –≤–∞–∂–Ω—ã—Ö –ø–µ—Ä—Å–æ–Ω–∞–∂–µ–π (–ù–ï "–º–æ–ª–æ–¥–æ–π —á–µ–ª–æ–≤–µ–∫", –∞ "–í–∞–Ω –õ–∏–Ω—å")
‚úÖ –ù–∞–∑–≤–∞–Ω–∏—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã—Ö –º–µ—Å—Ç (–ù–ï "–≥–æ—Ä–∞", –∞ "–ü–∏–∫ –ù–µ–±–µ—Å–Ω–æ–≥–æ –ú–µ—á–∞")
‚úÖ –£–Ω–∏–∫–∞–ª—å–Ω—ã–µ —Ç–µ—Ö–Ω–∏–∫–∏ –∫—É–ª—å—Ç–∏–≤–∞—Ü–∏–∏ (–ù–ï "—É–¥–∞—Ä", –∞ "–ö—É–ª–∞–∫ –†–∞–∑—Ä—É—à–µ–Ω–∏—è –ù–µ–±–µ—Å")
‚úÖ –°–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ –∞—Ä—Ç–µ—Ñ–∞–∫—Ç—ã (–ù–ï "–º–µ—á", –∞ "–ú–µ—á –õ–µ–¥—è–Ω–æ–π –î—É—à–∏")
‚úÖ –¢–µ—Ä–º–∏–Ω—ã –∫—É–ª—å—Ç–∏–≤–∞—Ü–∏–∏ –°–ü–ï–¶–ò–§–ò–ß–ù–´–ï –¥–ª—è —ç—Ç–æ–π –Ω–æ–≤–µ–ª–ª—ã

–ü—Ä–æ–≤–µ—Ä—å –∫–∞–∂–¥—ã–π —Ç–µ—Ä–º–∏–Ω:
1. –≠—Ç–æ –¥–µ–π—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ –£–ù–ò–ö–ê–õ–¨–ù–û –¥–ª—è —ç—Ç–æ–π –Ω–æ–≤–µ–ª–ª—ã?
2. –≠—Ç–æ –ù–ï –æ–±—â–µ—É–ø–æ—Ç—Ä–µ–±–∏—Ç–µ–ª—å–Ω–æ–µ —Å–ª–æ–≤–æ?
3. –≠—Ç–æ –±—É–¥–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å—Å—è –º–Ω–æ–≥–æ–∫—Ä–∞—Ç–Ω–æ –≤ —Ç–µ–∫—Å—Ç–µ?

–ï—Å–ª–∏ —Ç–µ—Ä–º–∏–Ω –Ω–µ –ø—Ä–æ—Ö–æ–¥–∏—Ç –í–°–ï —Ç—Ä–∏ –ø—Ä–æ–≤–µ—Ä–∫–∏ - –ù–ï –î–û–ë–ê–í–õ–Ø–ô –µ–≥–æ!

–§–æ—Ä–º–∞—Ç –æ—Ç–≤–µ—Ç–∞:
–ü–ï–†–°–û–ù–ê–ñ–ò:
- Chinese Name = –†—É—Å—Å–∫–∏–π –ü–µ—Ä–µ–≤–æ–¥

–õ–û–ö–ê–¶–ò–ò:
- Chinese Location = –†—É—Å—Å–∫–∞—è –õ–æ–∫–∞—Ü–∏—è

–¢–ï–†–ú–ò–ù–´:
- Chinese Term = –†—É—Å—Å–∫–∏–π –¢–µ—Ä–º–∏–Ω

–ï—Å–ª–∏ –Ω–æ–≤—ã—Ö –í–ê–ñ–ù–´–• —ç–ª–µ–º–µ–Ω—Ç–æ–≤ –Ω–µ—Ç, –ø–∏—à–∏:
–ö–ê–¢–ï–ì–û–†–ò–Ø:
- –Ω–µ—Ç –Ω–æ–≤—ã—Ö"""

# ===================== –ö–õ–ê–°–° –ü–ï–†–ï–í–û–î–ß–ò–ö–ê –° –§–ò–õ–¨–¢–†–ê–¶–ò–ï–ô =====================

class FilteredTranslator:
    """–ü–µ—Ä–µ–≤–æ–¥—á–∏–∫ —Å –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω–æ–π —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–µ–π –≥–ª–æ—Å—Å–∞—Ä–∏—è"""
    
    def __init__(self, config: TranslatorConfig):
        self.config = config
        self.glossary_filter = GlossaryFilter()
        self.stats = {
            'added': 0,
            'filtered': 0,
            'duplicates': 0
        }
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –æ—Å—Ç–∞–ª—å–Ω—ã—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤...
        self.db = DatabaseManager(config.db_path)
        
    def process_extracted_terms(self, extraction_result: str, chapter_num: int, novel_id: int) -> Tuple[int, int]:
        """
        –û–±—Ä–∞–±–æ—Ç–∫–∞ –∏–∑–≤–ª–µ—á—ë–Ω–Ω—ã—Ö —Ç–µ—Ä–º–∏–Ω–æ–≤ —Å –∂—ë—Å—Ç–∫–æ–π —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–µ–π
        
        Returns:
            (added_count, filtered_count)
        """
        
        if not extraction_result:
            return 0, 0
            
        added_count = 0
        filtered_count = 0
        
        # –ü–∞—Ä—Å–∏–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç –∏–∑–≤–ª–µ—á–µ–Ω–∏—è
        terms = self.parse_extraction_result(extraction_result)
        
        for category, items in terms.items():
            if not items or items == [('–Ω–µ—Ç –Ω–æ–≤—ã—Ö', '–Ω–µ—Ç –Ω–æ–≤—ã—Ö')]:
                continue
                
            for chinese, russian in items:
                # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –ø—É—Å—Ç—ã–µ
                if not chinese or not russian or chinese == '–Ω–µ—Ç –Ω–æ–≤—ã—Ö':
                    continue
                
                # –û—á–∏—â–∞–µ–º –ø–µ—Ä–µ–≤–æ–¥
                russian = self.glossary_filter.clean_russian_translation(russian)
                chinese = chinese.strip()
                
                # === –§–ò–õ–¨–¢–† 1: –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –¥—É–±–ª–∏–∫–∞—Ç—ã –ø–æ —Ä—É—Å—Å–∫–æ–º—É –ø–µ—Ä–µ–≤–æ–¥—É ===
                existing = self.check_duplicate(russian, novel_id)
                if existing:
                    self.stats['duplicates'] += 1
                    filtered_count += 1
                    print(f"    üîÑ –î—É–±–ª–∏–∫–∞—Ç: {chinese} = {russian} (—É–∂–µ –µ—Å—Ç—å)")
                    continue
                
                # === –§–ò–õ–¨–¢–† 2: –ò–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ ===
                should_add, reason = self.glossary_filter.should_add_term(
                    chinese, russian, category
                )
                
                if not should_add:
                    self.stats['filtered'] += 1
                    filtered_count += 1
                    print(f"    ‚ùå –û—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω: {chinese} = {russian}")
                    print(f"       –ü—Ä–∏—á–∏–Ω–∞: {reason}")
                    continue
                
                # === –§–ò–õ–¨–¢–† 3: –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –¥–ª—è –Ω–æ–≤—ã—Ö –Ω–æ–≤–µ–ª–ª ===
                if not self.is_novel_specific(chinese, russian, category):
                    filtered_count += 1
                    print(f"    ‚ö†Ô∏è –ù–µ —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω–æ: {chinese} = {russian}")
                    continue
                
                # –ï—Å–ª–∏ –ø—Ä–æ—à—ë–ª –≤—Å–µ —Ñ–∏–ª—å—Ç—Ä—ã - –¥–æ–±–∞–≤–ª—è–µ–º
                self.add_to_glossary(chinese, russian, category, chapter_num, novel_id)
                self.stats['added'] += 1
                added_count += 1
                print(f"    ‚úÖ –î–æ–±–∞–≤–ª–µ–Ω: {chinese} = {russian} ({category})")
        
        return added_count, filtered_count
    
    def check_duplicate(self, russian: str, novel_id: int) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –¥—É–±–ª–∏–∫–∞—Ç –ø–æ —Ä—É—Å—Å–∫–æ–º—É –ø–µ—Ä–µ–≤–æ–¥—É"""
        # –ó–¥–µ—Å—å –¥–æ–ª–∂–Ω–∞ –±—ã—Ç—å –ø—Ä–æ–≤–µ—Ä–∫–∞ –≤ –ë–î
        # –î–ª—è –ø—Ä–∏–º–µ—Ä–∞ - –∑–∞–≥–ª—É—à–∫–∞
        return False
    
    def is_novel_specific(self, chinese: str, russian: str, category: str) -> bool:
        """
        –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ - –¥–µ–π—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ –ª–∏ —Ç–µ—Ä–º–∏–Ω —Å–ø–µ—Ü–∏—Ñ–∏—á–µ–Ω –¥–ª—è –Ω–æ–≤–µ–ª–ª—ã
        """
        russian_lower = russian.lower()
        
        # –°–ø–∏—Å–æ–∫ —Å–ª–∏—à–∫–æ–º –æ–±—â–∏—Ö —Ç–µ—Ä–º–∏–Ω–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ –µ—Å—Ç—å –≤–æ –≤—Å–µ—Ö –Ω–æ–≤–µ–ª–ª–∞—Ö
        GENERIC_TERMS = {
            '—Å—Ç–∞—Ä–µ–π—à–∏–Ω–∞', '—É—á–µ–Ω–∏–∫', '–Ω–∞—Å—Ç–∞–≤–Ω–∏–∫', '–ø–∞—Ç—Ä–∏–∞—Ä—Ö',
            '–≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏–π —É—á–µ–Ω–∏–∫', '–≤–Ω–µ—à–Ω–∏–π —É—á–µ–Ω–∏–∫',
            '–¥—É—Ö–æ–≤–Ω–∞—è —ç–Ω–µ—Ä–≥–∏—è', '–¥—É—Ö–æ–≤–Ω–∞—è —Å–∏–ª–∞',
            '–ø—Ä–æ—Ä—ã–≤', '–∫—É–ª—å—Ç–∏–≤–∞—Ü–∏—è', '–º–µ–¥–∏—Ç–∞—Ü–∏—è',
            '–ø–∏–ª—é–ª—è', '—ç–ª–∏–∫—Å–∏—Ä', '—Ç–∞–ª–∏—Å–º–∞–Ω'
        }
        
        # –ï—Å–ª–∏ —ç—Ç–æ —Å–ª–∏—à–∫–æ–º –æ–±—â–∏–π —Ç–µ—Ä–º–∏–Ω - –Ω–µ –¥–æ–±–∞–≤–ª—è–µ–º
        for generic in GENERIC_TERMS:
            if generic in russian_lower:
                # –ù–æ –µ—Å–ª–∏ –µ—Å—Ç—å —É–Ω–∏–∫–∞–ª—å–Ω—ã–π –º–æ–¥–∏—Ñ–∏–∫–∞—Ç–æ—Ä - –¥–æ–±–∞–≤–ª—è–µ–º
                unique_modifiers = ['–Ω–µ–±–µ—Å–Ω', '–±–æ–∂–µ—Å—Ç–≤–µ–Ω', '–¥–µ–º–æ–Ω–∏—á–µ—Å–∫', '–¥—Ä–∞–∫–æ–Ω–∏–π',
                                  '–∏–º–ø–µ—Ä–∞—Ç–æ—Ä—Å–∫', '–¥—Ä–µ–≤–Ω', '–∑–∞–ø—Ä–µ—Ç–Ω', '—Ç–∞–π–Ω']
                if not any(mod in russian_lower for mod in unique_modifiers):
                    return False
        
        # –î–ª—è –ø–µ—Ä—Å–æ–Ω–∞–∂–µ–π - –ø—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —ç—Ç–æ –ø–æ–ª–Ω–æ–µ –∏–º—è
        if category == 'characters':
            # –î–æ–ª–∂–Ω–æ –±—ã—Ç—å —Ö–æ—Ç—è –±—ã 2 —á–∞—Å—Ç–∏ (–∏–º—è + —Ñ–∞–º–∏–ª–∏—è) –∏–ª–∏ —É–Ω–∏–∫–∞–ª—å–Ω–æ–µ –ø—Ä–æ–∑–≤–∏—â–µ
            if len(russian.split()) < 2 and len(chinese) < 3:
                return False
        
        # –î–ª—è —Ç–µ—Ö–Ω–∏–∫ - –¥–æ–ª–∂–Ω–æ –±—ã—Ç—å –æ–ø–∏—Å–∞—Ç–µ–ª—å–Ω–æ–µ –Ω–∞–∑–≤–∞–Ω–∏–µ
        if category == 'techniques':
            if len(russian.split()) < 2:
                return False
        
        return True
    
    def add_to_glossary(self, chinese: str, russian: str, category: str, 
                       chapter_num: int, novel_id: int):
        """–î–æ–±–∞–≤–ª–µ–Ω–∏–µ —Ç–µ—Ä–º–∏–Ω–∞ –≤ –≥–ª–æ—Å—Å–∞—Ä–∏–π –ø–æ—Å–ª–µ –≤—Å–µ—Ö –ø—Ä–æ–≤–µ—Ä–æ–∫"""
        # –ó–¥–µ—Å—å –∫–æ–¥ –¥–æ–±–∞–≤–ª–µ–Ω–∏—è –≤ –ë–î
        item = GlossaryItem(
            english=chinese,
            russian=russian, 
            category=category,
            first_appearance=chapter_num
        )
        self.db.save_glossary_item(item)
    
    def parse_extraction_result(self, result: str) -> Dict[str, List[Tuple[str, str]]]:
        """–ü–∞—Ä—Å–∏–Ω–≥ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Ç–µ—Ä–º–∏–Ω–æ–≤"""
        terms = {
            'characters': [],
            'locations': [],
            'terms': [],
            'techniques': [],
            'artifacts': []
        }
        
        current_category = None
        lines = result.split('\n')
        
        for line in lines:
            line = line.strip()
            if not line:
                continue
                
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∫–∞—Ç–µ–≥–æ—Ä–∏—é
            if '–ü–ï–†–°–û–ù–ê–ñ' in line.upper():
                current_category = 'characters'
            elif '–õ–û–ö–ê–¶' in line.upper():
                current_category = 'locations'
            elif '–¢–ï–†–ú–ò–ù' in line.upper():
                current_category = 'terms'
            elif '–¢–ï–•–ù–ò–ö' in line.upper():
                current_category = 'techniques'
            elif '–ê–†–¢–ï–§–ê–ö–¢' in line.upper():
                current_category = 'artifacts'
            elif line.startswith('-') and '=' in line and current_category:
                # –ü–∞—Ä—Å–∏–º —Ç–µ—Ä–º–∏–Ω
                parts = line[1:].split('=')
                if len(parts) == 2:
                    chinese = parts[0].strip()
                    russian = parts[1].strip()
                    if chinese and russian and chinese != '–Ω–µ—Ç –Ω–æ–≤—ã—Ö':
                        terms[current_category].append((chinese, russian))
        
        return terms
    
    def print_stats(self):
        """–í—ã–≤–æ–¥ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏"""
        print("\n" + "=" * 60)
        print("üìä –°–¢–ê–¢–ò–°–¢–ò–ö–ê –§–ò–õ–¨–¢–†–ê–¶–ò–ò –ì–õ–û–°–°–ê–†–ò–Ø:")
        print(f"  ‚úÖ –î–æ–±–∞–≤–ª–µ–Ω–æ: {self.stats['added']}")
        print(f"  ‚ùå –û—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–æ: {self.stats['filtered']}")
        print(f"  üîÑ –î—É–±–ª–∏–∫–∞—Ç–æ–≤: {self.stats['duplicates']}")
        print(f"  üìà –ü—Ä–æ—Ü–µ–Ω—Ç —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏: {self.stats['filtered']/(self.stats['added']+self.stats['filtered'])*100:.1f}%")
        print("=" * 60)


# ===================== –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø –î–õ–Ø –ù–û–í–´–• –ù–û–í–ï–õ–õ =====================

def create_config_for_new_novel(novel_name: str) -> Dict:
    """–°–æ–∑–¥–∞–Ω–∏–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –¥–ª—è –Ω–æ–≤–æ–π –Ω–æ–≤–µ–ª–ª—ã —Å –∂—ë—Å—Ç–∫–æ–π —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–µ–π"""
    
    return {
        'novel_name': novel_name,
        'glossary_settings': {
            'max_terms': 3000,  # –ú–∞–∫—Å–∏–º—É–º —Ç–µ—Ä–º–∏–Ω–æ–≤ –≤ –≥–ª–æ—Å—Å–∞—Ä–∏–∏
            'min_usage_to_keep': 3,  # –ú–∏–Ω–∏–º—É–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–π –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
            'auto_cleanup_every': 100,  # –ê–≤—Ç–æ–æ—á–∏—Å—Ç–∫–∞ –∫–∞–∂–¥—ã–µ N –≥–ª–∞–≤
            'strict_filtering': True,  # –°—Ç—Ä–æ–≥–∞—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è
            'require_uniqueness': True,  # –¢—Ä–µ–±–æ–≤–∞—Ç—å —É–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç–∏
            'min_chinese_length': {
                'characters': 2,
                'locations': 3,
                'terms': 3,
                'techniques': 4,
                'artifacts': 3
            }
        },
        'extraction_prompt': EXTRACT_TERMS_PROMPT_STRICT,
        'filters': {
            'use_intelligent_filter': True,
            'check_duplicates': True,
            'check_novel_specificity': True,
            'block_common_words': True
        }
    }


# ===================== –ì–õ–ê–í–ù–ê–Ø –§–£–ù–ö–¶–ò–Ø =====================

def translate_novel_with_filter(novel_name: str, start_chapter: int = 1):
    """–ü–µ—Ä–µ–≤–æ–¥ –Ω–æ–≤–µ–ª–ª—ã —Å –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω–æ–π —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–µ–π –≥–ª–æ—Å—Å–∞—Ä–∏—è"""
    
    print(f"\n{'=' * 60}")
    print(f"üöÄ –ó–ê–ü–£–°–ö –ü–ï–†–ï–í–û–î–ê –° –§–ò–õ–¨–¢–†–ê–¶–ò–ï–ô –ì–õ–û–°–°–ê–†–ò–Ø")
    print(f"üìö –ù–æ–≤–µ–ª–ª–∞: {novel_name}")
    print(f"üîç –†–µ–∂–∏–º: –°—Ç—Ä–æ–≥–∞—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è")
    print(f"{'=' * 60}\n")
    
    # –°–æ–∑–¥–∞—ë–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
    config = create_config_for_new_novel(novel_name)
    
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –ø–µ—Ä–µ–≤–æ–¥—á–∏–∫ —Å —Ñ–∏–ª—å—Ç—Ä–æ–º
    translator = FilteredTranslator(TranslatorConfig())
    
    # –ó–¥–µ—Å—å –æ—Å–Ω–æ–≤–Ω–æ–π —Ü–∏–∫–ª –ø–µ—Ä–µ–≤–æ–¥–∞...
    # –î–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏ - –ø—Ä–∏–º–µ—Ä –æ–±—Ä–∞–±–æ—Ç–∫–∏ –æ–¥–Ω–æ–π –≥–ª–∞–≤—ã
    
    print("‚úÖ –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∞!")
    print("\nüìã –ü—Ä–∞–≤–∏–ª–∞ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏:")
    print("  1. –ë–ª–æ–∫–∏—Ä–æ–≤–∫–∞ –æ–±—â–∏—Ö —Å–ª–æ–≤ (175 —Å—Ç–æ–ø-—Å–ª–æ–≤)")
    print("  2. –ü—Ä–æ–≤–µ—Ä–∫–∞ –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–π –¥–ª–∏–Ω—ã —Ç–µ—Ä–º–∏–Ω–∞")
    print("  3. –î–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—è –ø–æ —Ä—É—Å—Å–∫–æ–º—É –ø–µ—Ä–µ–≤–æ–¥—É")
    print("  4. –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω–æ—Å—Ç–∏ –¥–ª—è –Ω–æ–≤–µ–ª–ª—ã")
    print("  5. –í–∞–ª–∏–¥–∞—Ü–∏—è –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º")
    
    translator.print_stats()
    
    return translator


if __name__ == "__main__":
    # –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –¥–ª—è –Ω–æ–≤–æ–π –Ω–æ–≤–µ–ª–ª—ã
    novel = input("–í–≤–µ–¥–∏—Ç–µ –Ω–∞–∑–≤–∞–Ω–∏–µ –Ω–æ–≤–æ–π –Ω–æ–≤–µ–ª–ª—ã: ").strip()
    if novel:
        translate_novel_with_filter(novel)
    else:
        print("‚ùå –ù–∞–∑–≤–∞–Ω–∏–µ –Ω–æ–≤–µ–ª–ª—ã –Ω–µ —É–∫–∞–∑–∞–Ω–æ")