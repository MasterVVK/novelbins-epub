#!/usr/bin/env python3
"""
–ê–Ω–∞–ª–∏–∑ —Ç–µ—Ö–Ω–∏–∫ –≤ –≥–ª–æ—Å—Å–∞—Ä–∏–∏ –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –ª–∏—à–Ω–∏—Ö
"""
import sys
sys.path.insert(0, '/home/user/novelbins-epub/web_app')

from app import create_app, db
from app.models import Novel, GlossaryItem
from collections import Counter, defaultdict
import re

def analyze_techniques():
    """–ê–Ω–∞–ª–∏–∑ —Ç–µ—Ö–Ω–∏–∫ –≤ –≥–ª–æ—Å—Å–∞—Ä–∏–∏"""
    
    app = create_app()
    with app.app_context():
        novel = Novel.query.filter(Novel.title.like('%–∑–∞–ø–µ—á–∞—Ç–∞—Ç—å%')).first()
        
        if not novel:
            print("‚ùå –ù–æ–≤–µ–ª–ª–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")
            return
        
        print(f"‚úÖ –ê–Ω–∞–ª–∏–∑ —Ç–µ—Ö–Ω–∏–∫ –¥–ª—è: {novel.title}")
        
        # –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ –∞–∫—Ç–∏–≤–Ω—ã–µ —Ç–µ—Ö–Ω–∏–∫–∏
        techniques = GlossaryItem.query.filter_by(
            novel_id=novel.id,
            category='techniques',
            is_active=True
        ).all()
        
        print(f"\nüìä –í—Å–µ–≥–æ —Ç–µ—Ö–Ω–∏–∫: {len(techniques)}")
        
        # –ö–∞—Ç–µ–≥–æ—Ä–∏–∑–∞—Ü–∏—è —Ç–µ—Ö–Ω–∏–∫
        generic_techniques = []  # –û–±—â–∏–µ —Ç–µ—Ö–Ω–∏–∫–∏
        specific_techniques = []  # –°–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ —Ç–µ—Ö–Ω–∏–∫–∏
        simple_actions = []  # –ü—Ä–æ—Å—Ç—ã–µ –¥–µ–π—Å—Ç–≤–∏—è
        duplicate_similar = defaultdict(list)  # –ü–æ—Ö–æ–∂–∏–µ/–¥—É–±–ª–∏—Ä—É—é—â–∏–µ—Å—è
        
        # –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –¥–ª—è –æ–±—â–∏—Ö —Ç–µ—Ö–Ω–∏–∫
        generic_keywords = [
            '—É–¥–∞—Ä', '–∞—Ç–∞–∫–∞', '–≤–∑—Ä—ã–≤', '–≤–æ–ª–Ω–∞', '–ª—É—á', '—â–∏—Ç', '–±–∞—Ä—å–µ—Ä',
            '–ø—Ä—ã–∂–æ–∫', '–ø–æ–ª–µ—Ç', '—Å–∫–æ—Ä–æ—Å—Ç—å', '—Å–∏–ª–∞', '–∑–∞—â–∏—Ç–∞', '–±–ª–æ–∫',
            '—É–∫–ª–æ–Ω–µ–Ω–∏–µ', '–±—Ä–æ—Å–æ–∫', '—Ç–æ–ª—á–æ–∫', '—Ä—ã–≤–æ–∫', '—É—Å–∫–æ—Ä–µ–Ω–∏–µ'
        ]
        
        # –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –¥–ª—è –≤–∞–∂–Ω—ã—Ö —Ç–µ—Ö–Ω–∏–∫
        important_keywords = [
            '–ø–µ—á–∞—Ç—å', '—Ñ–æ—Ä–º–∞—Ü–∏—è', '–º–∞—Å—Å–∏–≤', '–∑–∞–∫–ª–∏–Ω–∞–Ω–∏–µ', '–∏—Å–∫—É—Å—Å—Ç–≤–æ',
            '–º–µ—Ç–æ–¥', '–ø—É—Ç—å', '–¥–∞–æ', '—Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏—è', '–ø—Ä–µ–≤—Ä–∞—â–µ–Ω–∏–µ',
            '–±–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω', '–Ω–µ–±–µ—Å–Ω', '–¥–µ–º–æ–Ω–∏—á', '–¥—å—è–≤–æ–ª—å—Å–∫', '–¥—Ä–∞–∫–æ–Ω—å',
            '—Ñ–µ–Ω–∏–∫—Å', '–±–µ—Å—Å–º–µ—Ä—Ç–Ω', '–∑–∞–ø—Ä–µ—Ç–Ω', '—Ç–∞–π–Ω', '–¥—Ä–µ–≤–Ω'
        ]
        
        print("\nüîç –ê–Ω–∞–ª–∏–∑ —Ç–µ—Ö–Ω–∏–∫...")
        
        for tech in techniques:
            chinese = tech.english_term
            russian = tech.russian_term.lower()
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –ø—Ä–æ—Å—Ç—ã–µ –¥–µ–π—Å—Ç–≤–∏—è
            if len(chinese) <= 2 and not any(kw in russian for kw in important_keywords):
                simple_actions.append(tech)
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –æ–±—â–∏–µ —Ç–µ—Ö–Ω–∏–∫–∏
            elif any(kw in russian for kw in generic_keywords) and not any(kw in russian for kw in important_keywords):
                generic_techniques.append(tech)
            # –°–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ —Ç–µ—Ö–Ω–∏–∫–∏
            elif any(kw in russian for kw in important_keywords):
                specific_techniques.append(tech)
            else:
                # –ì—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ –ø–æ—Ö–æ–∂–∏—Ö
                base_name = russian.split()[0] if russian.split() else russian
                duplicate_similar[base_name].append(tech)
        
        # –í—ã–≤–æ–¥ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        print(f"\nüìã –†–ï–ó–£–õ–¨–¢–ê–¢–´ –ê–ù–ê–õ–ò–ó–ê:")
        print(f"  - –ü—Ä–æ—Å—Ç—ã–µ –¥–µ–π—Å—Ç–≤–∏—è: {len(simple_actions)}")
        print(f"  - –û–±—â–∏–µ —Ç–µ—Ö–Ω–∏–∫–∏: {len(generic_techniques)}")
        print(f"  - –°–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ —Ç–µ—Ö–Ω–∏–∫–∏: {len(specific_techniques)}")
        print(f"  - –ì—Ä—É–ø–ø—ã –ø–æ—Ö–æ–∂–∏—Ö: {len([g for g in duplicate_similar.values() if len(g) > 1])}")
        
        # –ü—Ä–∏–º–µ—Ä—ã –ø—Ä–æ—Å—Ç—ã—Ö –¥–µ–π—Å—Ç–≤–∏–π –¥–ª—è —É–¥–∞–ª–µ–Ω–∏—è
        if simple_actions:
            print(f"\nüóëÔ∏è –ü–†–û–°–¢–´–ï –î–ï–ô–°–¢–í–ò–Ø (—Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è —É–¥–∞–ª–∏—Ç—å):")
            for i, tech in enumerate(simple_actions[:20], 1):
                print(f"  {i}. {tech.english_term} ‚Üí {tech.russian_term}")
        
        # –ü—Ä–∏–º–µ—Ä—ã –æ–±—â–∏—Ö —Ç–µ—Ö–Ω–∏–∫
        if generic_techniques:
            print(f"\n‚ö†Ô∏è –û–ë–©–ò–ï –¢–ï–•–ù–ò–ö–ò (–≤–æ–∑–º–æ–∂–Ω–æ —É–¥–∞–ª–∏—Ç—å):")
            for i, tech in enumerate(generic_techniques[:20], 1):
                print(f"  {i}. {tech.english_term} ‚Üí {tech.russian_term}")
        
        # –ü—Ä–∏–º–µ—Ä—ã –¥—É–±–ª–∏—Ä—É—é—â–∏—Ö—Å—è
        print(f"\nüîÑ –ü–û–•–û–ñ–ò–ï/–î–£–ë–õ–ò–†–£–Æ–©–ò–ï–°–Ø –¢–ï–•–ù–ò–ö–ò:")
        count = 0
        for base, techs in duplicate_similar.items():
            if len(techs) > 1 and count < 10:
                print(f"\n  –ì—Ä—É–ø–ø–∞ '{base}':")
                for tech in techs[:5]:
                    print(f"    - {tech.english_term} ‚Üí {tech.russian_term}")
                count += 1
        
        # –ü—Ä–∏–º–µ—Ä—ã –≤–∞–∂–Ω—ã—Ö —Ç–µ—Ö–Ω–∏–∫ (—Å–æ—Ö—Ä–∞–Ω–∏—Ç—å)
        if specific_techniques:
            print(f"\n‚úÖ –í–ê–ñ–ù–´–ï –¢–ï–•–ù–ò–ö–ò (–æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å):")
            for i, tech in enumerate(specific_techniques[:10], 1):
                print(f"  {i}. {tech.english_term} ‚Üí {tech.russian_term}")
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –¥–ª–∏–Ω–µ –Ω–∞–∑–≤–∞–Ω–∏–π
        print(f"\nüìè –°–¢–ê–¢–ò–°–¢–ò–ö–ê –ü–û –î–õ–ò–ù–ï:")
        length_stats = Counter(len(t.english_term) for t in techniques)
        for length in sorted(length_stats.keys())[:10]:
            print(f"  {length} —Å–∏–º–≤–æ–ª–æ–≤: {length_stats[length]} —Ç–µ—Ö–Ω–∏–∫")
        
        # –ü–æ–¥—Å—á–µ—Ç –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω–æ–≥–æ —É–¥–∞–ª–µ–Ω–∏—è
        to_remove_count = len(simple_actions) + len(generic_techniques)
        print(f"\nüéØ –ò–¢–û–ì–û:")
        print(f"  –†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è —É–¥–∞–ª–∏—Ç—å: {to_remove_count} —Ç–µ—Ö–Ω–∏–∫")
        print(f"  –ü—Ä–æ—Ü–µ–Ω—Ç –æ—Ç –æ–±—â–µ–≥–æ: {to_remove_count/len(techniques)*100:.1f}%")
        print(f"  –û—Å—Ç–∞–Ω–µ—Ç—Å—è: {len(techniques) - to_remove_count} —Ç–µ—Ö–Ω–∏–∫")
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Å–ø–∏—Å–∫–∞ –¥–ª—è —É–¥–∞–ª–µ–Ω–∏—è
        with open('/home/user/novelbins-epub/techniques_to_remove.txt', 'w', encoding='utf-8') as f:
            f.write("–¢–ï–•–ù–ò–ö–ò –î–õ–Ø –£–î–ê–õ–ï–ù–ò–Ø\n")
            f.write("=" * 50 + "\n\n")
            
            f.write("–ü–†–û–°–¢–´–ï –î–ï–ô–°–¢–í–ò–Ø:\n")
            for tech in simple_actions:
                f.write(f"{tech.id}\t{tech.english_term}\t{tech.russian_term}\n")
            
            f.write("\n–û–ë–©–ò–ï –¢–ï–•–ù–ò–ö–ò:\n")
            for tech in generic_techniques:
                f.write(f"{tech.id}\t{tech.english_term}\t{tech.russian_term}\n")
        
        print(f"\nüíæ –°–ø–∏—Å–æ–∫ —Ç–µ—Ö–Ω–∏–∫ –¥–ª—è —É–¥–∞–ª–µ–Ω–∏—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ techniques_to_remove.txt")
        
        return {
            'simple_actions': [t.id for t in simple_actions],
            'generic_techniques': [t.id for t in generic_techniques]
        }

if __name__ == '__main__':
    analyze_techniques()