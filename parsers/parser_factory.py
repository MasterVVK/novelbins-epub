#!/usr/bin/env python3
"""
–§–∞–±—Ä–∏–∫–∞ –ø–∞—Ä—Å–µ—Ä–æ–≤ –¥–ª—è —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –∫–Ω–∏–≥
–û–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç –µ–¥–∏–Ω—ã–π –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –ø–∞—Ä—Å–µ—Ä–æ–≤ —Ä–∞–∑–Ω—ã—Ö —Ç–∏–ø–æ–≤
"""
from typing import Dict, Optional, Type
from urllib.parse import urlparse
import re

from .base.base_parser import BaseParser
from .sources.qidian_parser import QidianParser
from .sources.epub_parser import EPUBParser


class ParserFactory:
    """
    –§–∞–±—Ä–∏–∫–∞ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –ø–∞—Ä—Å–µ—Ä–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ URL –∏–ª–∏ —Ç–∏–ø–∞ –∏—Å—Ç–æ—á–Ω–∏–∫–∞
    """
    
    # –†–µ–µ—Å—Ç—Ä –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –ø–∞—Ä—Å–µ—Ä–æ–≤
    _parsers: Dict[str, Type[BaseParser]] = {
        'qidian': QidianParser,
        'epub': EPUBParser,
    }
    
    # –ü–∞—Ç—Ç–µ—Ä–Ω—ã URL –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –ø–∞—Ä—Å–µ—Ä–∞
    _url_patterns: Dict[str, str] = {
        r'qidian\.com': 'qidian',
        r'm\.qidian\.com': 'qidian',
        r'book\.qidian\.com': 'qidian',
        r'\.epub$': 'epub',  # –§–∞–π–ª—ã EPUB
    }
    
    @classmethod
    def create_parser(cls, source: str, auth_cookies: str = None, socks_proxy: str = None, epub_path: str = None) -> BaseParser:
        """
        –°–æ–∑–¥–∞—Ç—å –ø–∞—Ä—Å–µ—Ä –ø–æ –Ω–∞–∑–≤–∞–Ω–∏—é –∏—Å—Ç–æ—á–Ω–∏–∫–∞
        
        Args:
            source: –ù–∞–∑–≤–∞–Ω–∏–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∞ ('qidian', 'webnovel', 'epub', etc.)
            auth_cookies: Cookies –¥–ª—è –∞–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
            socks_proxy: SOCKS –ø—Ä–æ–∫—Å–∏ –¥–ª—è –æ–±—Ö–æ–¥–∞ –±–ª–æ–∫–∏—Ä–æ–≤–æ–∫ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
            epub_path: –ü—É—Ç—å –∫ EPUB —Ñ–∞–π–ª—É (—Ç–æ–ª—å–∫–æ –¥–ª—è source='epub')
            
        Returns:
            –≠–∫–∑–µ–º–ø–ª—è—Ä –ø–∞—Ä—Å–µ—Ä–∞ –¥–ª—è —É–∫–∞–∑–∞–Ω–Ω–æ–≥–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞
            
        Raises:
            ValueError: –ï—Å–ª–∏ –ø–∞—Ä—Å–µ—Ä –¥–ª—è –∏—Å—Ç–æ—á–Ω–∏–∫–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω
        """
        source = source.lower()
        
        if source not in cls._parsers:
            available = ", ".join(cls._parsers.keys())
            raise ValueError(f"–ü–∞—Ä—Å–µ—Ä –¥–ª—è '{source}' –Ω–µ –Ω–∞–π–¥–µ–Ω. –î–æ—Å—Ç—É–ø–Ω—ã–µ: {available}")
        
        parser_class = cls._parsers[source]
        
        # –°–ø–µ—Ü–∏–∞–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–ª—è EPUB –ø–∞—Ä—Å–µ—Ä–∞
        if source == 'epub':
            if not epub_path:
                raise ValueError("–î–ª—è EPUB –ø–∞—Ä—Å–µ—Ä–∞ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ —É–∫–∞–∑–∞—Ç—å –ø—É—Ç—å –∫ —Ñ–∞–π–ª—É (epub_path)")
            return parser_class(epub_path=epub_path)
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –ª–∏ –ø–∞—Ä—Å–µ—Ä SOCKS –ø—Ä–æ–∫—Å–∏
        try:
            return parser_class(auth_cookies=auth_cookies, socks_proxy=socks_proxy)
        except TypeError:
            # Fallback –¥–ª—è –ø–∞—Ä—Å–µ—Ä–æ–≤ –±–µ–∑ –ø–æ–¥–¥–µ—Ä–∂–∫–∏ –ø—Ä–æ–∫—Å–∏
            return parser_class(auth_cookies=auth_cookies)
    
    @classmethod
    def create_parser_from_url(cls, url: str, auth_cookies: str = None, socks_proxy: str = None) -> BaseParser:
        """
        –°–æ–∑–¥–∞—Ç—å –ø–∞—Ä—Å–µ—Ä –Ω–∞ –æ—Å–Ω–æ–≤–µ URL
        
        Args:
            url: URL –∫–Ω–∏–≥–∏ –∏–ª–∏ —Å–∞–π—Ç–∞
            auth_cookies: Cookies –¥–ª—è –∞–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
            socks_proxy: SOCKS –ø—Ä–æ–∫—Å–∏ –¥–ª—è –æ–±—Ö–æ–¥–∞ –±–ª–æ–∫–∏—Ä–æ–≤–æ–∫ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
            
        Returns:
            –≠–∫–∑–µ–º–ø–ª—è—Ä –ø–æ–¥—Ö–æ–¥—è—â–µ–≥–æ –ø–∞—Ä—Å–µ—Ä–∞
            
        Raises:
            ValueError: –ï—Å–ª–∏ –Ω–µ —É–¥–∞–µ—Ç—Å—è –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å —Ç–∏–ø –ø–∞—Ä—Å–µ—Ä–∞ –ø–æ URL
        """
        source = cls.detect_source_from_url(url)
        
        if not source:
            raise ValueError(f"–ù–µ —É–¥–∞–µ—Ç—Å—è –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å –∏—Å—Ç–æ—á–Ω–∏–∫ –ø–æ URL: {url}")
        
        return cls.create_parser(source, auth_cookies=auth_cookies, socks_proxy=socks_proxy)
    
    @classmethod
    def detect_source_from_url(cls, url: str) -> Optional[str]:
        """
        –û–ø—Ä–µ–¥–µ–ª–∏—Ç—å —Ç–∏–ø –∏—Å—Ç–æ—á–Ω–∏–∫–∞ –ø–æ URL
        
        Args:
            url: URL –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
            
        Returns:
            –ù–∞–∑–≤–∞–Ω–∏–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∞ –∏–ª–∏ None, –µ—Å–ª–∏ –Ω–µ —É–¥–∞–µ—Ç—Å—è –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å
        """
        for pattern, source in cls._url_patterns.items():
            if re.search(pattern, url, re.IGNORECASE):
                return source
        
        return None
    
    @classmethod
    def get_available_sources(cls) -> list:
        """
        –ü–æ–ª—É—á–∏—Ç—å —Å–ø–∏—Å–æ–∫ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
        
        Returns:
            –°–ø–∏—Å–æ–∫ –Ω–∞–∑–≤–∞–Ω–∏–π –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
        """
        return list(cls._parsers.keys())
    
    @classmethod
    def register_parser(cls, source: str, parser_class: Type[BaseParser], url_patterns: Optional[list] = None):
        """
        –ó–∞—Ä–µ–≥–∏—Å—Ç—Ä–∏—Ä–æ–≤–∞—Ç—å –Ω–æ–≤—ã–π –ø–∞—Ä—Å–µ—Ä
        
        Args:
            source: –ù–∞–∑–≤–∞–Ω–∏–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∞
            parser_class: –ö–ª–∞—Å—Å –ø–∞—Ä—Å–µ—Ä–∞ (–¥–æ–ª–∂–µ–Ω –Ω–∞—Å–ª–µ–¥–æ–≤–∞—Ç—å—Å—è –æ—Ç BaseParser)
            url_patterns: –°–ø–∏—Å–æ–∫ —Ä–µ–≥—É–ª—è—Ä–Ω—ã—Ö –≤—ã—Ä–∞–∂–µ–Ω–∏–π –¥–ª—è –∞–≤—Ç–æ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –ø–æ URL
        """
        if not issubclass(parser_class, BaseParser):
            raise ValueError("–ö–ª–∞—Å—Å –ø–∞—Ä—Å–µ—Ä–∞ –¥–æ–ª–∂–µ–Ω –Ω–∞—Å–ª–µ–¥–æ–≤–∞—Ç—å—Å—è –æ—Ç BaseParser")
        
        cls._parsers[source.lower()] = parser_class
        
        # –î–æ–±–∞–≤–ª—è–µ–º –ø–∞—Ç—Ç–µ—Ä–Ω—ã URL –µ—Å–ª–∏ –æ–Ω–∏ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω—ã
        if url_patterns:
            for pattern in url_patterns:
                cls._url_patterns[pattern] = source.lower()
    
    @classmethod
    def get_parser_info(cls, source: str) -> Dict:
        """
        –ü–æ–ª—É—á–∏—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –ø–∞—Ä—Å–µ—Ä–µ
        
        Args:
            source: –ù–∞–∑–≤–∞–Ω–∏–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∞
            
        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –æ –ø–∞—Ä—Å–µ—Ä–µ
        """
        source = source.lower()
        
        if source not in cls._parsers:
            raise ValueError(f"–ü–∞—Ä—Å–µ—Ä –¥–ª—è '{source}' –Ω–µ –Ω–∞–π–¥–µ–Ω")
        
        parser_class = cls._parsers[source]
        
        # –°–æ–±–∏—Ä–∞–µ–º –ø–∞—Ç—Ç–µ—Ä–Ω—ã URL –¥–ª—è —ç—Ç–æ–≥–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞
        url_patterns = [pattern for pattern, src in cls._url_patterns.items() if src == source]
        
        return {
            'source': source,
            'class': parser_class.__name__,
            'module': parser_class.__module__,
            'url_patterns': url_patterns,
            'description': parser_class.__doc__ or "–ù–µ—Ç –æ–ø–∏—Å–∞–Ω–∏—è"
        }
    
    @classmethod
    def list_all_parsers(cls) -> Dict:
        """
        –ü–æ–ª—É—á–∏—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ–±–æ –≤—Å–µ—Ö –∑–∞—Ä–µ–≥–∏—Å—Ç—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –ø–∞—Ä—Å–µ—Ä–∞—Ö
        
        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –æ –≤—Å–µ—Ö –ø–∞—Ä—Å–µ—Ä–∞—Ö
        """
        return {source: cls.get_parser_info(source) for source in cls._parsers.keys()}


# –£–¥–æ–±–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
def create_parser(source: str, auth_cookies: str = None, socks_proxy: str = None, epub_path: str = None) -> BaseParser:
    """–°–æ–∑–¥–∞—Ç—å –ø–∞—Ä—Å–µ—Ä –ø–æ –Ω–∞–∑–≤–∞–Ω–∏—é –∏—Å—Ç–æ—á–Ω–∏–∫–∞"""
    return ParserFactory.create_parser(source, auth_cookies=auth_cookies, socks_proxy=socks_proxy, epub_path=epub_path)


def create_parser_from_url(url: str, auth_cookies: str = None, socks_proxy: str = None) -> BaseParser:
    """–°–æ–∑–¥–∞—Ç—å –ø–∞—Ä—Å–µ—Ä –Ω–∞ –æ—Å–Ω–æ–≤–µ URL"""
    return ParserFactory.create_parser_from_url(url, auth_cookies=auth_cookies, socks_proxy=socks_proxy)


def detect_source(url: str) -> Optional[str]:
    """
    –û–ø—Ä–µ–¥–µ–ª–∏—Ç—å —Ç–∏–ø –∏—Å—Ç–æ—á–Ω–∏–∫–∞ –ø–æ URL
    """
    return ParserFactory.detect_source_from_url(url)


def get_available_sources() -> list:
    """
    –ü–æ–ª—É—á–∏—Ç—å —Å–ø–∏—Å–æ–∫ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
    """
    return ParserFactory.get_available_sources()


def main():
    """
    –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è —Ä–∞–±–æ—Ç—ã —Ñ–∞–±—Ä–∏–∫–∏ –ø–∞—Ä—Å–µ—Ä–æ–≤
    """
    print("üè≠ PARSER FACTORY - –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è")
    print("=" * 50)
    
    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –¥–æ—Å—Ç—É–ø–Ω—ã–µ –ø–∞—Ä—Å–µ—Ä—ã
    print("üìã –î–æ—Å—Ç—É–ø–Ω—ã–µ –ø–∞—Ä—Å–µ—Ä—ã:")
    parsers_info = ParserFactory.list_all_parsers()
    for source, info in parsers_info.items():
        print(f"  üìö {source.upper()}")
        print(f"     –ö–ª–∞—Å—Å: {info['class']}")
        print(f"     URL –ø–∞—Ç—Ç–µ—Ä–Ω—ã: {info['url_patterns']}")
        print()
    
    # –¢–µ—Å—Ç–∏—Ä—É–µ–º –∞–≤—Ç–æ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∞
    test_urls = [
        "https://www.qidian.com/book/1209977/",
        "https://m.qidian.com/book/1209977/",
        "https://example.com/book/123"
    ]
    
    print("üîç –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∞–≤—Ç–æ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –∏—Å—Ç–æ—á–Ω–∏–∫–∞:")
    for url in test_urls:
        source = detect_source(url)
        if source:
            print(f"  ‚úÖ {url} -> {source}")
        else:
            print(f"  ‚ùå {url} -> –Ω–µ —Ä–∞—Å–ø–æ–∑–Ω–∞–Ω")
    print()
    
    # –°–æ–∑–¥–∞–µ–º –ø–∞—Ä—Å–µ—Ä –∏ —Ç–µ—Å—Ç–∏—Ä—É–µ–º
    try:
        print("üöÄ –°–æ–∑–¥–∞–Ω–∏–µ –ø–∞—Ä—Å–µ—Ä–∞ Qidian...")
        parser = create_parser('qidian')
        stats = parser.get_stats()
        print(f"  ‚úÖ –ü–∞—Ä—Å–µ—Ä —Å–æ–∑–¥–∞–Ω: {parser.source_name}")
        print(f"  üìä –ù–∞—á–∞–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞: {stats}")
        parser.close()
        
    except Exception as e:
        print(f"  ‚ùå –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è –ø–∞—Ä—Å–µ—Ä–∞: {e}")


if __name__ == "__main__":
    main()