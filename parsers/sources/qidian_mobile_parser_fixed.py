#!/usr/bin/env python3
"""
–ò—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã–π –ø–∞—Ä—Å–µ—Ä Qidian –Ω–∞ –æ—Å–Ω–æ–≤–µ –º–æ–±–∏–ª—å–Ω–æ–π –≤–µ—Ä—Å–∏–∏
–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç —Å–∂–∞—Ç—ã–µ HTTP –æ—Ç–≤–µ—Ç—ã –ø—Ä–∞–≤–∏–ª—å–Ω–æ
"""
import requests
import json
import time
import re
from pathlib import Path
from bs4 import BeautifulSoup
from typing import List, Dict, Optional
from dataclasses import dataclass


@dataclass
class QidianChapter:
    """–ú–æ–¥–µ–ª—å –≥–ª–∞–≤—ã Qidian"""
    number: int
    title: str
    content: str
    url: str
    chapter_id: str
    word_count: int


@dataclass
class QidianBook:
    """–ú–æ–¥–µ–ª—å –∫–Ω–∏–≥–∏ Qidian"""
    book_id: str
    title: str
    author: str
    status: str
    genre: str
    description: str
    total_chapters: int
    chapters: List[QidianChapter] = None


class QidianMobileParserFixed:
    """–ò—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã–π –ø–∞—Ä—Å–µ—Ä –º–æ–±–∏–ª—å–Ω–æ–π –≤–µ—Ä—Å–∏–∏ Qidian"""
    
    def __init__(self):
        self.session = requests.Session()
        self.base_url = "https://m.qidian.com"
        
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤ –¥–ª—è –º–æ–±–∏–ª—å–Ω–æ–π –≤–µ—Ä—Å–∏–∏
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (iPhone; CPU iPhone OS 14_0 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.0 Mobile/15E148 Safari/604.1',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
            'Accept-Encoding': 'identity',  # –û—Ç–∫–ª—é—á–∞–µ–º —Å–∂–∞—Ç–∏–µ –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏
            'Connection': 'keep-alive',
            'Cache-Control': 'no-cache',
            'Pragma': 'no-cache'
        })
    
    def _get_page_content(self, url: str, description: str = "") -> Optional[str]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –∏ –ø—Ä–æ–≤–µ—Ä–∫–∞ HTML –∫–æ–Ω—Ç–µ–Ω—Ç–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—ã"""
        try:
            print(f"üåê –ó–∞–ø—Ä–æ—Å {description}: {url}")
            response = self.session.get(url, timeout=15)
            print(f"   –°—Ç–∞—Ç—É—Å: {response.status_code}")
            print(f"   –†–∞–∑–º–µ—Ä: {len(response.content)} –±–∞–π—Ç")
            print(f"   Content-Type: {response.headers.get('Content-Type', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ')}")
            print(f"   Content-Encoding: {response.headers.get('Content-Encoding', '–ù–µ—Ç')}")
            
            if response.status_code != 200:
                print(f"‚ùå HTTP –æ—à–∏–±–∫–∞: {response.status_code}")
                return None
            
            # –ü–æ–ª—É—á–∞–µ–º —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ
            html_content = response.text
            print(f"   –î–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ä–∞–∑–º–µ—Ä: {len(html_content)} —Å–∏–º–≤–æ–ª–æ–≤")
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —ç—Ç–æ –≤–∞–ª–∏–¥–Ω—ã–π HTML
            if not html_content or len(html_content) < 500:
                print(f"‚ùå –°–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–π –æ—Ç–≤–µ—Ç: {len(html_content)} —Å–∏–º–≤–æ–ª–æ–≤")
                return None
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –Ω–∞—á–∏–Ω–∞–µ—Ç—Å—è –∫–∞–∫ HTML
            content_start = html_content.strip()[:100]
            if not (content_start.startswith('<!DOCTYPE') or content_start.startswith('<html')):
                print(f"‚ùå –û—Ç–≤–µ—Ç –Ω–µ —è–≤–ª—è–µ—Ç—Å—è HTML. –ù–∞—á–∞–ª–æ: {content_start}")
                return None
            
            print(f"‚úÖ HTML –∫–æ–Ω—Ç–µ–Ω—Ç –ø–æ–ª—É—á–µ–Ω —É—Å–ø–µ—à–Ω–æ")
            return html_content
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ —Å—Ç—Ä–∞–Ω–∏—Ü—ã: {e}")
            return None
    
    def get_book_info(self, book_id: str) -> Optional[QidianBook]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –∫–Ω–∏–≥–µ"""
        print(f"üìñ –ü–æ–ª—É—á–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –∫–Ω–∏–≥–µ {book_id}...")
        
        url = f"{self.base_url}/book/{book_id}"
        html_content = self._get_page_content(url, "–∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –∫–Ω–∏–≥–µ")
        
        if not html_content:
            return None
        
        try:
            soup = BeautifulSoup(html_content, 'html.parser')
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º HTML –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏
            debug_file = f"debug_book_{book_id}_fixed.html"
            with open(debug_file, 'w', encoding='utf-8') as f:
                f.write(html_content)
            print(f"üîç HTML —Å–æ—Ö—Ä–∞–Ω–µ–Ω –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏: {debug_file}")
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –∫–Ω–∏–≥–µ —Å —Ä–∞–∑–ª–∏—á–Ω—ã–º–∏ —Å—Ç—Ä–∞—Ç–µ–≥–∏—è–º–∏
            title = self._extract_title(soup)
            author = self._extract_author(soup)
            genre = self._extract_genre(soup)
            status = self._extract_status(soup, html_content)
            description = self._extract_description(soup)
            
            print(f"‚úÖ –ù–∞–π–¥–µ–Ω–∞ –∫–Ω–∏–≥–∞: {title}")
            print(f"   –ê–≤—Ç–æ—Ä: {author}")
            print(f"   –ñ–∞–Ω—Ä: {genre}")
            print(f"   –°—Ç–∞—Ç—É—Å: {status}")
            
            return QidianBook(
                book_id=book_id,
                title=title,
                author=author,
                status=status,
                genre=genre,
                description=description,
                total_chapters=0  # –ë—É–¥–µ—Ç –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–æ –ø–æ–∑–∂–µ
            )
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –∫–Ω–∏–≥–µ: {e}")
            return None
    
    def _extract_title(self, soup: BeautifulSoup) -> str:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –Ω–∞–∑–≤–∞–Ω–∏—è –∫–Ω–∏–≥–∏ —Å –Ω–µ—Å–∫–æ–ª—å–∫–∏–º–∏ —Å—Ç—Ä–∞—Ç–µ–≥–∏—è–º–∏"""
        selectors = [
            'h1.detail__header-detail__title',
            'h1[class*="title"]',
            'h1',
            '.book-title',
            '.title',
            '[class*="book-name"]',
            'title'
        ]
        
        for selector in selectors:
            elements = soup.select(selector)
            for elem in elements:
                text = elem.get_text().strip()
                if text and len(text) > 1 and not text.startswith('„Ää') and 'Â∞èËØ¥' not in text:
                    if selector == 'title':
                        # –ò–∑ title —Ç–µ–≥–∞ –∏–∑–≤–ª–µ–∫–∞–µ–º —Ç–æ–ª—å–∫–æ –Ω–∞–∑–≤–∞–Ω–∏–µ –∫–Ω–∏–≥–∏
                        text = text.split('„Äã')[0].replace('„Ää', '').split('Â∞èËØ¥')[0].strip()
                    print(f"   –ù–∞–π–¥–µ–Ω–æ –Ω–∞–∑–≤–∞–Ω–∏–µ ({selector}): {text}")
                    return text
        
        return "–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ"
    
    def _extract_author(self, soup: BeautifulSoup) -> str:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∞–≤—Ç–æ—Ä–∞"""
        selectors = [
            'a.detail__header-detail__author-link',
            '.detail__header-detail__author-link',
            'a[href*="author"]',
            '.author',
            '.writer',
            '[class*="author"]'
        ]
        
        for selector in selectors:
            elements = soup.select(selector)
            for elem in elements:
                text = elem.get_text().strip()
                if text and len(text) > 1:
                    # –£–±–∏—Ä–∞–µ–º —Ç–µ–≥–∏ –∏ —Å–∏–º–≤–æ–ª—ã
                    text = text.split()[0] if text else text
                    print(f"   –ù–∞–π–¥–µ–Ω –∞–≤—Ç–æ—Ä ({selector}): {text}")
                    return text
        
        return "–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ"
    
    def _extract_genre(self, soup: BeautifulSoup) -> str:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∂–∞–Ω—Ä–∞"""
        selectors = [
            'a.detail__header-detail__category',
            '.detail__header-detail__category',
            '.category',
            '.genre',
            '[class*="category"]'
        ]
        
        for selector in selectors:
            elements = soup.select(selector)
            for elem in elements:
                text = elem.get_text().strip()
                if text and len(text) > 1:
                    print(f"   –ù–∞–π–¥–µ–Ω –∂–∞–Ω—Ä ({selector}): {text}")
                    return text
        
        return "–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ"
    
    def _extract_status(self, soup: BeautifulSoup, html_content: str) -> str:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å—Ç–∞—Ç—É—Å–∞ –∫–Ω–∏–≥–∏"""
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –≤ HTML –Ω–∞–ø—Ä—è–º—É—é
        if "ÂÆåÊú¨" in html_content:
            return "ÂÆåÊú¨"
        elif "ËøûËΩΩ‰∏≠" in html_content:
            return "ËøûËΩΩ‰∏≠"
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á–µ—Ä–µ–∑ —Å–µ–ª–µ–∫—Ç–æ—Ä—ã
        status_selectors = [
            '.detail__header-detail__line',
            '.status',
            '[class*="status"]'
        ]
        
        for selector in status_selectors:
            elements = soup.select(selector)
            for elem in elements:
                text = elem.get_text()
                if "ÂÆåÊú¨" in text:
                    return "ÂÆåÊú¨"
                elif "ËøûËΩΩ" in text:
                    return "ËøûËΩΩ‰∏≠"
        
        return "ËøûËΩΩ‰∏≠"  # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é
    
    def _extract_description(self, soup: BeautifulSoup) -> str:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –æ–ø–∏—Å–∞–Ω–∏—è"""
        selectors = [
            'p.detail__summary__content',
            '.detail__summary__content',
            '.summary',
            '.description', 
            '[class*="summary"]',
            '[class*="intro"]'
        ]
        
        for selector in selectors:
            elements = soup.select(selector)
            for elem in elements:
                text = elem.get_text().strip()
                if text and len(text) > 10:
                    print(f"   –ù–∞–π–¥–µ–Ω–æ –æ–ø–∏—Å–∞–Ω–∏–µ ({selector}): {text[:50]}...")
                    return text
        
        return ""
    
    def get_chapter_list(self, book_id: str) -> List[Dict]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å–ø–∏—Å–∫–∞ –≥–ª–∞–≤"""
        print(f"üìë –ü–æ–ª—É—á–µ–Ω–∏–µ —Å–ø–∏—Å–∫–∞ –≥–ª–∞–≤ –¥–ª—è –∫–Ω–∏–≥–∏ {book_id}...")
        
        catalog_url = f"{self.base_url}/book/{book_id}/catalog/"
        html_content = self._get_page_content(catalog_url, "–∫–∞—Ç–∞–ª–æ–≥–∞ –≥–ª–∞–≤")
        
        if not html_content:
            return []
        
        try:
            soup = BeautifulSoup(html_content, 'html.parser')
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏
            debug_file = f"debug_catalog_{book_id}_fixed.html"
            with open(debug_file, 'w', encoding='utf-8') as f:
                f.write(html_content)
            print(f"üîç HTML –∫–∞—Ç–∞–ª–æ–≥–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {debug_file}")
            
            # –ò—â–µ–º —Å—Å—ã–ª–∫–∏ –Ω–∞ –≥–ª–∞–≤—ã —Ä–∞–∑–Ω—ã–º–∏ —Å–ø–æ—Å–æ–±–∞–º–∏
            chapters = []
            
            # –°–ø–æ—Å–æ–± 1: –ø–æ –∫–ª–∞—Å—Å—É chapter-link
            chapter_links = soup.find_all('a', class_='chapter-link')
            print(f"   –ù–∞–π–¥–µ–Ω–æ –ø–æ –∫–ª–∞—Å—Å—É 'chapter-link': {len(chapter_links)}")
            
            # –°–ø–æ—Å–æ–± 2: –ø–æ href –ø–∞—Ç—Ç–µ—Ä–Ω—É
            if not chapter_links:
                chapter_links = soup.find_all('a', href=re.compile(r'/chapter/\d+/\d+/'))
                print(f"   –ù–∞–π–¥–µ–Ω–æ –ø–æ href –ø–∞—Ç—Ç–µ—Ä–Ω—É: {len(chapter_links)}")
            
            # –°–ø–æ—Å–æ–± 3: –≤—Å–µ —Å—Å—ã–ª–∫–∏ —Å–æ —Å–ª–æ–≤–æ–º "chapter"
            if not chapter_links:
                chapter_links = soup.find_all('a', href=re.compile(r'chapter'))
                print(f"   –ù–∞–π–¥–µ–Ω–æ —Å—Å—ã–ª–æ–∫ —Å 'chapter': {len(chapter_links)}")
            
            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –Ω–∞–π–¥–µ–Ω–Ω—ã–µ —Å—Å—ã–ª–∫–∏
            for i, link in enumerate(chapter_links):
                href = link.get('href', '')
                title = link.get_text().strip()
                
                if not href or not title:
                    continue
                
                # –ò–∑–≤–ª–µ–∫–∞–µ–º ID –≥–ª–∞–≤—ã –∏–∑ URL
                chapter_id = ""
                match = re.search(r'/chapter/(\d+)/(\d+)/', href)
                if match:
                    chapter_id = match.group(2)
                
                # –§–æ—Ä–º–∏—Ä—É–µ–º –ø–æ–ª–Ω—ã–π URL
                if href.startswith('//'):
                    full_url = f"https:{href}"
                elif href.startswith('/'):
                    full_url = f"{self.base_url}{href}"
                else:
                    full_url = href
                
                # –ò—Å–ø—Ä–∞–≤–ª—è–µ–º –¥—É–±–ª–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–æ–º–µ–Ω–∞
                full_url = full_url.replace('//m.qidian.com//m.qidian.com/', '//m.qidian.com/')
                full_url = full_url.replace('https://m.qidian.com//m.qidian.com/', 'https://m.qidian.com/')
                
                chapters.append({
                    'number': i + 1,
                    'title': title,
                    'url': full_url,
                    'chapter_id': chapter_id or str(i + 1)
                })
            
            print(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ –≥–ª–∞–≤: {len(chapters)}")
            if chapters:
                print("üìñ –ü–µ—Ä–≤—ã–µ –Ω–∞–π–¥–µ–Ω–Ω—ã–µ –≥–ª–∞–≤—ã:")
                for ch in chapters[:3]:
                    print(f"   {ch['number']}. {ch['title'][:40]} ‚Üí {ch['url']}")
            
            return chapters
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ —Å–ø–∏—Å–∫–∞ –≥–ª–∞–≤: {e}")
            return []
    
    def get_chapter_content(self, chapter_url: str, chapter_number: int, title: str) -> Optional[QidianChapter]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ –≥–ª–∞–≤—ã"""
        print(f"üìÑ –ó–∞–≥—Ä—É–∑–∫–∞ –≥–ª–∞–≤—ã {chapter_number}: {title[:50]}...")
        
        html_content = self._get_page_content(chapter_url, f"–≥–ª–∞–≤—ã {chapter_number}")
        
        if not html_content:
            return None
        
        try:
            soup = BeautifulSoup(html_content, 'html.parser')
            
            # –ò—â–µ–º –∫–æ–Ω—Ç–µ–Ω—Ç –≥–ª–∞–≤—ã —Å —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–º–∏ —Å–µ–ª–µ–∫—Ç–æ—Ä–∞–º–∏
            content_selectors = [
                '#reader-content .content',    # –û—Å–Ω–æ–≤–Ω–æ–π –∫–æ–Ω—Ç–µ–Ω—Ç
                '#reader-content',             # –ö–æ–Ω—Ç–µ–π–Ω–µ—Ä —á–∏—Ç–∞–ª–∫–∏
                '.chapter-content',            # –ö–æ–Ω—Ç–µ–Ω—Ç –≥–ª–∞–≤—ã
                '.content main',               # –û—Å–Ω–æ–≤–Ω–∞—è –æ–±–ª–∞—Å—Ç—å
                'main.content',                # –ì–ª–∞–≤–Ω–∞—è –æ–±–ª–∞—Å—Ç—å –∫–æ–Ω—Ç–µ–Ω—Ç–∞
                '.print .content',             # –û–±–ª–∞—Å—Ç—å –ø–µ—á–∞—Ç–∏
                '.reader-content',             # –ß–∏—Ç–∞–ª–∫–∞
                '[class*="content"]'           # –õ—é–±–æ–π –∫–ª–∞—Å—Å —Å content
            ]
            
            content_elem = None
            for selector in content_selectors:
                content_elem = soup.select_one(selector)
                if content_elem:
                    print(f"   –ö–æ–Ω—Ç–µ–Ω—Ç –Ω–∞–π–¥–µ–Ω —á–µ—Ä–µ–∑ —Å–µ–ª–µ–∫—Ç–æ—Ä: {selector}")
                    break
            
            if not content_elem:
                print("‚ùå –ö–æ–Ω—Ç–µ–Ω—Ç –≥–ª–∞–≤—ã –Ω–µ –Ω–∞–π–¥–µ–Ω")
                return None
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –ø–∞—Ä–∞–≥—Ä–∞—Ñ—ã
            paragraphs = []
            p_elements = content_elem.find_all('p')
            
            for p in p_elements:
                text = p.get_text().strip()
                if text and len(text) > 10:  # –§–∏–ª—å—Ç—Ä—É–µ–º –∫–æ—Ä–æ—Ç–∫–∏–µ —Å—Ç—Ä–æ–∫–∏
                    paragraphs.append(text)
            
            if not paragraphs:
                # –ï—Å–ª–∏ –ø–∞—Ä–∞–≥—Ä–∞—Ñ—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã, –±–µ—Ä–µ–º –≤–µ—Å—å —Ç–µ–∫—Å—Ç
                full_text = content_elem.get_text()
                lines = [line.strip() for line in full_text.split('\n') if line.strip()]
                paragraphs = [line for line in lines if len(line) > 20]
            
            if not paragraphs:
                print("‚ùå –¢–µ–∫—Å—Ç –≥–ª–∞–≤—ã –Ω–µ –Ω–∞–π–¥–µ–Ω")
                return None
            
            content = '\n\n'.join(paragraphs)
            word_count = len(content)
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º chapter_id –∏–∑ URL
            chapter_id_match = re.search(r'/chapter/\d+/(\d+)/', chapter_url)
            chapter_id = chapter_id_match.group(1) if chapter_id_match else str(chapter_number)
            
            print(f"‚úÖ –ü–æ–ª—É—á–µ–Ω–æ {len(paragraphs)} –ø–∞—Ä–∞–≥—Ä–∞—Ñ–æ–≤, {word_count} —Å–∏–º–≤–æ–ª–æ–≤")
            
            return QidianChapter(
                number=chapter_number,
                title=title,
                content=content,
                url=chapter_url,
                chapter_id=chapter_id,
                word_count=word_count
            )
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –≥–ª–∞–≤—ã: {e}")
            return None
    
    def parse_book(self, book_id: str, max_chapters: int = 10) -> Optional[QidianBook]:
        """–ü–æ–ª–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥ –∫–Ω–∏–≥–∏"""
        print(f"üöÄ –ù–∞—á–∏–Ω–∞–µ–º –ø–∞—Ä—Å–∏–Ω–≥ –∫–Ω–∏–≥–∏ {book_id}")
        print("=" * 60)
        
        # –ü–æ–ª—É—á–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –∫–Ω–∏–≥–µ
        book = self.get_book_info(book_id)
        if not book:
            return None
        
        # –ü–æ–ª—É—á–∞–µ–º —Å–ø–∏—Å–æ–∫ –≥–ª–∞–≤
        chapter_list = self.get_chapter_list(book_id)
        if not chapter_list:
            print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å —Å–ø–∏—Å–æ–∫ –≥–ª–∞–≤")
            return book
        
        book.total_chapters = len(chapter_list)
        
        # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –≥–ª–∞–≤
        chapters_to_parse = chapter_list[:max_chapters]
        print(f"\nüìö –ë—É–¥–µ—Ç –∑–∞–≥—Ä—É–∂–µ–Ω–æ {len(chapters_to_parse)} –∏–∑ {len(chapter_list)} –≥–ª–∞–≤")
        
        # –ü–∞—Ä—Å–∏–º –≥–ª–∞–≤—ã
        parsed_chapters = []
        for i, chapter_info in enumerate(chapters_to_parse):
            print(f"\n[{i+1}/{len(chapters_to_parse)}]")
            
            chapter = self.get_chapter_content(
                chapter_info['url'],
                chapter_info['number'],
                chapter_info['title']
            )
            
            if chapter:
                parsed_chapters.append(chapter)
                
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≥–ª–∞–≤—É –≤ —Ñ–∞–π–ª
                self.save_chapter_to_file(chapter, book.title)
            
            # –ü–∞—É–∑–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏
            if i < len(chapters_to_parse) - 1:
                time.sleep(2)  # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º –ø–∞—É–∑—É –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
        
        book.chapters = parsed_chapters
        
        print(f"\n‚úÖ –ü–∞—Ä—Å–∏–Ω–≥ –∑–∞–≤–µ—Ä—à–µ–Ω!")
        print(f"üìä –£—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–æ: {len(parsed_chapters)}/{len(chapters_to_parse)} –≥–ª–∞–≤")
        
        return book
    
    def save_chapter_to_file(self, chapter: QidianChapter, book_title: str):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≥–ª–∞–≤—ã –≤ —Ñ–∞–π–ª"""
        # –°–æ–∑–¥–∞–µ–º –ø–∞–ø–∫—É –¥–ª—è –∫–Ω–∏–≥–∏
        safe_title = re.sub(r'[^\w\s-]', '', book_title)
        output_dir = Path(f"qidian_parsed/{safe_title}")
        output_dir.mkdir(parents=True, exist_ok=True)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≥–ª–∞–≤—É
        filename = f"chapter_{chapter.number:03d}_{chapter.chapter_id}.txt"
        filepath = output_dir / filename
        
        with open(filepath, 'w', encoding='utf-8') as f:
            f.write(f"{chapter.title}\n")
            f.write("=" * 60 + "\n\n")
            f.write(chapter.content)
        
        print(f"üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ: {filepath}")
    
    def save_book_info(self, book: QidianBook):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –∫–Ω–∏–≥–µ –≤ JSON"""
        safe_title = re.sub(r'[^\w\s-]', '', book.title)
        output_dir = Path(f"qidian_parsed/{safe_title}")
        output_dir.mkdir(parents=True, exist_ok=True)
        
        book_info = {
            'book_id': book.book_id,
            'title': book.title,
            'author': book.author,
            'status': book.status,
            'genre': book.genre,
            'description': book.description,
            'total_chapters': book.total_chapters,
            'parsed_chapters': len(book.chapters) if book.chapters else 0,
            'chapters': []
        }
        
        if book.chapters:
            for chapter in book.chapters:
                book_info['chapters'].append({
                    'number': chapter.number,
                    'title': chapter.title,
                    'chapter_id': chapter.chapter_id,
                    'url': chapter.url,
                    'word_count': chapter.word_count
                })
        
        filepath = output_dir / "book_info.json"
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(book_info, f, indent=2, ensure_ascii=False)
        
        print(f"üíæ –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –∫–Ω–∏–≥–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {filepath}")


def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –ø–∞—Ä—Å–µ—Ä–∞"""
    print("üöÄ Qidian Mobile Parser (Fixed) - –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ")
    print("=" * 70)
    
    parser = QidianMobileParserFixed()
    
    # –ü—Ä–∏–º–µ—Ä—ã –∫–Ω–∏–≥ –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
    test_books = [
        ("1209977", "ÊñóÁ†¥ËãçÁ©π (Battle Through the Heavens)"),
        ("1010868264", "ËØ°Áßò‰πã‰∏ª (Lord of the Mysteries)"),
    ]
    
    print("–î–æ—Å—Ç—É–ø–Ω—ã–µ –∫–Ω–∏–≥–∏ –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è:")
    for i, (book_id, title) in enumerate(test_books, 1):
        print(f"{i}. {title} (ID: {book_id})")
    
    # –î–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø–µ—Ä–≤—É—é –∫–Ω–∏–≥—É
    book_id = test_books[0][0]
    max_chapters = 2  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
    
    print(f"\nüéØ –¢–µ—Å—Ç–∏—Ä—É–µ–º –∫–Ω–∏–≥—É ID: {book_id}")
    print(f"üìä –ú–∞–∫—Å–∏–º—É–º –≥–ª–∞–≤: {max_chapters}")
    
    # –ü–∞—Ä—Å–∏–º –∫–Ω–∏–≥—É
    book = parser.parse_book(book_id, max_chapters)
    
    if book:
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –∫–Ω–∏–≥–µ
        parser.save_book_info(book)
        
        print(f"\n" + "=" * 70)
        print("üìã –ò–¢–û–ì–ò –ü–ê–†–°–ò–ù–ì–ê")
        print("=" * 70)
        print(f"üìñ –ö–Ω–∏–≥–∞: {book.title}")
        print(f"‚úçÔ∏è –ê–≤—Ç–æ—Ä: {book.author}")
        print(f"üìä –°—Ç–∞—Ç—É—Å: {book.status}")
        print(f"üè∑Ô∏è –ñ–∞–Ω—Ä: {book.genre}")
        print(f"üìö –í—Å–µ–≥–æ –≥–ª–∞–≤: {book.total_chapters}")
        print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ –≥–ª–∞–≤: {len(book.chapters) if book.chapters else 0}")
        
        if book.chapters:
            print(f"\nüìù –ó–∞–≥—Ä—É–∂–µ–Ω–Ω—ã–µ –≥–ª–∞–≤—ã:")
            for chapter in book.chapters:
                print(f"   {chapter.number}. {chapter.title} ({chapter.word_count} —Å–∏–º–≤–æ–ª–æ–≤)")
        
        safe_title = re.sub(r'[^\w\s-]', '', book.title)
        print(f"\nüíæ –§–∞–π–ª—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ –ø–∞–ø–∫–µ: qidian_parsed/{safe_title}/")
        
    else:
        print("‚ùå –ü–∞—Ä—Å–∏–Ω–≥ –Ω–µ —É–¥–∞–ª—Å—è")


if __name__ == "__main__":
    main()