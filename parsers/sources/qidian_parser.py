#!/usr/bin/env python3
"""
–ü–∞—Ä—Å–µ—Ä Qidian –Ω–∞ –æ—Å–Ω–æ–≤–µ –±–∞–∑–æ–≤–æ–≥–æ –∫–ª–∞—Å—Å–∞
–û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –º–æ–±–∏–ª—å–Ω–æ–π –≤–µ—Ä—Å–∏–µ–π —Å–∞–π—Ç–∞
"""
import time
import random
from typing import Dict, List, Optional
from bs4 import BeautifulSoup
import re
import sys
import os

# –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Ç—å –∫ –±–∞–∑–æ–≤–æ–º—É –∫–ª–∞—Å—Å—É
sys.path.append(os.path.join(os.path.dirname(__file__), '..', 'base'))
from base_parser import BaseParser


class QidianParser(BaseParser):
    """
    –ü–∞—Ä—Å–µ—Ä –¥–ª—è Qidian.com (–∫–∏—Ç–∞–π—Å–∫–∞—è –ø–ª–∞—Ç—Ñ–æ—Ä–º–∞ –≤–µ–±-–Ω–æ–≤–µ–ª–ª)
    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç –º–æ–±–∏–ª—å–Ω—É—é –≤–µ—Ä—Å–∏—é –¥–ª—è –æ–±—Ö–æ–¥–∞ –∑–∞—â–∏—Ç—ã –æ—Ç –±–æ—Ç–æ–≤
    """
    
    def __init__(self):
        super().__init__("qidian")
        
        # –°–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ –∑–∞–≥–æ–ª–æ–≤–∫–∏ –¥–ª—è Qidian (–º–æ–±–∏–ª—å–Ω–∞—è –≤–µ—Ä—Å–∏—è)
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (iPhone; CPU iPhone OS 15_0 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/15.0 Mobile/15E148 Safari/604.1',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
        })
        
        # –ë–∞–∑–æ–≤—ã–µ URL
        self.mobile_base_url = "https://m.qidian.com"
        
        # –°—á–µ—Ç—á–∏–∫–∏ –¥–ª—è –∞–¥–∞–ø—Ç–∏–≤–Ω—ã—Ö –ø–∞—É–∑
        self.consecutive_errors = 0
    
    def get_book_info(self, book_url: str) -> Dict:
        """
        –ü–æ–ª—É—á–∏—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –∫–Ω–∏–≥–µ
        """
        book_id = self._extract_book_id(book_url)
        if not book_id:
            raise ValueError(f"–ù–µ —É–¥–∞–µ—Ç—Å—è –∏–∑–≤–ª–µ—á—å ID –∫–Ω–∏–≥–∏ –∏–∑ URL: {book_url}")
        
        mobile_url = f"{self.mobile_base_url}/book/{book_id}/"
        html_content = self._get_page_content(mobile_url, description=f"–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –∫–Ω–∏–≥–µ {book_id}")
        
        if not html_content:
            raise Exception(f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å —Å—Ç—Ä–∞–Ω–∏—Ü—É –∫–Ω–∏–≥–∏: {mobile_url}")
        
        soup = BeautifulSoup(html_content, 'html.parser')
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –∫–Ω–∏–≥–µ
        book_info = {
            'book_id': book_id,
            'title': self._extract_title(soup),
            'author': self._extract_author(soup),
            'status': self._extract_status(soup),
            'genre': self._extract_genre(soup),
            'description': self._extract_description(soup),
            'total_chapters': 0  # –ë—É–¥–µ—Ç –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–æ –≤ get_chapter_list
        }
        
        return book_info
    
    def get_chapter_list(self, book_url: str) -> List[Dict]:
        """
        –ü–æ–ª—É—á–∏—Ç—å —Å–ø–∏—Å–æ–∫ –≥–ª–∞–≤ –∫–Ω–∏–≥–∏
        """
        book_id = self._extract_book_id(book_url)
        if not book_id:
            raise ValueError(f"–ù–µ —É–¥–∞–µ—Ç—Å—è –∏–∑–≤–ª–µ—á—å ID –∫–Ω–∏–≥–∏ –∏–∑ URL: {book_url}")
        
        catalog_url = f"{self.mobile_base_url}/book/{book_id}/catalog"
        html_content = self._get_page_content(catalog_url, description=f"–ö–∞—Ç–∞–ª–æ–≥ –∫–Ω–∏–≥–∏ {book_id}")
        
        if not html_content:
            raise Exception(f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –∫–∞—Ç–∞–ª–æ–≥ –∫–Ω–∏–≥–∏: {catalog_url}")
        
        soup = BeautifulSoup(html_content, 'html.parser')
        
        chapters = []
        
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –æ–±–Ω–æ–≤–ª–µ–Ω–Ω—ã–π —Å–µ–ª–µ–∫—Ç–æ—Ä –¥–ª—è –ø–æ–∏—Å–∫–∞ —Å—Å—ã–ª–æ–∫ –Ω–∞ –≥–ª–∞–≤—ã
        chapter_links = soup.select('a[href*="/chapter/"]')
        
        print(f"üîç –ù–∞–π–¥–µ–Ω–æ —Å—Å—ã–ª–æ–∫ –Ω–∞ –≥–ª–∞–≤—ã: {len(chapter_links)}")
        
        # –§–∏–ª—å—Ç—Ä—É–µ–º –∏ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –≥–ª–∞–≤—ã
        all_chapters = []
        story_chapters = []
        
        # –°–Ω–∞—á–∞–ª–∞ —Å–æ–±–∏—Ä–∞–µ–º –≤—Å–µ –≤–∞–ª–∏–¥–Ω—ã–µ –≥–ª–∞–≤—ã
        for link in chapter_links:
            title = link.get_text(strip=True)
            href = link.get('href', '')
            
            if href:
                # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—ã–π URL –≤ –∞–±—Å–æ–ª—é—Ç–Ω—ã–π
                if href.startswith('//'):
                    chapter_url = f"https:{href}"
                elif href.startswith('/'):
                    chapter_url = f"{self.mobile_base_url}{href}"
                else:
                    chapter_url = href
                
                # –ò–∑–≤–ª–µ–∫–∞–µ–º chapter_id –∏–∑ URL
                chapter_id = self._extract_chapter_id(chapter_url)
                
                if chapter_id:
                    chapter_info = {
                        'title': title,
                        'url': chapter_url, 
                        'chapter_id': chapter_id,
                        'is_story_chapter': self._is_story_chapter(title),
                        'word_count': 0
                    }
                    all_chapters.append(chapter_info)
        
        # –ü—Ä–∏–æ—Ä–∏—Ç–∏–∑–∏—Ä—É–µ–º –≥–ª–∞–≤—ã –∏—Å—Ç–æ—Ä–∏–∏ (–Ω–∞—á–∏–Ω–∞—é—â–∏–µ—Å—è —Å "Á¨¨")
        story_chapters = [ch for ch in all_chapters if ch['is_story_chapter']]
        
        if not story_chapters:
            # –ï—Å–ª–∏ –Ω–µ—Ç –ø—Ä–æ–Ω—É–º–µ—Ä–æ–≤–∞–Ω–Ω—ã—Ö –≥–ª–∞–≤, –±–µ—Ä–µ–º –≤—Å–µ –∫—Ä–æ–º–µ —è–≤–Ω–æ —Å–ª—É–∂–µ–±–Ω—ã—Ö
            print("‚ö†Ô∏è –ü—Ä–æ–Ω—É–º–µ—Ä–æ–≤–∞–Ω–Ω—ã–µ –≥–ª–∞–≤—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã, –∏—Å–ø–æ–ª—å–∑—É–µ–º –≤—Å–µ –¥–æ—Å—Ç—É–ø–Ω—ã–µ")
            story_chapters = [ch for ch in all_chapters if not self._is_service_chapter(ch['title'])]
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –ø–æ—Ä—è–¥–∫–æ–≤–æ–º—É –Ω–æ–º–µ—Ä—É –≤ URL (chapter_id)
        story_chapters.sort(key=lambda x: int(x['chapter_id']))
        
        # –î–æ–±–∞–≤–ª—è–µ–º –Ω–æ–º–µ—Ä–∞ –≥–ª–∞–≤
        for i, chapter in enumerate(story_chapters, 1):
            chapter['number'] = i
            del chapter['is_story_chapter']  # –£–±–∏—Ä–∞–µ–º —Å–ª—É–∂–µ–±–Ω–æ–µ –ø–æ–ª–µ
        
        print(f"üìñ –ù–∞–π–¥–µ–Ω–æ –≥–ª–∞–≤ –∏—Å—Ç–æ—Ä–∏–∏: {len(story_chapters)}")
        
        if story_chapters:
            print(f"üìã –ü–µ—Ä–≤—ã–µ 3 –≥–ª–∞–≤—ã:")
            for i, ch in enumerate(story_chapters[:3]):
                print(f"   {i+1}. {ch['title']}")
        
        return story_chapters

    def _is_service_chapter(self, title: str) -> bool:
        """
        –ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ –≥–ª–∞–≤–∞ —Å–ª—É–∂–µ–±–Ω–æ–π (—É–≤–µ–¥–æ–º–ª–µ–Ω–∏—è –∞–≤—Ç–æ—Ä–∞ –∏ —Ç.–¥.)
        """
        service_keywords = [
            # –°–ª—É–∂–µ–±–Ω—ã–µ —É–≤–µ–¥–æ–º–ª–µ–Ω–∏—è
            'Êñ∞‰π¶', 'ÂèëÂ∏É', 'ÈÄöÁü•', 'ÂÖ¨Âëä', 'ËØ¥Êòé', 'ÊäΩÂ•ñ', 'Ê¥ªÂä®',
            'ÊïôÁ®ã', 'Â§ñ‰º†', 'Áï™Â§ñ', 'ÊÑüË®Ä', 'Êé®Ëçê', 'È™óÂ≠ê', 'ÂÜíÂÖÖ',
            'Êµ∑Èáè', 'iPad', 'Ëµ∑ÁÇπÂ∏Å', 'ÁªèÈ™å', 'Êé®ËçêÁ•®',
            # –î–∞—Ç—ã –≤ –∑–∞–≥–æ–ª–æ–≤–∫–∞—Ö (–ø—Ä–∏–∑–Ω–∞–∫ —É–≤–µ–¥–æ–º–ª–µ–Ω–∏–π)
            '2022-', '2023-', '2024-', '2025-',
            '‰ΩúÂÆ∂ÂÖ•È©ª', 'Âç≥Êõ¥Âç≥Áúã', 'ËøòÊúâÁï™Â§ñ'
        ]
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ –Ω–∞–ª–∏—á–∏–µ —Å–ª—É–∂–µ–±–Ω—ã—Ö –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
        for keyword in service_keywords:
            if keyword in title:
                return True
        
        # –ì–ª–∞–≤—ã —Å –æ—á–µ–Ω—å –∫–æ—Ä–æ—Ç–∫–∏–º–∏ –Ω–∞–∑–≤–∞–Ω–∏—è–º–∏ —á–∞—Å—Ç–æ —Å–ª—É–∂–µ–±–Ω—ã–µ
        if len(title.strip()) < 3:
            return True
        
        # –ì–ª–∞–≤—ã —Å–æ–¥–µ—Ä–∂–∞—â–∏–µ —Ç–æ–ª—å–∫–æ –¥–∞—Ç—ã –∏ –≤—Ä–µ–º—è
        if '19:00' in title or '‰ΩúÂÆ∂ÂÖ•È©ª' in title:
            return True
            
        return False

    def _is_story_chapter(self, title: str) -> bool:
        """
        –ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ –≥–ª–∞–≤–∞ —á–∞—Å—Ç—å—é –æ—Å–Ω–æ–≤–Ω–æ–π –∏—Å—Ç–æ—Ä–∏–∏
        """
        # –ì–ª–∞–≤—ã –∏—Å—Ç–æ—Ä–∏–∏ –æ–±—ã—á–Ω–æ –Ω–∞—á–∏–Ω–∞—é—Ç—Å—è —Å "Á¨¨" (–≥–ª–∞–≤–∞) –∏ —Å–æ–¥–µ—Ä–∂–∞—Ç –Ω–æ–º–µ—Ä
        if title.startswith('Á¨¨') and ('Á´†' in title or 'Âõû' in title):
            return True
        
        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è –≥–ª–∞–≤ –∏—Å—Ç–æ—Ä–∏–∏
        story_patterns = [
            r'Á¨¨\d+Á´†',  # Á¨¨1Á´†, Á¨¨123Á´† –∏ —Ç.–¥.
            r'Á¨¨\d+Âõû',  # Á¨¨1Âõû, Á¨¨123Âõû –∏ —Ç.–¥.
            r'Chapter \d+',  # –ê–Ω–≥–ª–∏–π—Å–∫–∏–π —Ñ–æ—Ä–º–∞—Ç
            r'Ch\.\d+',  # –°–æ–∫—Ä–∞—â–µ–Ω–Ω—ã–π –∞–Ω–≥–ª–∏–π—Å–∫–∏–π —Ñ–æ—Ä–º–∞—Ç
        ]
        
        import re
        for pattern in story_patterns:
            if re.search(pattern, title):
                return True
        
        return False
    
    def get_chapter_content(self, chapter_url: str) -> Dict:
        """
        –ü–æ–ª—É—á–∏—Ç—å —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ –≥–ª–∞–≤—ã
        """
        html_content = self._get_page_content(chapter_url, description="–°–æ–¥–µ—Ä–∂–∏–º–æ–µ –≥–ª–∞–≤—ã")
        
        if not html_content:
            raise Exception(f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ –≥–ª–∞–≤—ã: {chapter_url}")
        
        soup = BeautifulSoup(html_content, 'html.parser')
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫ –≥–ª–∞–≤—ã
        title_elem = soup.select_one('h1.chapter__title, .title')
        title = title_elem.get_text(strip=True) if title_elem else "–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è –≥–ª–∞–≤–∞"
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ –≥–ª–∞–≤—ã
        content_elem = soup.select_one('.chapter__content, .content')
        if not content_elem:
            raise Exception("–ù–µ —É–¥–∞–ª–æ—Å—å –Ω–∞–π—Ç–∏ —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ –≥–ª–∞–≤—ã")
        
        # –û—á–∏—â–∞–µ–º —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ –æ—Ç –ª–∏—à–Ω–∏—Ö —ç–ª–µ–º–µ–Ω—Ç–æ–≤
        content = self._clean_chapter_content(content_elem)
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º chapter_id –∏–∑ URL
        chapter_id = self._extract_chapter_id(chapter_url)
        
        return {
            'title': title,
            'content': content,
            'chapter_id': chapter_id,
            'word_count': len(content)
        }
    
    def _delay_between_requests(self):
        """
        –ê–¥–∞–ø—Ç–∏–≤–Ω–∞—è –ø–∞—É–∑–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏ –¥–ª—è Qidian
        """
        # –ë–∞–∑–æ–≤–∞—è –ø–∞—É–∑–∞
        base_delay = 1.5
        
        # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º –ø–∞—É–∑—É –ø—Ä–∏ –æ—à–∏–±–∫–∞—Ö
        if self.consecutive_errors > 0:
            base_delay *= (1 + self.consecutive_errors * 0.5)
        
        # –°–ª—É—á–∞–π–Ω–∞—è —Å–æ—Å—Ç–∞–≤–ª—è—é—â–∞—è –¥–ª—è –∏–º–∏—Ç–∞—Ü–∏–∏ —á–µ–ª–æ–≤–µ–∫–∞
        random_factor = random.uniform(0.2, 0.8)
        delay = base_delay + random_factor
        
        print(f"‚è≥ –ü–∞—É–∑–∞ {delay:.1f}s...")
        time.sleep(delay)
    
    def _get_page_content(self, url: str, timeout: int = 10, description: str = "") -> Optional[str]:
        """
        –ü–µ—Ä–µ–æ–ø—Ä–µ–¥–µ–ª—è–µ–º –º–µ—Ç–æ–¥ –¥–ª—è —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏ Qidian
        """
        try:
            self.request_count += 1
            
            if description:
                print(f"üåê {description}: {url}")
            
            response = self.session.get(url, timeout=timeout)
            
            print(f"   –°—Ç–∞—Ç—É—Å: {response.status_code}")
            print(f"   –†–∞–∑–º–µ—Ä: {len(response.content):,} –±–∞–π—Ç")
            
            if response.status_code == 200:
                html_content = response.text
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–∞—á–µ—Å—Ç–≤–æ HTML –¥–ª—è Qidian
                if (html_content and 
                    len(html_content) > 5000 and
                    html_content.strip().startswith('<') and
                    ('Ëµ∑ÁÇπ‰∏≠ÊñáÁΩë' in html_content or 'qidian.com' in html_content)):
                    
                    self.success_count += 1
                    self.consecutive_errors = 0
                    print(f"‚úÖ –ö–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–π HTML –ø–æ–ª—É—á–µ–Ω")
                    return html_content
                else:
                    print("‚ö†Ô∏è HTML –Ω–µ –ø—Ä–æ—à–µ–ª –ø—Ä–æ–≤–µ—Ä–∫—É –∫–∞—á–µ—Å—Ç–≤–∞")
                    self.consecutive_errors += 1
                    
            elif response.status_code == 202:
                print("‚ö†Ô∏è –°–µ—Ä–≤–µ—Ä –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç 202 - –≤–æ–∑–º–æ–∂–Ω–∞—è –∑–∞—â–∏—Ç–∞ –æ—Ç –±–æ—Ç–æ–≤")
                self.consecutive_errors += 1
                time.sleep(5)
                
            elif response.status_code == 429:
                print("‚ö†Ô∏è Rate limiting - —Å–ª–∏—à–∫–æ–º –º–Ω–æ–≥–æ –∑–∞–ø—Ä–æ—Å–æ–≤")
                self.consecutive_errors += 1
                time.sleep(10)
                
            else:
                print(f"‚ùå HTTP –æ—à–∏–±–∫–∞: {response.status_code}")
                self.consecutive_errors += 1
                
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–ø—Ä–æ—Å–∞ –∫ {url}: {e}")
            self.consecutive_errors += 1
            
        return None
    
    def _extract_book_id(self, book_url: str) -> Optional[str]:
        """
        –ò–∑–≤–ª–µ—á—å ID –∫–Ω–∏–≥–∏ –∏–∑ URL
        """
        # –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º —Ä–∞–∑–ª–∏—á–Ω—ã–µ —Ñ–æ—Ä–º–∞—Ç—ã URL
        patterns = [
            r'/book/(\d+)/?',
            r'qidian\.com/book/(\d+)',
            r'm\.qidian\.com/book/(\d+)',
        ]
        
        for pattern in patterns:
            match = re.search(pattern, book_url)
            if match:
                return match.group(1)
        
        return None
    
    def _extract_chapter_id(self, chapter_url: str) -> Optional[str]:
        """
        –ò–∑–≤–ª–µ—á—å ID –≥–ª–∞–≤—ã –∏–∑ URL
        """
        match = re.search(r'/chapter/\d+/(\d+)/?', chapter_url)
        return match.group(1) if match else None
    
    def _extract_title(self, soup: BeautifulSoup) -> str:
        """
        –ò–∑–≤–ª–µ—á—å –Ω–∞–∑–≤–∞–Ω–∏–µ –∫–Ω–∏–≥–∏
        """
        # –û—Å–Ω–æ–≤–Ω–æ–π –∑–∞–≥–æ–ª–æ–≤–æ–∫ –∫–Ω–∏–≥–∏ –Ω–∞—Ö–æ–¥–∏—Ç—Å—è –≤ h1
        title_elem = soup.select_one('h1')
        if title_elem:
            title = title_elem.get_text(strip=True)
            if title and title != 'ÁÆÄ‰ªã' and title != 'ÁõÆÂΩï':  # –ò—Å–∫–ª—é—á–∞–µ–º —Å–ª—É–∂–µ–±–Ω—ã–µ –∑–∞–≥–æ–ª–æ–≤–∫–∏
                return title
        
        # Fallback —Å–µ–ª–µ–∫—Ç–æ—Ä—ã
        selectors = [
            '[class*="title"]',
            '.title',
            '.book-name',
            '.book__name'
        ]
        
        for selector in selectors:
            elem = soup.select_one(selector)
            if elem:
                title = elem.get_text(strip=True)
                if title and len(title) > 2 and title not in ['ÁÆÄ‰ªã', 'ÁõÆÂΩï']:
                    return title
        
        return "–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ"
    
    def _extract_author(self, soup: BeautifulSoup) -> str:
        """
        –ò–∑–≤–ª–µ—á—å –∞–≤—Ç–æ—Ä–∞ –∫–Ω–∏–≥–∏
        """
        # –ò—â–µ–º –∞–≤—Ç–æ—Ä–∞ –ø–æ –∫–ª–∞—Å—Å–∞–º —Å–æ–¥–µ—Ä–∂–∞—â–∏–º "author"
        author_elements = soup.select('[class*="author"]')
        for elem in author_elements:
            text = elem.get_text(strip=True)
            if text and len(text) > 1 and len(text) < 50:  # –†–∞–∑—É–º–Ω–∞—è –¥–ª–∏–Ω–∞ –¥–ª—è –∏–º–µ–Ω–∏ –∞–≤—Ç–æ—Ä–∞
                return text
        
        # Fallback —Å–µ–ª–µ–∫—Ç–æ—Ä—ã
        selectors = [
            '.book__author',
            '.book-author',
            '.author',
            '.book-info__author',
            '.writer',
            '.book-writer'
        ]
        
        for selector in selectors:
            elem = soup.select_one(selector)
            if elem:
                return elem.get_text(strip=True)
        
        return "–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ"
    
    def _extract_status(self, soup: BeautifulSoup) -> str:
        """
        –ò–∑–≤–ª–µ—á—å —Å—Ç–∞—Ç—É—Å –∫–Ω–∏–≥–∏
        """
        selectors = [
            '.book__status',
            '.book-status',
            '.status'
        ]
        
        for selector in selectors:
            elem = soup.select_one(selector)
            if elem:
                return elem.get_text(strip=True)
        
        return "–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ"
    
    def _extract_genre(self, soup: BeautifulSoup) -> str:
        """
        –ò–∑–≤–ª–µ—á—å –∂–∞–Ω—Ä –∫–Ω–∏–≥–∏
        """
        selectors = [
            '.book__genre',
            '.book-genre',
            '.genre',
            '.tag'
        ]
        
        for selector in selectors:
            elem = soup.select_one(selector)
            if elem:
                return elem.get_text(strip=True)
        
        return "–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ"
    
    def _extract_description(self, soup: BeautifulSoup) -> str:
        """
        –ò–∑–≤–ª–µ—á—å –æ–ø–∏—Å–∞–Ω–∏–µ –∫–Ω–∏–≥–∏
        """
        selectors = [
            '.book__desc',
            '.book-description',
            '.description',
            '.book-intro'
        ]
        
        for selector in selectors:
            elem = soup.select_one(selector)
            if elem:
                return elem.get_text(strip=True)
        
        return "–ù–µ—Ç –æ–ø–∏—Å–∞–Ω–∏—è"
    
    def _clean_chapter_content(self, content_elem) -> str:
        """
        –û—á–∏—Å—Ç–∏—Ç—å —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ –≥–ª–∞–≤—ã –æ—Ç –ª–∏—à–Ω–∏—Ö —ç–ª–µ–º–µ–Ω—Ç–æ–≤
        """
        # –£–¥–∞–ª—è–µ–º –Ω–µ–Ω—É–∂–Ω—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã
        for unwanted in content_elem.select('script, style, .ad, .advertisement'):
            unwanted.decompose()
        
        # –ü–æ–ª—É—á–∞–µ–º —Ç–µ–∫—Å—Ç —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º –∞–±–∑–∞—Ü–µ–≤
        paragraphs = []
        for p in content_elem.find_all(['p', 'div']):
            text = p.get_text(strip=True)
            if text and len(text) > 10:  # –ò–≥–Ω–æ—Ä–∏—Ä—É–µ–º –∫–æ—Ä–æ—Ç–∫–∏–µ —Å—Ç—Ä–æ–∫–∏
                paragraphs.append(text)
        
        return '\n\n'.join(paragraphs) if paragraphs else content_elem.get_text(strip=True)


def main():
    """
    –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è —Ä–∞–±–æ—Ç—ã –ø–∞—Ä—Å–µ—Ä–∞ Qidian
    """
    print("üìö QIDIAN PARSER - –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è")
    print("=" * 50)
    
    # –¢–µ—Å—Ç–æ–≤–∞—è –∫–Ω–∏–≥–∞
    book_url = "https://www.qidian.com/book/1209977/"  # ÊñóÁ†¥ËãçÁ©π
    
    with QidianParser() as parser:
        try:
            # –ü–æ–ª—É—á–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –∫–Ω–∏–≥–µ
            print("üìñ –ü–æ–ª—É—á–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –∫–Ω–∏–≥–µ...")
            book_info = parser.get_book_info(book_url)
            print(f"‚úÖ –ù–∞–∑–≤–∞–Ω–∏–µ: {book_info['title']}")
            print(f"‚úÖ –ê–≤—Ç–æ—Ä: {book_info['author']}")
            print(f"‚úÖ –ñ–∞–Ω—Ä: {book_info['genre']}")
            print()
            
            # –°–∫–∞—á–∏–≤–∞–µ–º –∫–Ω–∏–≥—É (–æ–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º 3 –≥–ª–∞–≤–∞–º–∏ –¥–ª—è –¥–µ–º–æ)
            print("üìö –°–∫–∞—á–∏–≤–∞–Ω–∏–µ –∫–Ω–∏–≥–∏...")
            results = parser.download_book(book_url, "./qidian_parsed", chapter_limit=3)
            
            # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
            stats = parser.get_stats()
            print(f"\nüìä –°–¢–ê–¢–ò–°–¢–ò–ö–ê:")
            print(f"   –í—Å–µ–≥–æ –∑–∞–ø—Ä–æ—Å–æ–≤: {stats['total_requests']}")
            print(f"   –£—Å–ø–µ—à–Ω—ã—Ö: {stats['successful_requests']}")
            print(f"   –£—Å–ø–µ—à–Ω–æ—Å—Ç—å: {stats['success_rate']:.1%}")
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞: {e}")


if __name__ == "__main__":
    main()