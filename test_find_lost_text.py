#!/usr/bin/env python3
"""
–ê–Ω–∞–ª–∏–∑ –ø–æ—Ç–µ—Ä—è–Ω–Ω—ã—Ö —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤ —Ç–µ–∫—Å—Ç–∞ –ø—Ä–∏ —Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∏–∏
–ù–æ–≤–µ–ª–ª–∞ 21, –ì–ª–∞–≤–∞ 1
"""
import sys
import re
from pathlib import Path
from difflib import SequenceMatcher

# –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Ç—å –∫ –ø—Ä–æ–µ–∫—Ç—É
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root / 'web_app'))

from app import create_app
from app.models import Chapter, BilingualAlignment, Translation

app = create_app()


def split_sentences(text, language='ru'):
    """–†–∞–∑–±–∏–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –Ω–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è"""
    if language == 'zh':
        # –ö–∏—Ç–∞–π—Å–∫–∏–π: —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª–∏ „ÄÇÔºÅÔºü
        sentences = re.split(r'([„ÄÇÔºÅÔºü])', text)
    else:
        # –†—É—Å—Å–∫–∏–π: —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª–∏ .!?
        sentences = re.split(r'([.!?])', text)

    # –û–±—ä–µ–¥–∏–Ω—è–µ–º –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ —Å —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª–µ–º
    result = []
    for i in range(0, len(sentences) - 1, 2):
        if i + 1 < len(sentences):
            sent = sentences[i] + sentences[i + 1]
            sent = sent.strip()
            if sent:
                result.append(sent)

    # –ü–æ—Å–ª–µ–¥–Ω–µ–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ –±–µ–∑ —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—è
    if len(sentences) % 2 == 1 and sentences[-1].strip():
        result.append(sentences[-1].strip())

    return result


def find_similar(text, text_list, threshold=0.8):
    """–ü–æ–∏—Å–∫ –ø–æ—Ö–æ–∂–µ–≥–æ —Ç–µ–∫—Å—Ç–∞ –≤ —Å–ø–∏—Å–∫–µ"""
    text_lower = text.lower().strip()

    for item in text_list:
        item_lower = item.lower().strip()

        # –¢–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ
        if text_lower == item_lower:
            return True, 1.0

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –≤—Ö–æ–∂–¥–µ–Ω–∏—è
        if text_lower in item_lower or item_lower in text_lower:
            return True, 0.9

        # –°—Ö–æ–∂–µ—Å—Ç—å —á–µ—Ä–µ–∑ SequenceMatcher
        ratio = SequenceMatcher(None, text_lower, item_lower).ratio()
        if ratio >= threshold:
            return True, ratio

    return False, 0.0


def analyze_lost_text():
    """–ê–Ω–∞–ª–∏–∑ –ø–æ—Ç–µ—Ä—è–Ω–Ω—ã—Ö —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤"""
    with app.app_context():
        # –ü–æ–ª—É—á–∞–µ–º –¥–∞–Ω–Ω—ã–µ
        chapter = Chapter.query.filter_by(novel_id=21, chapter_number=1).first()
        translation = Translation.query.filter_by(
            chapter_id=chapter.id,
            translation_type='edited'
        ).first()
        alignment = BilingualAlignment.query.filter_by(chapter_id=chapter.id).first()

        if not all([chapter, translation, alignment]):
            print("‚ùå –î–∞–Ω–Ω—ã–µ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")
            return

        # –ò—Å—Ö–æ–¥–Ω—ã–µ —Ç–µ–∫—Å—Ç—ã
        russian_text = translation.translated_text
        chinese_text = chapter.original_text

        # –í—ã—Ä–æ–≤–Ω–µ–Ω–Ω—ã–µ —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã
        alignments = alignment.alignment_data.get('alignments', [])

        aligned_ru_fragments = [pair['ru'] for pair in alignments]
        aligned_zh_fragments = [pair['zh'] for pair in alignments]

        print("=" * 80)
        print("üîç –ê–ù–ê–õ–ò–ó –ü–û–¢–ï–†–Ø–ù–ù–´–• –§–†–ê–ì–ú–ï–ù–¢–û–í")
        print("=" * 80)

        # –ê–Ω–∞–ª–∏–∑ —Ä—É—Å—Å–∫–æ–≥–æ —Ç–µ–∫—Å—Ç–∞
        print("\nüìù –ê–ù–ê–õ–ò–ó –†–£–°–°–ö–û–ì–û –¢–ï–ö–°–¢–ê:")
        print(f"–ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç: {len(russian_text)} —Å–∏–º–≤–æ–ª–æ–≤")

        ru_sentences = split_sentences(russian_text, 'ru')
        print(f"–ü—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π: {len(ru_sentences)}")

        ru_lost = []
        ru_found = 0

        for sent in ru_sentences:
            found, ratio = find_similar(sent, aligned_ru_fragments, threshold=0.7)
            if found:
                ru_found += 1
            else:
                ru_lost.append(sent)

        print(f"–ù–∞–π–¥–µ–Ω–æ –≤ –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–∏: {ru_found}/{len(ru_sentences)}")
        print(f"–ü–æ—Ç–µ—Ä—è–Ω–æ: {len(ru_lost)} –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π")

        if ru_lost:
            print(f"\n‚ùå –ü–û–¢–ï–†–Ø–ù–ù–´–ï –†–£–°–°–ö–ò–ï –§–†–ê–ì–ú–ï–ù–¢–´:")
            for i, sent in enumerate(ru_lost, 1):
                print(f"  [{i}] {sent[:100]}{'...' if len(sent) > 100 else ''}")

        # –ê–Ω–∞–ª–∏–∑ –∫–∏—Ç–∞–π—Å–∫–æ–≥–æ —Ç–µ–∫—Å—Ç–∞
        print("\n" + "=" * 80)
        print("üìù –ê–ù–ê–õ–ò–ó –ö–ò–¢–ê–ô–°–ö–û–ì–û –¢–ï–ö–°–¢–ê:")
        print(f"–ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç: {len(chinese_text)} —Å–∏–º–≤–æ–ª–æ–≤")

        zh_sentences = split_sentences(chinese_text, 'zh')
        print(f"–ü—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π: {len(zh_sentences)}")

        zh_lost = []
        zh_found = 0

        for sent in zh_sentences:
            found, ratio = find_similar(sent, aligned_zh_fragments, threshold=0.7)
            if found:
                zh_found += 1
            else:
                zh_lost.append(sent)

        print(f"–ù–∞–π–¥–µ–Ω–æ –≤ –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–∏: {zh_found}/{len(zh_sentences)}")
        print(f"–ü–æ—Ç–µ—Ä—è–Ω–æ: {len(zh_lost)} –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π")

        if zh_lost:
            print(f"\n‚ùå –ü–û–¢–ï–†–Ø–ù–ù–´–ï –ö–ò–¢–ê–ô–°–ö–ò–ï –§–†–ê–ì–ú–ï–ù–¢–´:")
            for i, sent in enumerate(zh_lost, 1):
                print(f"  [{i}] {sent}")

        # –ê–Ω–∞–ª–∏–∑ –æ–±—ä–µ–º–æ–≤ –ø–æ—Ç–µ—Ä—å
        print("\n" + "=" * 80)
        print("üìä –°–¢–ê–¢–ò–°–¢–ò–ö–ê –ü–û–¢–ï–†–¨:")

        ru_lost_chars = sum(len(s) for s in ru_lost)
        zh_lost_chars = sum(len(s) for s in zh_lost)

        print(f"\n–†—É—Å—Å–∫–∏–π:")
        print(f"  –ü–æ—Ç–µ—Ä—è–Ω–æ —Å–∏–º–≤–æ–ª–æ–≤: {ru_lost_chars}/{len(russian_text)} ({ru_lost_chars/len(russian_text)*100:.2f}%)")
        print(f"  –ü–æ—Ç–µ—Ä—è–Ω–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π: {len(ru_lost)}/{len(ru_sentences)} ({len(ru_lost)/len(ru_sentences)*100:.2f}%)")

        print(f"\n–ö–∏—Ç–∞–π—Å–∫–∏–π:")
        print(f"  –ü–æ—Ç–µ—Ä—è–Ω–æ —Å–∏–º–≤–æ–ª–æ–≤: {zh_lost_chars}/{len(chinese_text)} ({zh_lost_chars/len(chinese_text)*100:.2f}%)")
        print(f"  –ü–æ—Ç–µ—Ä—è–Ω–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π: {len(zh_lost)}/{len(zh_sentences)} ({len(zh_lost)/len(zh_sentences)*100:.2f}%)")

        print("\n" + "=" * 80)


if __name__ == '__main__':
    analyze_lost_text()
