#!/usr/bin/env python3
"""
Debug version of Qidian parser to analyze HTML structure
"""
import requests
from bs4 import BeautifulSoup
import json

def debug_book_info():
    """Debug book info extraction"""
    print("üîç Debug: Book Info Extraction")
    print("=" * 50)
    
    session = requests.Session()
    session.headers.update({
        'User-Agent': 'Mozilla/5.0 (iPhone; CPU iPhone OS 14_0 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.0 Mobile/15E148 Safari/604.1',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
        'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
        'Accept-Encoding': 'gzip, deflate, br',
        'Connection': 'keep-alive',
    })
    
    book_id = "1209977"
    url = f"https://m.qidian.com/book/{book_id}"
    
    print(f"üìû –ó–∞–ø—Ä–æ—Å: {url}")
    response = session.get(url, timeout=10)
    print(f"üìä –°—Ç–∞—Ç—É—Å: {response.status_code}")
    print(f"üìè –†–∞–∑–º–µ—Ä: {len(response.text)} —Å–∏–º–≤–æ–ª–æ–≤")
    
    if response.status_code == 200:
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –≤—Å–µ –≤–æ–∑–º–æ–∂–Ω—ã–µ —Å–µ–ª–µ–∫—Ç–æ—Ä—ã –¥–ª—è –Ω–∞–∑–≤–∞–Ω–∏—è
        print("\nüîç –ü–æ–∏—Å–∫ –Ω–∞–∑–≤–∞–Ω–∏—è –∫–Ω–∏–≥–∏:")
        title_selectors = [
            'h1.detail__header-detail__title',
            'h1',
            '.detail__header-detail__title',
            '.title',
            '.book-title'
        ]
        
        for selector in title_selectors:
            elements = soup.select(selector)
            print(f"   {selector}: {len(elements)} —ç–ª–µ–º–µ–Ω—Ç–æ–≤")
            for i, elem in enumerate(elements[:3]):  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ 3
                print(f"      {i+1}: {elem.text.strip()[:50]}")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –≤—Å–µ –≤–æ–∑–º–æ–∂–Ω—ã–µ —Å–µ–ª–µ–∫—Ç–æ—Ä—ã –¥–ª—è –∞–≤—Ç–æ—Ä–∞
        print("\n‚úçÔ∏è –ü–æ–∏—Å–∫ –∞–≤—Ç–æ—Ä–∞:")
        author_selectors = [
            'a.detail__header-detail__author-link',
            '.detail__header-detail__author-link',
            '.author',
            '.writer',
            'a[href*="author"]'
        ]
        
        for selector in author_selectors:
            elements = soup.select(selector)
            print(f"   {selector}: {len(elements)} —ç–ª–µ–º–µ–Ω—Ç–æ–≤")
            for i, elem in enumerate(elements[:3]):
                print(f"      {i+1}: {elem.text.strip()[:50]}")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –≤—Å–µ –≤–æ–∑–º–æ–∂–Ω—ã–µ —Å–µ–ª–µ–∫—Ç–æ—Ä—ã –¥–ª—è –∂–∞–Ω—Ä–∞
        print("\nüè∑Ô∏è –ü–æ–∏—Å–∫ –∂–∞–Ω—Ä–∞:")
        genre_selectors = [
            'a.detail__header-detail__category',
            '.detail__header-detail__category',
            '.category',
            '.genre'
        ]
        
        for selector in genre_selectors:
            elements = soup.select(selector)
            print(f"   {selector}: {len(elements)} —ç–ª–µ–º–µ–Ω—Ç–æ–≤")
            for i, elem in enumerate(elements[:3]):
                print(f"      {i+1}: {elem.text.strip()[:30]}")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –æ–ø–∏—Å–∞–Ω–∏–µ
        print("\nüìù –ü–æ–∏—Å–∫ –æ–ø–∏—Å–∞–Ω–∏—è:")
        desc_selectors = [
            'p.detail__summary__content',
            '.detail__summary__content',
            '.summary',
            '.description'
        ]
        
        for selector in desc_selectors:
            elements = soup.select(selector)
            print(f"   {selector}: {len(elements)} —ç–ª–µ–º–µ–Ω—Ç–æ–≤")
            for i, elem in enumerate(elements[:2]):
                content = elem.text.strip()
                print(f"      {i+1}: {content[:100]}...")
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º HTML –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
        with open('debug_book_page.html', 'w', encoding='utf-8') as f:
            f.write(response.text)
        print(f"\nüíæ HTML —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ debug_book_page.html")

def debug_catalog():
    """Debug catalog extraction"""
    print("\nüîç Debug: Catalog Extraction")
    print("=" * 50)
    
    session = requests.Session()
    session.headers.update({
        'User-Agent': 'Mozilla/5.0 (iPhone; CPU iPhone OS 14_0 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.0 Mobile/15E148 Safari/604.1',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
        'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
        'Accept-Encoding': 'gzip, deflate, br',
        'Connection': 'keep-alive',
    })
    
    book_id = "1209977"
    catalog_url = f"https://m.qidian.com/book/{book_id}/catalog/"
    
    print(f"üìû –ó–∞–ø—Ä–æ—Å –∫–∞—Ç–∞–ª–æ–≥–∞: {catalog_url}")
    response = session.get(catalog_url, timeout=10)
    print(f"üìä –°—Ç–∞—Ç—É—Å: {response.status_code}")
    print(f"üìè –†–∞–∑–º–µ—Ä: {len(response.text)} —Å–∏–º–≤–æ–ª–æ–≤")
    
    if response.status_code == 200:
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # –ò—â–µ–º —Å—Å—ã–ª–∫–∏ –Ω–∞ –≥–ª–∞–≤—ã —Ä–∞–∑–Ω—ã–º–∏ —Å–ø–æ—Å–æ–±–∞–º–∏
        print("\nüìë –ü–æ–∏—Å–∫ –≥–ª–∞–≤:")
        
        # –°–ø–æ—Å–æ–± 1: –ø–æ –∫–ª–∞—Å—Å—É
        chapter_links_class = soup.find_all('a', class_='chapter-link')
        print(f"   –ü–æ –∫–ª–∞—Å—Å—É 'chapter-link': {len(chapter_links_class)} —ç–ª–µ–º–µ–Ω—Ç–æ–≤")
        
        # –°–ø–æ—Å–æ–± 2: –ø–æ href –ø–∞—Ç—Ç–µ—Ä–Ω—É
        import re
        chapter_links_href = soup.find_all('a', href=re.compile(r'/chapter/\d+/\d+/'))
        print(f"   –ü–æ href –ø–∞—Ç—Ç–µ—Ä–Ω—É: {len(chapter_links_href)} —ç–ª–µ–º–µ–Ω—Ç–æ–≤")
        
        # –°–ø–æ—Å–æ–± 3: –≤—Å–µ —Å—Å—ã–ª–∫–∏ —Å–æ —Å–ª–æ–≤–æ–º "chapter"
        all_chapter_links = soup.find_all('a', href=re.compile(r'chapter'))
        print(f"   –í—Å–µ —Å—Å—ã–ª–∫–∏ —Å 'chapter': {len(all_chapter_links)} —ç–ª–µ–º–µ–Ω—Ç–æ–≤")
        
        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ –Ω–∞–π–¥–µ–Ω–Ω—ã–µ —Å—Å—ã–ª–∫–∏
        links_to_show = chapter_links_class or chapter_links_href or all_chapter_links
        if links_to_show:
            print("\nüìñ –ü–µ—Ä–≤—ã–µ –Ω–∞–π–¥–µ–Ω–Ω—ã–µ –≥–ª–∞–≤—ã:")
            for i, link in enumerate(links_to_show[:5]):
                href = link.get('href', '–ù–µ—Ç href')
                title = link.text.strip()
                print(f"      {i+1}: {title[:40]} ‚Üí {href}")
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º HTML –∫–∞—Ç–∞–ª–æ–≥–∞
        with open('debug_catalog_page.html', 'w', encoding='utf-8') as f:
            f.write(response.text)
        print(f"\nüíæ HTML –∫–∞—Ç–∞–ª–æ–≥–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ debug_catalog_page.html")
        
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—É —Å—Ç—Ä–∞–Ω–∏—Ü—ã
        print(f"\nüèóÔ∏è –°—Ç—Ä—É–∫—Ç—É—Ä–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—ã:")
        
        # –ò—â–µ–º –æ—Å–Ω–æ–≤–Ω—ã–µ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä—ã
        containers = [
            ('div', 'class', 'catalog'),
            ('div', 'class', 'chapter-list'),
            ('ul', None, None),
            ('div', 'id', 'chapter-list')
        ]
        
        for tag, attr, value in containers:
            if attr and value:
                elements = soup.find_all(tag, {attr: value})
            else:
                elements = soup.find_all(tag)
            print(f"   {tag}" + (f"[{attr}='{value}']" if attr else "") + f": {len(elements)} —ç–ª–µ–º–µ–Ω—Ç–æ–≤")

if __name__ == "__main__":
    debug_book_info()
    debug_catalog()