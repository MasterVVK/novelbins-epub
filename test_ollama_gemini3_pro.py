#!/usr/bin/env python3
"""
ะขะตััะพะฒัะน ัะบัะธะฟั ะดะปั ะพะฑัะฐัะตะฝะธั ะบ ะผะพะดะตะปะธ gemini-3-pro-preview ัะตัะตะท Ollama

ะัะพะฒะตััะตั:
- ะะพัััะฟะฝะพััั ะผะพะดะตะปะธ ะฒ Ollama
- ะะพััะตะบัะฝะพััั ะณะตะฝะตัะฐัะธะธ ะบะพะฝัะตะฝัะฐ
- ะะฐัะฐะผะตััั ะทะฐะฟัะพัะฐ (temperature, num_ctx, num_predict)
- ะะพะณะธัะพะฒะฐะฝะธะต ัะตะทัะปััะฐัะพะฒ
"""
import httpx
import json
import time
from datetime import datetime


class OllamaGemini3ProTester:
    """ะขะตััะตั ะดะปั Ollama ะผะพะดะตะปะธ gemini-3-pro-preview"""

    def __init__(self, base_url: str = "http://localhost:11434"):
        self.base_url = base_url
        self.model_name = "gemini-3-pro-preview"

    def check_model_availability(self) -> bool:
        """ะัะพะฒะตัะบะฐ ะดะพัััะฟะฝะพััะธ ะผะพะดะตะปะธ"""
        print("\n" + "="*80)
        print("๐ ะะะะะะะะ ะะะกะขะฃะะะะกะขะ ะะะะะะ")
        print("="*80)

        try:
            response = httpx.get(f"{self.base_url}/api/tags", timeout=10.0)
            if response.status_code == 200:
                data = response.json()
                available_models = [m['name'] for m in data.get('models', [])]

                print(f"โ Ollama ัะตัะฒะตั ะดะพัััะฟะตะฝ")
                print(f"๐ ะัะตะณะพ ะผะพะดะตะปะตะน: {len(available_models)}")

                # ะัะตะผ ะฝะฐัั ะผะพะดะตะปั
                matching_models = [m for m in available_models if 'gemini-3-pro-preview' in m]

                if matching_models:
                    print(f"โ ะะพะดะตะปั ะฝะฐะนะดะตะฝะฐ: {matching_models[0]}")
                    self.model_name = matching_models[0]  # ะัะฟะพะปัะทัะตะผ ะฟะพะปะฝะพะต ะธะผั ั ัะตะณะพะผ
                    return True
                else:
                    print(f"โ ะะพะดะตะปั gemini-3-pro-preview ะฝะต ะฝะฐะนะดะตะฝะฐ")
                    print(f"\nะะพัััะฟะฝัะต ะผะพะดะตะปะธ:")
                    for model in available_models[:10]:  # ะะพะบะฐะทัะฒะฐะตะผ ะฟะตัะฒัะต 10
                        print(f"  - {model}")
                    return False
            else:
                print(f"โ ะัะธะฑะบะฐ ะฟะพะดะบะปััะตะฝะธั ะบ Ollama: HTTP {response.status_code}")
                return False

        except httpx.ConnectError as e:
            print(f"โ ะะต ัะดะฐะปะพัั ะฟะพะดะบะปััะธัััั ะบ Ollama ัะตัะฒะตัั: {self.base_url}")
            print(f"   ะฃะฑะตะดะธัะตัั, ััะพ Ollama ะทะฐะฟััะตะฝ")
            return False
        except Exception as e:
            print(f"โ ะะตะพะถะธะดะฐะฝะฝะฐั ะพัะธะฑะบะฐ: {e}")
            return False

    def estimate_tokens(self, text: str) -> int:
        """
        ะัะตะฝะบะฐ ะบะพะปะธัะตััะฒะฐ ัะพะบะตะฝะพะฒ (ัะฟัะพัะตะฝะฝะฐั ะฒะตััะธั ะธะท ะฟัะพะตะบัะฐ)

        ะัะฟะพะปัะทัะตััั ะดะปั ัะฐััะตัะฐ num_ctx ะธ num_predict
        """
        if not text:
            return 0

        total_chars = len(text)

        # ะะพะดััะธััะฒะฐะตะผ ัะธะผะฒะพะปั ัะฐะทะฝัั ัะธะฟะพะฒ
        cyrillic_count = sum(1 for c in text if '\u0400' <= c <= '\u04FF')
        cjk_count = sum(1 for c in text if '\u4E00' <= c <= '\u9FFF')

        cyrillic_ratio = cyrillic_count / total_chars if total_chars > 0 else 0
        cjk_ratio = cjk_count / total_chars if total_chars > 0 else 0

        # ะัะฑะธัะฐะตะผ ะบะพัััะธัะธะตะฝั ะฝะฐ ะพัะฝะพะฒะต ัะทัะบะฐ
        if cjk_ratio > 0.3:
            chars_per_token = 1.5
            language = "ะบะธัะฐะนัะบะธะน"
        elif cyrillic_ratio > 0.3:
            chars_per_token = 2.5
            language = "ััััะบะธะน"
        else:
            chars_per_token = 4.0
            language = "ะฐะฝะณะปะธะนัะบะธะน"

        estimated_tokens = int(total_chars / chars_per_token)

        print(f"   ๐ ะัะตะฝะบะฐ ัะพะบะตะฝะพะฒ: {total_chars:,} ัะธะผะฒะพะปะพะฒ, ัะทัะบ: {language}")
        print(f"   ๐ ~{estimated_tokens:,} ัะพะบะตะฝะพะฒ ({chars_per_token} ัะธะผะฒ/ัะพะบะตะฝ)")

        return estimated_tokens

    def test_generation(self, system_prompt: str, user_prompt: str,
                       temperature: float = 0.5, max_output_tokens: int = 8192) -> dict:
        """
        ะขะตััะธัะพะฒะฐะฝะธะต ะณะตะฝะตัะฐัะธะธ ะบะพะฝัะตะฝัะฐ

        Args:
            system_prompt: ะกะธััะตะผะฝัะน ะฟัะพะผะฟั
            user_prompt: ะะพะปัะทะพะฒะฐัะตะปััะบะธะน ะฟัะพะผะฟั
            temperature: ะขะตะผะฟะตัะฐัััะฐ (0.0-1.0)
            max_output_tokens: ะะฐะบัะธะผัะผ ัะพะบะตะฝะพะฒ ะฝะฐ ะฒััะพะด

        Returns:
            Dict ั ัะตะทัะปััะฐัะพะผ
        """
        print("\n" + "="*80)
        print("๐ ะะะะฃะกะ ะขะะกะขะ ะะะะะะะฆะะ")
        print("="*80)

        # ะะฑัะตะดะธะฝัะตะผ ะฟัะพะผะฟัั
        full_prompt = f"{system_prompt}\n\n{user_prompt}"

        # ะัะตะฝะบะฐ ัะฐะทะผะตัะฐ ะฟัะพะผะฟัะฐ
        print("\n๐ ะะฝะฐะปะธะท ะฟัะพะผะฟัะฐ:")
        prompt_tokens = self.estimate_tokens(full_prompt)

        # ะะฐััะตั ะฟะฐัะฐะผะตััะพะฒ (ะบะฐะบ ะฒ ะฟัะพะตะบัะต)
        num_ctx = int(prompt_tokens * 1.2)  # ะัะพะผะฟั + 20% ะฑััะตั
        min_context_size = 2048

        if num_ctx < min_context_size:
            num_ctx = min_context_size

        num_predict = min(num_ctx * 2, max_output_tokens)

        print(f"\nโ๏ธ  ะะฐัะฐะผะตััั ะทะฐะฟัะพัะฐ:")
        print(f"   ๐ num_ctx (ะฟัะพะผะฟั + 20%): {num_ctx:,} ัะพะบะตะฝะพะฒ")
        print(f"   ๐ง num_predict: {num_predict:,} ัะพะบะตะฝะพะฒ")
        print(f"   ๐ก๏ธ  temperature: {temperature}")
        print(f"   ๐ฆ model: {self.model_name}")

        # ะะพะดะณะพัะพะฒะบะฐ ะทะฐะฟัะพัะฐ
        request_json = {
            'model': self.model_name,
            'prompt': full_prompt,
            'stream': False,
            'options': {
                'temperature': temperature,
                'num_predict': num_predict,
                'num_ctx': num_ctx,
                'num_keep': num_ctx
            }
        }

        print(f"\nโณ ะัะฟัะฐะฒะบะฐ ะทะฐะฟัะพัะฐ ะบ Ollama...")
        start_time = time.time()

        try:
            response = httpx.post(
                f"{self.base_url}/api/generate",
                json=request_json,
                timeout=1200.0  # 20 ะผะธะฝัั ะบะฐะบ ะฒ ะฟัะพะตะบัะต
            )

            elapsed_time = time.time() - start_time

            if response.status_code == 200:
                data = response.json()
                content = data.get('response', '')

                # ะกัะฐัะธััะธะบะฐ
                prompt_eval_count = data.get('prompt_eval_count', 0)
                eval_count = data.get('eval_count', 0)
                total_tokens = prompt_eval_count + eval_count

                print(f"\nโ ะฃะกะะะจะะซะ ะะขะะะข")
                print(f"   โฑ๏ธ  ะัะตะผั ะฒัะฟะพะปะฝะตะฝะธั: {elapsed_time:.2f} ัะตะบ")
                print(f"   ๐ ะขะพะบะตะฝั ะฟัะพะผะฟัะฐ: {prompt_eval_count:,}")
                print(f"   ๐ ะขะพะบะตะฝั ะพัะฒะตัะฐ: {eval_count:,}")
                print(f"   ๐ ะัะตะณะพ ัะพะบะตะฝะพะฒ: {total_tokens:,}")
                print(f"   ๐ ะะปะธะฝะฐ ะพัะฒะตัะฐ: {len(content):,} ัะธะผะฒะพะปะพะฒ")
                print(f"   โ ะะฐะฒะตััะตะฝะพ: {data.get('done', False)}")

                # ะัะพะฒะตัะบะฐ ะฝะฐ ะพะฑัะตะทะบั
                if not data.get('done'):
                    print(f"\nโ๏ธ  ะะะะะะะะ: ะัะฒะตั ะฑัะป ะพะฑัะตะทะฐะฝ (done=False)")

                return {
                    'success': True,
                    'content': content,
                    'stats': {
                        'prompt_tokens': prompt_eval_count,
                        'completion_tokens': eval_count,
                        'total_tokens': total_tokens,
                        'elapsed_time': elapsed_time,
                        'chars': len(content),
                        'done': data.get('done', False)
                    }
                }

            else:
                error_text = response.text
                print(f"\nโ ะะจะะะะ: HTTP {response.status_code}")
                print(f"   {error_text[:500]}")

                return {
                    'success': False,
                    'error': f'HTTP {response.status_code}: {error_text[:200]}'
                }

        except httpx.TimeoutException:
            elapsed_time = time.time() - start_time
            print(f"\nโ ะขะะะะะฃะข: ะะฐะฟัะพั ะฟัะตะฒััะธะป 20 ะผะธะฝัั")
            print(f"   ะัะพัะปะพ ะฒัะตะผะตะฝะธ: {elapsed_time:.2f} ัะตะบ")

            return {
                'success': False,
                'error': 'ะขะฐะนะผะฐัั ะฟัะธ ะพะฑัะฐัะตะฝะธะธ ะบ Ollama (>20 ะผะธะฝัั)'
            }

        except Exception as e:
            elapsed_time = time.time() - start_time
            print(f"\nโ ะะกะะะฎะงะะะะ: {type(e).__name__}")
            print(f"   {str(e)}")
            print(f"   ะัะพัะปะพ ะฒัะตะผะตะฝะธ: {elapsed_time:.2f} ัะตะบ")

            return {
                'success': False,
                'error': f'{type(e).__name__}: {str(e)}'
            }


def main():
    """ะะปะฐะฒะฝะฐั ััะฝะบัะธั ัะตััะฐ"""
    print("\n" + "๐งช"*40)
    print("ะขะะกะขะะะะะะะะ OLLAMA: gemini-3-pro-preview")
    print("๐งช"*40)
    print(f"ะัะตะผั ะทะฐะฟััะบะฐ: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")

    # ะกะพะทะดะฐะตะผ ัะตััะตั
    tester = OllamaGemini3ProTester()

    # ะัะพะฒะตััะตะผ ะดะพัััะฟะฝะพััั ะผะพะดะตะปะธ
    if not tester.check_model_availability():
        print("\nโ ะขะตัั ะฟัะตัะฒะฐะฝ: ะผะพะดะตะปั ะฝะตะดะพัััะฟะฝะฐ")
        return

    # ะขะตััะพะฒัะต ะฟัะพะผะฟัั
    system_prompt = """ะขั โ ะฟัะพัะตััะธะพะฝะฐะปัะฝัะน ะฟะตัะตะฒะพะดัะธะบ ั ะบะธัะฐะนัะบะพะณะพ ะฝะฐ ััััะบะธะน ัะทัะบ.
ะขะฒะพั ะทะฐะดะฐัะฐ โ ะฟะตัะตะฒะพะดะธัั ัะตะบัั ะผะฐะบัะธะผะฐะปัะฝะพ ัะพัะฝะพ, ัะพััะฐะฝัั ััะธะปั ะธ ัะผััะป ะพัะธะณะธะฝะฐะปะฐ.
ะะต ะดะพะฑะฐะฒะปัะน ะฝะธะบะฐะบะธั ะบะพะผะผะตะฝัะฐัะธะตะฒ, ะฟะตัะตะฒะพะดะธ ัะพะปัะบะพ ัะตะบัั."""

    user_prompt = """ะะตัะตะฒะตะดะธ ัะปะตะดัััะธะน ัะตะบัั ั ะบะธัะฐะนัะบะพะณะพ ะฝะฐ ััััะบะธะน:

ไฟฎ็ผไน่ทฏๆผซ้ฟ่่ฐ่พ๏ผไฝไปไปๆชๆพๅผ่ฟใๆฏไธๆฌก็ช็ด๏ผ้ฝ่ฎฉไปๅๅพๆดๅๅผบๅคงใ
ไปๅคฉ๏ผไป็ปไบ่พพๅฐไบ้ไธนๆ๏ผ่ฟๆฏไธไธช้่ฆ็้็จ็ขใ
ไป็ฅ้๏ผๅๆน่ฟๆๆดๅค็ๆๆๅจ็ญๅพ็ไป๏ผไฝไปๅทฒ็ปๅๅคๅฅฝไบใ"""

    # ะะฐะฟััะบะฐะตะผ ัะตัั
    result = tester.test_generation(
        system_prompt=system_prompt,
        user_prompt=user_prompt,
        temperature=0.5,
        max_output_tokens=8192
    )

    # ะัะฒะพะดะธะผ ัะตะทัะปััะฐั
    if result['success']:
        print("\n" + "="*80)
        print("๐ ะกะะะะะะะะะะะะะซะ ะะะะขะะะข")
        print("="*80)
        print(result['content'])
        print("\n" + "="*80)
        print("โ ะขะะกะข ะะะะะะจะะ ะฃะกะะะจะะ")
        print("="*80)

        # ะกัะฐัะธััะธะบะฐ
        stats = result['stats']
        print(f"\n๐ ะัะพะณะพะฒะฐั ััะฐัะธััะธะบะฐ:")
        print(f"   โฑ๏ธ  ะัะตะผั: {stats['elapsed_time']:.2f} ัะตะบ")
        print(f"   ๐ ะขะพะบะตะฝั: {stats['total_tokens']:,} ({stats['prompt_tokens']:,} ะฟัะพะผะฟั + {stats['completion_tokens']:,} ะพัะฒะตั)")
        print(f"   ๐ ะกะธะผะฒะพะปะพะฒ: {stats['chars']:,}")
        print(f"   ๐ ะกะบะพัะพััั: {stats['completion_tokens'] / stats['elapsed_time']:.2f} ัะพะบะตะฝะพะฒ/ัะตะบ")

    else:
        print("\n" + "="*80)
        print("โ ะขะะกะข ะะะะะะจะะ ะก ะะจะะะะะ")
        print("="*80)
        print(f"ะัะธะฑะบะฐ: {result['error']}")

    print(f"\nะัะตะผั ะทะฐะฒะตััะตะฝะธั: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("\n" + "๐งช"*40 + "\n")


if __name__ == "__main__":
    main()
