#!/usr/bin/env python3
"""
–î–µ—Ç–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –∫–∏—Ç–∞–π—Å–∫–æ–≥–æ EPUB —Ñ–∞–π–ª–∞ —Å –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ–º —Ñ–∞–∫—Ç–∏—á–µ—Å–∫–æ–≥–æ —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ –≥–ª–∞–≤
"""

import sys
import os
import re
import json
import zipfile
import xml.etree.ElementTree as ET
from collections import Counter
from pathlib import Path
from typing import Dict, List, Optional
import logging

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(level=logging.WARNING)
logger = logging.getLogger(__name__)

class ChineseEPUBParser:
    """–ü–∞—Ä—Å–µ—Ä –¥–ª—è –∫–∏—Ç–∞–π—Å–∫–∏—Ö EPUB —Ñ–∞–π–ª–æ–≤"""
    
    def __init__(self, epub_path: str, max_chapters: Optional[int] = None):
        self.epub_path = epub_path
        self.max_chapters = max_chapters
        self.epub_data = {}
        self.chapters = []
        
        if epub_path:
            self.load_epub(epub_path)
    
    def load_epub(self, epub_path: str) -> bool:
        """–ó–∞–≥—Ä—É–∑–∏—Ç—å –∏ —Ä–∞–∑–æ–±—Ä–∞—Ç—å EPUB —Ñ–∞–π–ª"""
        try:
            if not os.path.exists(epub_path):
                logger.error(f"EPUB —Ñ–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {epub_path}")
                return False
            
            with zipfile.ZipFile(epub_path, 'r') as epub_zip:
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —ç—Ç–æ –¥–µ–π—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ EPUB
                if 'META-INF/container.xml' not in epub_zip.namelist():
                    logger.error("–§–∞–π–ª –Ω–µ —Å–æ–¥–µ—Ä–∂–∏—Ç META-INF/container.xml")
                    return False
                
                # –ß–∏—Ç–∞–µ–º container.xml –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –ø—É—Ç–∏ –∫ OPF —Ñ–∞–π–ª—É
                container_xml = epub_zip.read('META-INF/container.xml')
                container_tree = ET.fromstring(container_xml)
                
                # –ù–∞—Ö–æ–¥–∏–º OPF —Ñ–∞–π–ª
                rootfile = container_tree.find('.//{*}rootfile')
                if rootfile is None:
                    logger.error("–ù–µ –Ω–∞–π–¥–µ–Ω rootfile –≤ container.xml")
                    return False
                
                opf_path = rootfile.get('full-path')
                if not opf_path:
                    logger.error("–ü—É—Ç—å –∫ OPF —Ñ–∞–π–ª—É –Ω–µ –Ω–∞–π–¥–µ–Ω")
                    return False
                
                # –ß–∏—Ç–∞–µ–º OPF —Ñ–∞–π–ª
                opf_content = epub_zip.read(opf_path)
                opf_tree = ET.fromstring(opf_content)
                
                # –ò–∑–≤–ª–µ–∫–∞–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
                self._extract_metadata(opf_tree)
                
                # –ò–∑–≤–ª–µ–∫–∞–µ–º –º–∞–Ω–∏—Ñ–µ—Å—Ç (—Å–ø–∏—Å–æ–∫ —Ñ–∞–π–ª–æ–≤)
                manifest = self._extract_manifest(opf_tree, epub_zip, opf_path)
                
                # –ò–∑–≤–ª–µ–∫–∞–µ–º spine (–ø–æ—Ä—è–¥–æ–∫ —á—Ç–µ–Ω–∏—è)
                spine = self._extract_spine(opf_tree)
                
                if not spine:
                    # –ï—Å–ª–∏ spine –ø—É—Å—Ç, –∏—â–µ–º HTML —Ñ–∞–π–ª—ã –≤ –º–∞–Ω–∏—Ñ–µ—Å—Ç–µ
                    html_items = [item_id for item_id, item in manifest.items() 
                                if item.get('media_type') in ['application/xhtml+xml', 'text/html']]
                    spine = html_items
                
                # –ò–∑–≤–ª–µ–∫–∞–µ–º –≥–ª–∞–≤—ã
                self._extract_chapters(manifest, spine, epub_zip)
                
                return len(self.chapters) > 0
                
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ EPUB: {e}")
            return False
    
    def _extract_metadata(self, opf_tree: ET.Element):
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö –∏–∑ OPF"""
        metadata = opf_tree.find('.//{*}metadata')
        if metadata is None:
            return
        
        self.epub_data['metadata'] = {}
        
        # –ù–∞–∑–≤–∞–Ω–∏–µ
        title_elem = metadata.find('.//{*}title')
        if title_elem is not None:
            self.epub_data['metadata']['title'] = title_elem.text or ''
        
        # –ê–≤—Ç–æ—Ä
        creator_elem = metadata.find('.//{*}creator')
        if creator_elem is not None:
            self.epub_data['metadata']['author'] = creator_elem.text or ''
        
        # –û–ø–∏—Å–∞–Ω–∏–µ
        description_elem = metadata.find('.//{*}description')
        if description_elem is not None:
            self.epub_data['metadata']['description'] = description_elem.text or ''
        
        # –Ø–∑—ã–∫
        language_elem = metadata.find('.//{*}language')
        if language_elem is not None:
            self.epub_data['metadata']['language'] = language_elem.text or 'en'
    
    def _extract_manifest(self, opf_tree: ET.Element, epub_zip: zipfile.ZipFile, opf_path: str) -> Dict:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –º–∞–Ω–∏—Ñ–µ—Å—Ç–∞ (—Å–ø–∏—Å–∫–∞ —Ñ–∞–π–ª–æ–≤)"""
        manifest = {}
        manifest_elem = opf_tree.find('.//{*}manifest')
        
        if manifest_elem is None:
            return manifest
        
        opf_dir = os.path.dirname(opf_path)
        
        for item in manifest_elem.findall('.//{*}item'):
            item_id = item.get('id')
            href = item.get('href')
            media_type = item.get('media-type')
            
            if item_id and href:
                # –ü–æ–ª–Ω—ã–π –ø—É—Ç—å –∫ —Ñ–∞–π–ª—É
                full_path = os.path.join(opf_dir, href) if opf_dir else href
                manifest[item_id] = {
                    'href': href,
                    'full_path': full_path,
                    'media_type': media_type
                }
        
        return manifest
    
    def _extract_spine(self, opf_tree: ET.Element) -> List[str]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ spine (–ø–æ—Ä—è–¥–∫–∞ —á—Ç–µ–Ω–∏—è)"""
        spine = []
        spine_elem = opf_tree.find('.//{*}spine')
        
        if spine_elem is None:
            return spine
        
        for itemref in spine_elem.findall('.//{*}itemref'):
            idref = itemref.get('idref')
            if idref:
                spine.append(idref)
        
        return spine
    
    def _extract_chapters(self, manifest: Dict, spine: List[str], epub_zip: zipfile.ZipFile):
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –≥–ª–∞–≤ –∏–∑ EPUB"""
        self.chapters = []
        chapter_number = 1
        extracted_count = 0
        
        # –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ —Ñ–∞–π–ª—ã –∏–∑ –∞—Ä—Ö–∏–≤–∞ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
        all_files = epub_zip.namelist()
        print(f"–í—Å–µ–≥–æ —Ñ–∞–π–ª–æ–≤ –≤ EPUB: {len(all_files)}")
        
        # –ò—â–µ–º HTML/XHTML —Ñ–∞–π–ª—ã, –∫–æ—Ç–æ—Ä—ã–µ –º–æ–≥—É—Ç —Å–æ–¥–µ—Ä–∂–∞—Ç—å –≥–ª–∞–≤—ã
        html_files = [f for f in all_files if f.endswith(('.html', '.xhtml', '.htm'))]
        print(f"HTML —Ñ–∞–π–ª–æ–≤ –Ω–∞–π–¥–µ–Ω–æ: {len(html_files)}")
        
        # –ü—ã—Ç–∞–µ–º—Å—è –∏–∑–≤–ª–µ—á—å –≥–ª–∞–≤—ã –∏–∑ spine
        for item_id in spine:
            if self.max_chapters and extracted_count >= self.max_chapters:
                break
            
            if item_id not in manifest:
                continue
            
            item = manifest[item_id]
            media_type = item.get('media_type', '')
            
            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ç–æ–ª—å–∫–æ HTML/XHTML —Ñ–∞–π–ª—ã
            if media_type not in ['application/xhtml+xml', 'text/html']:
                continue
            
            try:
                full_path = item['full_path']
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —Ñ–∞–π–ª —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –≤ –∞—Ä—Ö–∏–≤–µ
                if full_path not in epub_zip.namelist():
                    continue
                
                # –ß–∏—Ç–∞–µ–º —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ —Ñ–∞–π–ª–∞
                try:
                    content = epub_zip.read(full_path).decode('utf-8')
                except UnicodeDecodeError:
                    try:
                        content = epub_zip.read(full_path).decode('gb2312')
                    except:
                        try:
                            content = epub_zip.read(full_path).decode('gbk')
                        except:
                            continue
                
                # –ü–∞—Ä—Å–∏–º HTML –∫–æ–Ω—Ç–µ–Ω—Ç
                chapter_info = self._parse_html_content(content, chapter_number, full_path)
                if chapter_info and len(chapter_info['content'].strip()) > 50:  # –¢–æ–ª—å–∫–æ –≥–ª–∞–≤—ã —Å –¥–æ—Å—Ç–∞—Ç–æ—á–Ω—ã–º —Å–æ–¥–µ—Ä–∂–∏–º—ã–º
                    self.chapters.append(chapter_info)
                    chapter_number += 1
                    extracted_count += 1
                    
            except Exception as e:
                continue
    
    def _parse_html_content(self, html_content: str, chapter_number: int, file_path: str) -> Optional[Dict]:
        """–ü–∞—Ä—Å–∏–Ω–≥ HTML –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –≥–ª–∞–≤—ã"""
        try:
            # –ü—Ä–æ—Å—Ç–æ–π –ø–∞—Ä—Å–∏–Ω–≥ HTML —Å –ø–æ–º–æ—â—å—é —Ä–µ–≥—É–ª—è—Ä–Ω—ã—Ö –≤—ã—Ä–∞–∂–µ–Ω–∏–π
            html_content = re.sub(r'<\?xml[^>]*\?>', '', html_content)
            html_content = re.sub(r'<!DOCTYPE[^>]*>', '', html_content)
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫ –∏–∑ title –∏–ª–∏ –∏–∑ –ø–µ—Ä–≤–æ–≥–æ h1/h2/h3
            title_match = re.search(r'<title[^>]*>(.*?)</title>', html_content, re.IGNORECASE | re.DOTALL)
            title = title_match.group(1).strip() if title_match else ""
            
            # –ï—Å–ª–∏ –∑–∞–≥–æ–ª–æ–≤–æ–∫ –ø—É—Å—Ç–æ–π –∏–ª–∏ —Å–ª–∏—à–∫–æ–º –æ–±—â–∏–π, –∏—â–µ–º –≤ h1-h3
            if not title or title in ['Cover', 'Table Of Contents']:
                h_match = re.search(r'<h[1-3][^>]*>(.*?)</h[1-3]>', html_content, re.IGNORECASE | re.DOTALL)
                if h_match:
                    title = self._clean_html_tags(h_match.group(1).strip())
            
            if not title:
                title = f"Á¨¨{chapter_number}Á´†"
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ body
            body_match = re.search(r'<body[^>]*>(.*?)</body>', html_content, re.IGNORECASE | re.DOTALL)
            if not body_match:
                # –ï—Å–ª–∏ –Ω–µ—Ç body, –±–µ—Ä–µ–º –≤–µ—Å—å –∫–æ–Ω—Ç–µ–Ω—Ç
                body_content = html_content
            else:
                body_content = body_match.group(1)
            
            # –û—á–∏—â–∞–µ–º HTML —Ç–µ–≥–∏, –æ—Å—Ç–∞–≤–ª—è—è —Ç–æ–ª—å–∫–æ —Ç–µ–∫—Å—Ç
            content = self._clean_html_content(body_content)
            
            if not content.strip():
                return None
            
            return {
                'number': chapter_number,
                'title': title,
                'content': content,
                'chapter_id': f"chapter_{chapter_number}",
                'word_count': len(content),  # –î–ª—è –∫–∏—Ç–∞–π—Å–∫–æ–≥–æ —Ç–µ–∫—Å—Ç–∞ —Å—á–∏—Ç–∞–µ–º —Å–∏–º–≤–æ–ª—ã
                'file_path': file_path
            }
            
        except Exception as e:
            return None
    
    def _clean_html_tags(self, text: str) -> str:
        """–û—á–∏—Å—Ç–∫–∞ HTML —Ç–µ–≥–æ–≤ –∏–∑ —Ç–µ–∫—Å—Ç–∞"""
        # –£–¥–∞–ª—è–µ–º –≤—Å–µ HTML —Ç–µ–≥–∏
        text = re.sub(r'<[^>]+>', '', text)
        # –î–µ–∫–æ–¥–∏—Ä—É–µ–º HTML —Å—É—â–Ω–æ—Å—Ç–∏
        text = text.replace('&nbsp;', ' ')
        text = text.replace('&amp;', '&')
        text = text.replace('&lt;', '<')
        text = text.replace('&gt;', '>')
        text = text.replace('&quot;', '"')
        text = text.replace('&#39;', "'")
        return text.strip()
    
    def _clean_html_content(self, html_content: str) -> str:
        """–û—á–∏—Å—Ç–∫–∞ HTML –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –æ—Ç —Ç–µ–≥–æ–≤"""
        # –£–¥–∞–ª—è–µ–º —Å–∫—Ä–∏–ø—Ç—ã –∏ —Å—Ç–∏–ª–∏
        html_content = re.sub(r'<script[^>]*>.*?</script>', '', html_content, flags=re.IGNORECASE | re.DOTALL)
        html_content = re.sub(r'<style[^>]*>.*?</style>', '', html_content, flags=re.IGNORECASE | re.DOTALL)
        
        # –ó–∞–º–µ–Ω—è–µ–º –Ω–µ–∫–æ—Ç–æ—Ä—ã–µ —Ç–µ–≥–∏ –Ω–∞ –ø–µ—Ä–µ–Ω–æ—Å—ã —Å—Ç—Ä–æ–∫
        html_content = re.sub(r'</?(?:p|div|br|h[1-6])[^>]*>', '\n', html_content, flags=re.IGNORECASE)
        
        # –£–¥–∞–ª—è–µ–º –≤—Å–µ –æ—Å—Ç–∞–ª—å–Ω—ã–µ HTML —Ç–µ–≥–∏
        html_content = re.sub(r'<[^>]+>', '', html_content)
        
        # –î–µ–∫–æ–¥–∏—Ä—É–µ–º HTML —Å—É—â–Ω–æ—Å—Ç–∏
        html_content = html_content.replace('&nbsp;', ' ')
        html_content = html_content.replace('&amp;', '&')
        html_content = html_content.replace('&lt;', '<')
        html_content = html_content.replace('&gt;', '>')
        html_content = html_content.replace('&quot;', '"')
        html_content = html_content.replace('&#39;', "'")
        
        # –û—á–∏—â–∞–µ–º –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã –∏ –ø–µ—Ä–µ–Ω–æ—Å—ã —Å—Ç—Ä–æ–∫
        lines = [line.strip() for line in html_content.split('\n')]
        lines = [line for line in lines if line and not line.startswith('www.') and not line.startswith('Txt,Epub,Mobi')]
        
        return '\n\n'.join(lines)
    
    def get_book_info(self) -> Dict:
        """–ü–æ–ª—É—á–∏—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –∫–Ω–∏–≥–µ"""
        metadata = self.epub_data.get('metadata', {})
        
        return {
            'title': metadata.get('title', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è –∫–Ω–∏–≥–∞'),
            'author': metadata.get('author', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π –∞–≤—Ç–æ—Ä'),
            'description': metadata.get('description', ''),
            'language': metadata.get('language', 'zh-CN'),
            'total_chapters': len(self.chapters)
        }


def analyze_chinese_names(text: str) -> dict:
    """–ê–Ω–∞–ª–∏–∑ –∫–∏—Ç–∞–π—Å–∫–∏—Ö –∏–º–µ–Ω –∏ —Ç–µ—Ä–º–∏–Ω–æ–≤"""
    
    # –ò—â–µ–º –∫–∏—Ç–∞–π—Å–∫–∏–µ –∏–º–µ–Ω–∞ (–æ–±—ã—á–Ω–æ 2-3 —Å–∏–º–≤–æ–ª–∞)
    chinese_names = re.findall(r'[\u4e00-\u9fff]{2,3}(?=[\u4e00-\u9fff\sÔºå„ÄÇÔºÅÔºü]|$)', text)
    name_counter = Counter(chinese_names)
    
    # –ò—Å–∫–ª—é—á–∞–µ–º –æ–±—â–∏–µ —Å–ª–æ–≤–∞ –∏ —Ñ—Ä–∞–∑—ã
    common_words = {'Ëøô‰∏™', 'ÈÇ£‰∏™', '‰ªÄ‰πà', 'ÊÄé‰πà', '‰∏∫‰ªÄ‰πà', '‰∏çÊòØ', 'Â∞±ÊòØ', 'ÂèØÊòØ', '‰ΩÜÊòØ', 'ÁÑ∂Âêé', 'Âõ†‰∏∫', 'ÊâÄ‰ª•', 'Â¶ÇÊûú', 'ËôΩÁÑ∂', 'ËôΩÁÑ∂', 'ËôΩË™™', 'Êó†ËÆ∫', '‰∏çÁÆ°', '‰πãÂêé', '‰ª•Âêé', '‰πãÂâç', '‰ª•Ââç', 'Áé∞Âú®', 'Êú™Êù•', 'Êó∂ÂÄô', 'Âú∞Êñπ', '‰∏úË•ø', '‰∫ãÊÉÖ', 'ÈóÆÈ¢ò', 'ÂäûÊ≥ï', 'ÊñπÊ≥ï', 'Ê†∑Â≠ê', 'Ê†∑Â≠ê'}
    
    # –§–∏–ª—å—Ç—Ä—É–µ–º –∏–º–µ–Ω–∞ (—É–±–∏—Ä–∞–µ–º —Å–ª–∏—à–∫–æ–º –æ–±—â–∏–µ —Å–ª–æ–≤–∞)
    potential_names = {name: count for name, count in name_counter.most_common(20) 
                      if count > 1 and name not in common_words and len(name) >= 2}
    
    # –ò—â–µ–º —Ç–∏—Ç—É–ª—ã –∏ –∑–≤–∞–Ω–∏—è
    titles = re.findall(r'[\u4e00-\u9fff]*(?:Â∏à|Â∏àÂÇÖ|Â∏àÂÖÑ|Â∏àÂßê|Â∏àÂèî|Â∏à‰ºØ|Â∏àÁ•ñ|ÈïøËÄÅ|ÂÆó‰∏ª|ÊéåÈó®|ÂºüÂ≠ê|Èó®‰∫∫)[\u4e00-\u9fff]*', text)
    title_counter = Counter(titles)
    
    # –ò—â–µ–º –º–µ—Å—Ç–∞ –∏ –ª–æ–∫–∞—Ü–∏–∏
    locations = re.findall(r'[\u4e00-\u9fff]*(?:Â±±|Â≥∞|Ë∞∑|Ê¥û|Â∫ú|ÂÆó|Èó®|Âüé|ÂõΩ|Âüü|Áïå|Âú∞|Â≤õ)[\u4e00-\u9fff]*', text)
    location_counter = Counter(locations)
    
    return {
        'characters': dict(potential_names),
        'titles': dict(title_counter.most_common(10)),
        'locations': dict(location_counter.most_common(10))
    }


def analyze_chinese_text_style(content: str) -> dict:
    """–ê–Ω–∞–ª–∏–∑ —Å—Ç–∏–ª—è –∫–∏—Ç–∞–π—Å–∫–æ–≥–æ —Ç–µ–∫—Å—Ç–∞"""
    
    # –ü–æ–¥—Å—á–µ—Ç —Å–∏–º–≤–æ–ª–æ–≤ (–¥–ª—è –∫–∏—Ç–∞–π—Å–∫–æ–≥–æ —Ç–µ–∫—Å—Ç–∞)
    char_count = len(content)
    chinese_chars = len(re.findall(r'[\u4e00-\u9fff]', content))
    
    # –ê–Ω–∞–ª–∏–∑ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π (–ø–æ –∑–Ω–∞–∫–∞–º –ø—Ä–µ–ø–∏–Ω–∞–Ω–∏—è)
    sentences = re.split(r'[„ÄÇÔºÅÔºü]', content)
    sentences = [s.strip() for s in sentences if s.strip()]
    avg_sentence_length = sum(len(s) for s in sentences) / len(sentences) if sentences else 0
    
    # –¢–∏–ø—ã –∑–Ω–∞–∫–æ–≤ –ø—Ä–µ–ø–∏–Ω–∞–Ω–∏—è
    punctuation_analysis = {
        'periods': content.count('„ÄÇ'),
        'exclamation_marks': content.count('ÔºÅ'),
        'question_marks': content.count('Ôºü'),
        'commas': content.count('Ôºå'),
        'quotes': content.count('"') + content.count('"') + content.count('„Äå') + content.count('„Äç')
    }
    
    # –î–∏–∞–ª–æ–≥–∏ (—Å—Ç—Ä–æ–∫–∏ –≤ –∫–∞–≤—ã—á–∫–∞—Ö)
    dialogue_lines = len(re.findall(r'[""„Äå„Äç„Äé„Äè].+?[""„Äå„Äç„Äé„Äè]', content))
    
    return {
        'character_count': char_count,
        'chinese_character_count': chinese_chars,
        'chinese_ratio': chinese_chars / char_count if char_count > 0 else 0,
        'sentence_count': len(sentences),
        'avg_sentence_length': round(avg_sentence_length, 1),
        'punctuation_analysis': punctuation_analysis,
        'dialogue_lines': dialogue_lines,
        'writing_style': {
            'dialogue_ratio': dialogue_lines / max(len(sentences), 1) * 100,
            'complexity_score': avg_sentence_length / 20  # –ê–¥–∞–ø—Ç–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –º–µ—Ç—Ä–∏–∫–∞ –¥–ª—è –∫–∏—Ç–∞–π—Å–∫–æ–≥–æ
        }
    }


def analyze_chinese_epub(epub_path: str, max_chapters: int = 10):
    """–ê–Ω–∞–ª–∏–∑ –∫–∏—Ç–∞–π—Å–∫–æ–≥–æ EPUB —Ñ–∞–π–ª–∞"""
    
    print(f"–ê–Ω–∞–ª–∏–∑ –∫–∏—Ç–∞–π—Å–∫–æ–≥–æ EPUB —Ñ–∞–π–ª–∞: {epub_path}")
    print("=" * 80)
    
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø–∞—Ä—Å–µ—Ä–∞ —Å –ª–∏–º–∏—Ç–æ–º –≥–ª–∞–≤
    parser = ChineseEPUBParser(epub_path=epub_path, max_chapters=max_chapters)
    
    if not parser.chapters:
        print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å EPUB —Ñ–∞–π–ª –∏–ª–∏ –∏–∑–≤–ª–µ—á—å –≥–ª–∞–≤—ã")
        return
    
    # –ü–æ–ª—É—á–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –∫–Ω–∏–≥–µ
    book_info = parser.get_book_info()
    
    print("üìö –ò–ù–§–û–†–ú–ê–¶–ò–Ø –û –ü–†–û–ò–ó–í–ï–î–ï–ù–ò–ò")
    print("-" * 50)
    print(f"–ù–∞–∑–≤–∞–Ω–∏–µ: {book_info['title']} (ÊàëÊ¨≤Â∞ÅÂ§© - –Ø —Ö–æ—á—É –∑–∞–ø–µ—á–∞—Ç–∞—Ç—å –Ω–µ–±–µ—Å–∞)")
    print(f"–ê–≤—Ç–æ—Ä: {book_info['author']} (–≤–µ—Ä–æ—è—Ç–Ω–æ, ËÄ≥Ê†π - Ergen)")
    print(f"–ñ–∞–Ω—Ä: –°—è–Ω—å—Å—è (‰ªô‰æ†) - –∫—É–ª—å—Ç–∏–≤–∞—Ü–∏—è/–±–æ–µ–≤—ã–µ –∏—Å–∫—É—Å—Å—Ç–≤–∞")
    print(f"–Ø–∑—ã–∫: {book_info['language']} (–∫–∏—Ç–∞–π—Å–∫–∏–π)")
    print(f"–í—Å–µ–≥–æ –≥–ª–∞–≤ –≤ —Ñ–∞–π–ª–µ: {book_info['total_chapters']}")
    print(f"–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º—ã—Ö –≥–ª–∞–≤: {len(parser.chapters)}")
    print()
    
    # –ê–Ω–∞–ª–∏–∑ –≥–ª–∞–≤
    print("üìñ –ü–ï–†–í–´–ï 10 –ì–õ–ê–í")
    print("-" * 50)
    
    all_content = ""
    chapter_analyses = []
    
    for i, chapter in enumerate(parser.chapters, 1):
        print(f"\n{i}. {chapter['title']}")
        print(f"   –°–∏–º–≤–æ–ª–æ–≤: {chapter['word_count']}")
        print(f"   –§–∞–π–ª: {chapter['file_path']}")
        
        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –Ω–∞—á–∞–ª–æ –≥–ª–∞–≤—ã (–ø–µ—Ä–≤—ã–µ 200 —Å–∏–º–≤–æ–ª–æ–≤)
        content_preview = chapter['content'][:200].replace('\n', ' ')
        print(f"   –ù–∞—á–∞–ª–æ: {content_preview}{'...' if len(chapter['content']) > 200 else ''}")
        
        # –î–æ–±–∞–≤–ª—è–µ–º –∫ –æ–±—â–µ–º—É –∫–æ–Ω—Ç–µ–Ω—Ç—É –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
        all_content += "\n\n" + chapter['content']
        
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Å—Ç–∏–ª—å –∫–∞–∂–¥–æ–π –≥–ª–∞–≤—ã
        chapter_analysis = analyze_chinese_text_style(chapter['content'])
        chapter_analyses.append(chapter_analysis)
    
    print("\n")
    
    # –û–±—â–∏–π –∞–Ω–∞–ª–∏–∑ —Å—Ç–∏–ª—è
    print("üé® –ê–ù–ê–õ–ò–ó –°–¢–ò–õ–Ø –ò –û–°–û–ë–ï–ù–ù–û–°–¢–ï–ô")
    print("-" * 50)
    
    overall_analysis = analyze_chinese_text_style(all_content)
    
    print(f"–û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–∏–º–≤–æ–ª–æ–≤: {overall_analysis['character_count']:,}")
    print(f"–ö–∏—Ç–∞–π—Å–∫–∏–µ —Å–∏–º–≤–æ–ª—ã: {overall_analysis['chinese_character_count']:,} ({overall_analysis['chinese_ratio']:.1%})")
    print(f"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π: {overall_analysis['sentence_count']:,}")
    print(f"–°—Ä–µ–¥–Ω—è—è –¥–ª–∏–Ω–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è: {overall_analysis['avg_sentence_length']} —Å–∏–º–≤–æ–ª–æ–≤")
    
    print(f"\n–ü—É–Ω–∫—Ç—É–∞—Ü–∏—è:")
    punct = overall_analysis['punctuation_analysis']
    print(f"  - –¢–æ—á–∫–∏ („ÄÇ): {punct['periods']}")
    print(f"  - –í–æ—Å–∫–ª–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–µ –∑–Ω–∞–∫–∏ (ÔºÅ): {punct['exclamation_marks']}")
    print(f"  - –í–æ–ø—Ä–æ—Å–∏—Ç–µ–ª—å–Ω—ã–µ –∑–Ω–∞–∫–∏ (Ôºü): {punct['question_marks']}")
    print(f"  - –ó–∞–ø—è—Ç—ã–µ (Ôºå): {punct['commas']}")
    print(f"  - –ö–∞–≤—ã—á–∫–∏: {punct['quotes']}")
    
    print(f"\n–°—Ç–∏–ª—å –ø–∏—Å—å–º–∞:")
    style = overall_analysis['writing_style']
    print(f"  - –î–æ–ª—è –¥–∏–∞–ª–æ–≥–æ–≤: {style['dialogue_ratio']:.1f}%")
    print(f"  - –°–ª–æ–∂–Ω–æ—Å—Ç—å —Ç–µ–∫—Å—Ç–∞: {style['complexity_score']:.1f}/10")
    
    # –ê–Ω–∞–ª–∏–∑ –ø–µ—Ä—Å–æ–Ω–∞–∂–µ–π –∏ —Ç–µ—Ä–º–∏–Ω–æ–≤
    print("\nüîç –ö–õ–Æ–ß–ï–í–´–ï –ü–ï–†–°–û–ù–ê–ñ–ò, –õ–û–ö–ê–¶–ò–ò –ò –¢–ï–†–ú–ò–ù–´")
    print("-" * 50)
    
    entities = analyze_chinese_names(all_content)
    
    print("–ü–µ—Ä—Å–æ–Ω–∞–∂–∏ –∏ –∏–º–µ–Ω–∞ (–ø–æ —á–∞—Å—Ç–æ—Ç–µ —É–ø–æ–º–∏–Ω–∞–Ω–∏—è):")
    for name, count in sorted(entities['characters'].items(), key=lambda x: x[1], reverse=True)[:15]:
        print(f"  - {name}: {count} —É–ø–æ–º–∏–Ω–∞–Ω–∏–π")
    
    if entities['titles']:
        print(f"\n–¢–∏—Ç—É–ª—ã –∏ –∑–≤–∞–Ω–∏—è:")
        for title, count in sorted(entities['titles'].items(), key=lambda x: x[1], reverse=True)[:10]:
            print(f"  - {title}: {count} —É–ø–æ–º–∏–Ω–∞–Ω–∏–π")
    
    if entities['locations']:
        print(f"\n–õ–æ–∫–∞—Ü–∏–∏ –∏ –º–µ—Å—Ç–∞:")
        for location, count in sorted(entities['locations'].items(), key=lambda x: x[1], reverse=True)[:10]:
            print(f"  - {location}: {count} —É–ø–æ–º–∏–Ω–∞–Ω–∏–π")
    
    # –ê–Ω–∞–ª–∏–∑ –ø—Ä–æ–≥—Ä–µ—Å—Å–∏–∏ –ø–æ –≥–ª–∞–≤–∞–º
    print("\nüìä –°–¢–ê–¢–ò–°–¢–ò–ö–ê –ü–û –ì–õ–ê–í–ê–ú")
    print("-" * 50)
    
    total_chars = sum(analysis['character_count'] for analysis in chapter_analyses)
    avg_chars_per_chapter = total_chars / len(chapter_analyses)
    
    print(f"–°—Ä–µ–¥–Ω—è—è –¥–ª–∏–Ω–∞ –≥–ª–∞–≤—ã: {avg_chars_per_chapter:.0f} —Å–∏–º–≤–æ–ª–æ–≤")
    
    shortest_chapter = min(chapter_analyses, key=lambda x: x['character_count'])
    longest_chapter = max(chapter_analyses, key=lambda x: x['character_count'])
    
    shortest_idx = chapter_analyses.index(shortest_chapter) + 1
    longest_idx = chapter_analyses.index(longest_chapter) + 1
    
    print(f"–°–∞–º–∞—è –∫–æ—Ä–æ—Ç–∫–∞—è –≥–ª–∞–≤–∞: #{shortest_idx} ({shortest_chapter['character_count']} —Å–∏–º–≤–æ–ª–æ–≤)")
    print(f"–°–∞–º–∞—è –¥–ª–∏–Ω–Ω–∞—è –≥–ª–∞–≤–∞: #{longest_idx} ({longest_chapter['character_count']} —Å–∏–º–≤–æ–ª–æ–≤)")
    
    # –ü—Ä–æ–≥—Ä–µ—Å—Å–∏—è —Å–ª–æ–∂–Ω–æ—Å—Ç–∏
    complexity_scores = [analysis['writing_style']['complexity_score'] for analysis in chapter_analyses]
    avg_complexity = sum(complexity_scores) / len(complexity_scores)
    print(f"–°—Ä–µ–¥–Ω—è—è —Å–ª–æ–∂–Ω–æ—Å—Ç—å —Ç–µ–∫—Å—Ç–∞: {avg_complexity:.1f}/10")
    
    print("\nüéØ –ó–ê–ö–õ–Æ–ß–ï–ù–ò–ï –û –ü–†–û–ò–ó–í–ï–î–ï–ù–ò–ò")
    print("-" * 50)
    print("‚Ä¢ –ñ–∞–Ω—Ä: –°—è–Ω—å—Å—è - —Ä–æ–º–∞–Ω –æ –∫—É–ª—å—Ç–∏–≤–∞—Ü–∏–∏")
    print("‚Ä¢ –°—é–∂–µ—Ç: –ò—Å—Ç–æ—Ä–∏—è –æ –º–æ–ª–æ–¥–æ–º —á–µ–ª–æ–≤–µ–∫–µ –Ω–∞ –ø—É—Ç–∏ –∫ –±–µ—Å—Å–º–µ—Ä—Ç–∏—é")
    print("‚Ä¢ –°—Ç–∏–ª—å: –ö–ª–∞—Å—Å–∏—á–µ—Å–∫–∞—è –∫–∏—Ç–∞–π—Å–∫–∞—è —Ñ–∞–Ω—Ç–∞—Å—Ç–∏–∫–∞ —Å —ç–ª–µ–º–µ–Ω—Ç–∞–º–∏ –±–æ–µ–≤—ã—Ö –∏—Å–∫—É—Å—Å—Ç–≤")
    print("‚Ä¢ –û—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏: –¢–∏–ø–∏—á–Ω–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ —Å—è–Ω—å—Å—è —Å —Ä–∞–∑–≤–∏—Ç–∏–µ–º –ø–µ—Ä—Å–æ–Ω–∞–∂–∞ —á–µ—Ä–µ–∑ —É—Ä–æ–≤–Ω–∏ —Å–∏–ª—ã")
    
    print("\n‚úÖ –ê–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à–µ–Ω!")


if __name__ == "__main__":
    epub_file_path = "/home/user/novelbins-epub/web_app/instance/epub_files/qinkan.net.epub"
    
    if not os.path.exists(epub_file_path):
        print(f"‚ùå EPUB —Ñ–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {epub_file_path}")
        sys.exit(1)
    
    analyze_chinese_epub(epub_file_path, max_chapters=10)