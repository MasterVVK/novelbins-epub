#!/usr/bin/env python3
"""
–°–∫—Ä–∏–ø—Ç –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã —Å–∞–π—Ç–∞ novelbins.com
"""
import requests
from bs4 import BeautifulSoup
import re

def analyze_novelbins_structure():
    """–ê–Ω–∞–ª–∏–∑ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã —Å–∞–π—Ç–∞ novelbins.com"""
    url = "https://novelbins.com/novel/shrouding-the-heavens-1150192/"
    
    print("üîç –ê–Ω–∞–ª–∏–∑ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã —Å–∞–π—Ç–∞ novelbins.com")
    print("=" * 50)
    
    try:
        # –ü–æ–ª—É—á–∞–µ–º —Å—Ç—Ä–∞–Ω–∏—Ü—É
        headers = {
            'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
        }
        response = requests.get(url, headers=headers, timeout=30)
        response.raise_for_status()
        
        soup = BeautifulSoup(response.text, 'html.parser')
        
        print(f"‚úÖ –°—Ç—Ä–∞–Ω–∏—Ü–∞ –∑–∞–≥—Ä—É–∂–µ–Ω–∞: {response.status_code}")
        print(f"üìÑ –†–∞–∑–º–µ—Ä HTML: {len(response.text)} —Å–∏–º–≤–æ–ª–æ–≤")
        
        # –ò—â–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫
        title = soup.find('title')
        if title:
            print(f"üìñ –ó–∞–≥–æ–ª–æ–≤–æ–∫: {title.text}")
        
        # –ò—â–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –Ω–æ–≤–µ–ª–ª–µ
        novel_info = soup.find('h1')
        if novel_info:
            print(f"üìö –ù–∞–∑–≤–∞–Ω–∏–µ: {novel_info.text.strip()}")
        
        # –ò—â–µ–º –∞–≤—Ç–æ—Ä–∞
        author = soup.find('p', string=re.compile(r'Author:', re.I))
        if author:
            print(f"‚úçÔ∏è –ê–≤—Ç–æ—Ä: {author.text.strip()}")
        
        # –ò—â–µ–º –ø–æ—Å–ª–µ–¥–Ω—é—é –≥–ª–∞–≤—É
        latest_chapter = soup.find('p', string=re.compile(r'Latest:', re.I))
        if latest_chapter:
            print(f"üìñ –ü–æ—Å–ª–µ–¥–Ω—è—è –≥–ª–∞–≤–∞: {latest_chapter.text.strip()}")
        
        # –ò—â–µ–º –≤—Å–µ —Å—Å—ã–ª–∫–∏ –Ω–∞ –≥–ª–∞–≤—ã
        chapter_links = soup.find_all('a', href=re.compile(r'/chapter/\d+'))
        print(f"\nüîó –ù–∞–π–¥–µ–Ω–æ —Å—Å—ã–ª–æ–∫ –Ω–∞ –≥–ª–∞–≤—ã: {len(chapter_links)}")
        
        if chapter_links:
            print("üìã –ü—Ä–∏–º–µ—Ä—ã —Å—Å—ã–ª–æ–∫ –Ω–∞ –≥–ª–∞–≤—ã:")
            for i, link in enumerate(chapter_links[:5]):
                print(f"  {i+1}. {link.text.strip()} -> {link['href']}")
        
        # –ò—â–µ–º –Ω–∞–≤–∏–≥–∞—Ü–∏–æ–Ω–Ω—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã
        nav_elements = soup.find_all(['nav', 'ul', 'div'], class_=re.compile(r'nav|tab|menu', re.I))
        print(f"\nüß≠ –ù–∞–π–¥–µ–Ω–æ –Ω–∞–≤–∏–≥–∞—Ü–∏–æ–Ω–Ω—ã—Ö —ç–ª–µ–º–µ–Ω—Ç–æ–≤: {len(nav_elements)}")
        
        for i, nav in enumerate(nav_elements[:3]):
            print(f"  {i+1}. {nav.name} class='{nav.get('class', [])}'")
        
        # –ò—â–µ–º –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä—ã —Å –∫–æ–Ω—Ç–µ–Ω—Ç–æ–º
        content_containers = soup.find_all(['div', 'section'], class_=re.compile(r'content|chapter|list', re.I))
        print(f"\nüì¶ –ù–∞–π–¥–µ–Ω–æ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–æ–≤ –∫–æ–Ω—Ç–µ–Ω—Ç–∞: {len(content_containers)}")
        
        for i, container in enumerate(content_containers[:3]):
            print(f"  {i+1}. {container.name} class='{container.get('class', [])}'")
        
        # –ò—â–µ–º —Å–∫—Ä–∏–ø—Ç—ã
        scripts = soup.find_all('script')
        print(f"\nüìú –ù–∞–π–¥–µ–Ω–æ —Å–∫—Ä–∏–ø—Ç–æ–≤: {len(scripts)}")
        
        # –ò—â–µ–º —ç–ª–µ–º–µ–Ω—Ç—ã —Å data-–∞—Ç—Ä–∏–±—É—Ç–∞–º–∏
        data_elements = soup.find_all(attrs={"data-": True})
        print(f"\nüíæ –ù–∞–π–¥–µ–Ω–æ —ç–ª–µ–º–µ–Ω—Ç–æ–≤ —Å data-–∞—Ç—Ä–∏–±—É—Ç–∞–º–∏: {len(data_elements)}")
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º HTML –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
        with open('novelbins_page.html', 'w', encoding='utf-8') as f:
            f.write(response.text)
        print(f"\nüíæ HTML —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ —Ñ–∞–π–ª: novelbins_page.html")
        
        return True
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∞–Ω–∞–ª–∏–∑–µ: {e}")
        return False

if __name__ == "__main__":
    analyze_novelbins_structure() 